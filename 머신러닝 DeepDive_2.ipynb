{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1.1477093 0.75\n",
      "2 1.147703 0.75\n",
      "3 1.1477076 0.75\n",
      "4 1.1477064 0.75\n",
      "5 1.1477069 0.75\n",
      "6 1.1477087 0.75\n",
      "7 1.147704 0.75\n",
      "8 1.1477008 0.75\n",
      "9 1.1477047 0.75\n",
      "10 1.1477046 0.75\n",
      "11 1.147703 0.75\n",
      "12 1.1477062 0.75\n",
      "13 1.1476977 0.75\n",
      "14 1.1476969 0.75\n",
      "15 1.1477002 0.75\n",
      "16 1.1476973 0.75\n",
      "17 1.1476984 0.75\n",
      "18 1.1476978 0.75\n",
      "19 1.1476957 0.75\n",
      "20 1.1476961 0.75\n",
      "21 1.1476917 0.75\n",
      "22 1.1476932 0.75\n",
      "23 1.1476923 0.75\n",
      "24 1.1476955 0.75\n",
      "25 1.1476957 0.75\n",
      "26 1.1476933 0.75\n",
      "27 1.1476871 0.75\n",
      "28 1.147689 0.75\n",
      "29 1.1476879 0.75\n",
      "30 1.1476903 0.75\n",
      "31 1.1476864 0.75\n",
      "32 1.1476909 0.75\n",
      "33 1.1476823 0.75\n",
      "34 1.147687 0.75\n",
      "35 1.1476871 0.75\n",
      "36 1.1476871 0.75\n",
      "37 1.1476804 0.75\n",
      "38 1.1476871 0.75\n",
      "39 1.1476876 0.75\n",
      "40 1.1476809 0.75\n",
      "41 1.1476797 0.75\n",
      "42 1.1476794 0.75\n",
      "43 1.1476802 0.75\n",
      "44 1.1476785 0.75\n",
      "45 1.1476756 0.75\n",
      "46 1.1476785 0.75\n",
      "47 1.1476803 0.75\n",
      "48 1.1476736 0.75\n",
      "49 1.1476785 0.75\n",
      "50 1.1476742 0.75\n",
      "51 1.1476734 0.75\n",
      "52 1.1476711 0.75\n",
      "53 1.147669 0.75\n",
      "54 1.1476752 0.75\n",
      "55 1.1476712 0.75\n",
      "56 1.1476717 0.75\n",
      "57 1.1476718 0.75\n",
      "58 1.147674 0.75\n",
      "59 1.1476717 0.75\n",
      "60 1.1476686 0.75\n",
      "61 1.1476642 0.75\n",
      "62 1.1476709 0.75\n",
      "63 1.1476642 0.75\n",
      "64 1.1476694 0.75\n",
      "65 1.1476625 0.75\n",
      "66 1.1476649 0.75\n",
      "67 1.1476649 0.75\n",
      "68 1.147665 0.75\n",
      "69 1.1476655 0.75\n",
      "70 1.1476611 0.75\n",
      "71 1.1476603 0.75\n",
      "72 1.147657 0.75\n",
      "73 1.1476626 0.75\n",
      "74 1.1476603 0.75\n",
      "75 1.1476537 0.75\n",
      "76 1.1476558 0.75\n",
      "77 1.1476542 0.75\n",
      "78 1.1476564 0.75\n",
      "79 1.1476588 0.75\n",
      "80 1.1476558 0.75\n",
      "81 1.1476535 0.75\n",
      "82 1.1476518 0.75\n",
      "83 1.1476518 0.75\n",
      "84 1.1476537 0.75\n",
      "85 1.1476542 0.75\n",
      "86 1.1476506 0.75\n",
      "87 1.147655 0.75\n",
      "88 1.1476519 0.75\n",
      "89 1.147648 0.75\n",
      "90 1.1476488 0.75\n",
      "91 1.1476449 0.75\n",
      "92 1.1476467 0.75\n",
      "93 1.1476489 0.75\n",
      "94 1.1476451 0.75\n",
      "95 1.147645 0.75\n",
      "96 1.1476427 0.75\n",
      "97 1.1476443 0.75\n",
      "98 1.1476421 0.75\n",
      "99 1.147642 0.75\n",
      "101 1.1476372 0.75\n",
      "102 1.1476381 0.75\n",
      "103 1.1476383 0.75\n",
      "104 1.1476365 0.75\n",
      "105 1.1476406 0.75\n",
      "106 1.1476353 0.75\n",
      "107 1.1476344 0.75\n",
      "108 1.1476353 0.75\n",
      "109 1.1476358 0.75\n",
      "110 1.1476337 0.75\n",
      "111 1.1476359 0.75\n",
      "112 1.1476274 0.75\n",
      "113 1.1476293 0.75\n",
      "114 1.1476341 0.75\n",
      "115 1.1476284 0.75\n",
      "116 1.147633 0.75\n",
      "117 1.1476303 0.75\n",
      "118 1.1476268 0.75\n",
      "119 1.1476284 0.75\n",
      "120 1.147629 0.75\n",
      "121 1.1476268 0.75\n",
      "122 1.147623 0.75\n",
      "123 1.147623 0.75\n",
      "124 1.1476275 0.75\n",
      "125 1.1476253 0.75\n",
      "126 1.1476268 0.75\n",
      "127 1.1476221 0.75\n",
      "128 1.1476191 0.75\n",
      "129 1.1476227 0.75\n",
      "130 1.1476206 0.75\n",
      "131 1.1476225 0.75\n",
      "132 1.1476223 0.75\n",
      "133 1.1476216 0.75\n",
      "134 1.1476167 0.75\n",
      "135 1.1476177 0.75\n",
      "136 1.147619 0.75\n",
      "137 1.1476195 0.75\n",
      "138 1.1476133 0.75\n",
      "139 1.1476202 0.75\n",
      "140 1.1476154 0.75\n",
      "141 1.1476115 0.75\n",
      "142 1.1476115 0.75\n",
      "143 1.147616 0.75\n",
      "144 1.1476108 0.75\n",
      "145 1.14761 0.75\n",
      "146 1.1476091 0.75\n",
      "147 1.1476109 0.75\n",
      "148 1.1476086 0.75\n",
      "149 1.1476088 0.75\n",
      "150 1.1476092 0.75\n",
      "151 1.1476057 0.75\n",
      "152 1.1476098 0.75\n",
      "153 1.1476008 0.75\n",
      "154 1.1476077 0.75\n",
      "155 1.1476015 0.75\n",
      "156 1.1476023 0.75\n",
      "157 1.1476005 0.75\n",
      "158 1.1476064 0.75\n",
      "159 1.1476046 0.75\n",
      "160 1.147603 0.75\n",
      "161 1.1475984 0.75\n",
      "162 1.147603 0.75\n",
      "163 1.147601 0.75\n",
      "164 1.1476022 0.75\n",
      "165 1.1476022 0.75\n",
      "166 1.1475978 0.75\n",
      "167 1.1475939 0.75\n",
      "168 1.1475979 0.75\n",
      "169 1.147594 0.75\n",
      "170 1.1475967 0.75\n",
      "171 1.1475927 0.75\n",
      "172 1.1475962 0.75\n",
      "173 1.147591 0.75\n",
      "174 1.14759 0.75\n",
      "175 1.1475916 0.75\n",
      "176 1.1475871 0.75\n",
      "177 1.1475872 0.75\n",
      "178 1.1475893 0.75\n",
      "179 1.147581 0.75\n",
      "180 1.1475837 0.75\n",
      "181 1.1475849 0.75\n",
      "182 1.1475881 0.75\n",
      "183 1.1475854 0.75\n",
      "184 1.1475848 0.75\n",
      "185 1.1475848 0.75\n",
      "186 1.1475809 0.75\n",
      "187 1.1475827 0.75\n",
      "188 1.147583 0.75\n",
      "189 1.1475849 0.75\n",
      "190 1.1475863 0.75\n",
      "191 1.1475793 0.75\n",
      "192 1.1475796 0.75\n",
      "193 1.1475797 0.75\n",
      "194 1.1475801 0.75\n",
      "195 1.1475797 0.75\n",
      "196 1.1475762 0.75\n",
      "197 1.1475741 0.75\n",
      "198 1.1475781 0.75\n",
      "199 1.1475773 0.75\n",
      "201 1.1475765 0.75\n",
      "202 1.1475729 0.75\n",
      "203 1.1475769 0.75\n",
      "204 1.1475716 0.75\n",
      "205 1.1475712 0.75\n",
      "206 1.1475673 0.75\n",
      "207 1.1475668 0.75\n",
      "208 1.1475655 0.75\n",
      "209 1.1475674 0.75\n",
      "210 1.1475687 0.75\n",
      "211 1.1475688 0.75\n",
      "212 1.1475703 0.75\n",
      "213 1.1475595 0.75\n",
      "214 1.1475626 0.75\n",
      "215 1.1475626 0.75\n",
      "216 1.1475668 0.75\n",
      "217 1.1475605 0.75\n",
      "218 1.1475673 0.75\n",
      "219 1.1475587 0.75\n",
      "220 1.1475593 0.75\n",
      "221 1.1475612 0.75\n",
      "222 1.1475617 0.75\n",
      "223 1.1475581 0.75\n",
      "224 1.147562 0.75\n",
      "225 1.1475581 0.75\n",
      "226 1.1475604 0.75\n",
      "227 1.1475538 0.75\n",
      "228 1.1475619 0.75\n",
      "229 1.147553 0.75\n",
      "230 1.1475536 0.75\n",
      "231 1.1475536 0.75\n",
      "232 1.1475534 0.75\n",
      "233 1.1475536 0.75\n",
      "234 1.1475536 0.75\n",
      "235 1.1475492 0.75\n",
      "236 1.1475537 0.75\n",
      "237 1.1475528 0.75\n",
      "238 1.1475552 0.75\n",
      "239 1.1475512 0.75\n",
      "240 1.1475483 0.75\n",
      "241 1.1475437 0.75\n",
      "242 1.147545 0.75\n",
      "243 1.147545 0.75\n",
      "244 1.1475474 0.75\n",
      "245 1.1475428 0.75\n",
      "246 1.1475438 0.75\n",
      "247 1.1475434 0.75\n",
      "248 1.1475397 0.75\n",
      "249 1.1475412 0.75\n",
      "250 1.1475424 0.75\n",
      "251 1.1475359 0.75\n",
      "252 1.1475445 0.75\n",
      "253 1.1475364 0.75\n",
      "254 1.1475384 0.75\n",
      "255 1.1475343 0.75\n",
      "256 1.1475362 0.75\n",
      "257 1.1475345 0.75\n",
      "258 1.1475376 0.75\n",
      "259 1.1475337 0.75\n",
      "260 1.1475337 0.75\n",
      "261 1.1475315 0.75\n",
      "262 1.1475359 0.75\n",
      "263 1.1475321 0.75\n",
      "264 1.1475338 0.75\n",
      "265 1.1475344 0.75\n",
      "266 1.1475276 0.75\n",
      "267 1.1475277 0.75\n",
      "268 1.147529 0.75\n",
      "269 1.147529 0.75\n",
      "270 1.1475269 0.75\n",
      "271 1.147523 0.75\n",
      "272 1.1475269 0.75\n",
      "273 1.1475271 0.75\n",
      "274 1.1475253 0.75\n",
      "275 1.1475168 0.75\n",
      "276 1.1475254 0.75\n",
      "277 1.1475168 0.75\n",
      "278 1.1475238 0.75\n",
      "279 1.1475183 0.75\n",
      "280 1.1475205 0.75\n",
      "281 1.147514 0.75\n",
      "282 1.1475185 0.75\n",
      "283 1.1475185 0.75\n",
      "284 1.1475185 0.75\n",
      "285 1.1475186 0.75\n",
      "286 1.1475191 0.75\n",
      "287 1.1475124 0.75\n",
      "288 1.1475178 0.75\n",
      "289 1.1475129 0.75\n",
      "290 1.1475161 0.75\n",
      "291 1.1475143 0.75\n",
      "292 1.1475183 0.75\n",
      "293 1.1475096 0.75\n",
      "294 1.1475109 0.75\n",
      "295 1.1475109 0.75\n",
      "296 1.1475133 0.75\n",
      "297 1.1475116 0.75\n",
      "298 1.147507 0.75\n",
      "299 1.1475054 0.75\n",
      "301 1.1475078 0.75\n",
      "302 1.1475055 0.75\n",
      "303 1.147502 0.75\n",
      "304 1.1475086 0.75\n",
      "305 1.147501 0.75\n",
      "306 1.1475047 0.75\n",
      "307 1.1475047 0.75\n",
      "308 1.1475004 0.75\n",
      "309 1.1475009 0.75\n",
      "310 1.1474987 0.75\n",
      "311 1.1474986 0.75\n",
      "312 1.1475018 0.75\n",
      "313 1.147501 0.75\n",
      "314 1.1474934 0.75\n",
      "315 1.1475002 0.75\n",
      "316 1.1474956 0.75\n",
      "317 1.1474985 0.75\n",
      "318 1.1474935 0.75\n",
      "319 1.1474941 0.75\n",
      "320 1.1474917 0.75\n",
      "321 1.1474918 0.75\n",
      "322 1.1474942 0.75\n",
      "323 1.1474932 0.75\n",
      "324 1.1474906 0.75\n",
      "325 1.1474911 0.75\n",
      "326 1.147489 0.75\n",
      "327 1.1474886 0.75\n",
      "328 1.1474844 0.75\n",
      "329 1.1474873 0.75\n",
      "330 1.1474942 0.75\n",
      "331 1.1474851 0.75\n",
      "332 1.1474857 0.75\n",
      "333 1.147488 0.75\n",
      "334 1.1474866 0.75\n",
      "335 1.1474828 0.75\n",
      "336 1.1474805 0.75\n",
      "337 1.1474828 0.75\n",
      "338 1.1474828 0.75\n",
      "339 1.1474788 0.75\n",
      "340 1.1474811 0.75\n",
      "341 1.1474767 0.75\n",
      "342 1.1474748 0.75\n",
      "343 1.147478 0.75\n",
      "344 1.1474781 0.75\n",
      "345 1.1474763 0.75\n",
      "346 1.1474742 0.75\n",
      "347 1.1474756 0.75\n",
      "348 1.1474742 0.75\n",
      "349 1.1474782 0.75\n",
      "350 1.1474743 0.75\n",
      "351 1.147472 0.75\n",
      "352 1.1474733 0.75\n",
      "353 1.1474735 0.75\n",
      "354 1.1474739 0.75\n",
      "355 1.1474695 0.75\n",
      "356 1.147469 0.75\n",
      "357 1.1474696 0.75\n",
      "358 1.1474653 0.75\n",
      "359 1.1474696 0.75\n",
      "360 1.1474676 0.75\n",
      "361 1.1474646 0.75\n",
      "362 1.1474668 0.75\n",
      "363 1.1474671 0.75\n",
      "364 1.1474651 0.75\n",
      "365 1.1474633 0.75\n",
      "366 1.1474607 0.75\n",
      "367 1.1474568 0.75\n",
      "368 1.1474612 0.75\n",
      "369 1.147463 0.75\n",
      "370 1.1474595 0.75\n",
      "371 1.1474568 0.75\n",
      "372 1.1474644 0.75\n",
      "373 1.1474584 0.75\n",
      "374 1.1474566 0.75\n",
      "375 1.147456 0.75\n",
      "376 1.147452 0.75\n",
      "377 1.1474549 0.75\n",
      "378 1.1474553 0.75\n",
      "379 1.1474501 0.75\n",
      "380 1.1474524 0.75\n",
      "381 1.1474537 0.75\n",
      "382 1.147453 0.75\n",
      "383 1.1474515 0.75\n",
      "384 1.1474515 0.75\n",
      "385 1.1474494 0.75\n",
      "386 1.1474515 0.75\n",
      "387 1.14745 0.75\n",
      "388 1.1474504 0.75\n",
      "389 1.147446 0.75\n",
      "390 1.1474469 0.75\n",
      "391 1.1474509 0.75\n",
      "392 1.147451 0.75\n",
      "393 1.1474429 0.75\n",
      "394 1.1474448 0.75\n",
      "395 1.1474429 0.75\n",
      "396 1.1474415 0.75\n",
      "397 1.1474432 0.75\n",
      "398 1.1474379 0.75\n",
      "399 1.1474379 0.75\n",
      "401 1.1474406 0.75\n",
      "402 1.1474425 0.75\n",
      "403 1.1474429 0.75\n",
      "404 1.1474409 0.75\n",
      "405 1.1474364 0.75\n",
      "406 1.1474346 0.75\n",
      "407 1.1474333 0.75\n",
      "408 1.1474369 0.75\n",
      "409 1.1474334 0.75\n",
      "410 1.1474352 0.75\n",
      "411 1.1474339 0.75\n",
      "412 1.1474317 0.75\n",
      "413 1.1474322 0.75\n",
      "414 1.1474295 0.75\n",
      "415 1.1474341 0.75\n",
      "416 1.147431 0.75\n",
      "417 1.1474301 0.75\n",
      "418 1.1474265 0.75\n",
      "419 1.1474248 0.75\n",
      "420 1.1474248 0.75\n",
      "421 1.1474254 0.75\n",
      "422 1.1474233 0.75\n",
      "423 1.1474233 0.75\n",
      "424 1.1474278 0.75\n",
      "425 1.147421 0.75\n",
      "426 1.1474265 0.75\n",
      "427 1.1474224 0.75\n",
      "428 1.1474217 0.75\n",
      "429 1.1474248 0.75\n",
      "430 1.1474222 0.75\n",
      "431 1.1474156 0.75\n",
      "432 1.1474165 0.75\n",
      "433 1.1474165 0.75\n",
      "434 1.1474189 0.75\n",
      "435 1.147417 0.75\n",
      "436 1.1474148 0.75\n",
      "437 1.1474203 0.75\n",
      "438 1.1474164 0.75\n",
      "439 1.1474158 0.75\n",
      "440 1.1474162 0.75\n",
      "441 1.1474103 0.75\n",
      "442 1.1474147 0.75\n",
      "443 1.1474098 0.75\n",
      "444 1.1474102 0.75\n",
      "445 1.1474091 0.75\n",
      "446 1.147409 0.75\n",
      "447 1.147405 0.75\n",
      "448 1.1474072 0.75\n",
      "449 1.1474073 0.75\n",
      "450 1.1474056 0.75\n",
      "451 1.1474029 0.75\n",
      "452 1.1474053 0.75\n",
      "453 1.1474034 0.75\n",
      "454 1.1473995 0.75\n",
      "455 1.1474012 0.75\n",
      "456 1.1474062 0.75\n",
      "457 1.1474029 0.75\n",
      "458 1.1473988 0.75\n",
      "459 1.1474007 0.75\n",
      "460 1.147399 0.75\n",
      "461 1.1474006 0.75\n",
      "462 1.147401 0.75\n",
      "463 1.1473999 0.75\n",
      "464 1.1473967 0.75\n",
      "465 1.147398 0.75\n",
      "466 1.1473883 0.75\n",
      "467 1.1473984 0.75\n",
      "468 1.1473917 0.75\n",
      "469 1.1474005 0.75\n",
      "470 1.1473914 0.75\n",
      "471 1.1473927 0.75\n",
      "472 1.1473922 0.75\n",
      "473 1.1473881 0.75\n",
      "474 1.1473914 0.75\n",
      "475 1.1473905 0.75\n",
      "476 1.1473875 0.75\n",
      "477 1.1473899 0.75\n",
      "478 1.1473917 0.75\n",
      "479 1.1473922 0.75\n",
      "480 1.1473836 0.75\n",
      "481 1.1473837 0.75\n",
      "482 1.1473837 0.75\n",
      "483 1.147386 0.75\n",
      "484 1.1473807 0.75\n",
      "485 1.1473829 0.75\n",
      "486 1.1473823 0.75\n",
      "487 1.1473813 0.75\n",
      "488 1.1473852 0.75\n",
      "489 1.1473813 0.75\n",
      "490 1.1473808 0.75\n",
      "491 1.1473774 0.75\n",
      "492 1.1473793 0.75\n",
      "493 1.1473805 0.75\n",
      "494 1.1473739 0.75\n",
      "495 1.1473762 0.75\n",
      "496 1.147374 0.75\n",
      "497 1.14737 0.75\n",
      "498 1.1473745 0.75\n",
      "499 1.1473724 0.75\n",
      "501 1.1473724 0.75\n",
      "502 1.1473716 0.75\n",
      "503 1.1473718 0.75\n",
      "504 1.1473715 0.75\n",
      "505 1.14737 0.75\n",
      "506 1.1473671 0.75\n",
      "507 1.1473638 0.75\n",
      "508 1.1473651 0.75\n",
      "509 1.147366 0.75\n",
      "510 1.1473616 0.75\n",
      "511 1.1473638 0.75\n",
      "512 1.147364 0.75\n",
      "513 1.147365 0.75\n",
      "514 1.1473665 0.75\n",
      "515 1.1473671 0.75\n",
      "516 1.1473609 0.75\n",
      "517 1.1473588 0.75\n",
      "518 1.1473571 0.75\n",
      "519 1.1473571 0.75\n",
      "520 1.1473594 0.75\n",
      "521 1.1473594 0.75\n",
      "522 1.1473602 0.75\n",
      "523 1.147358 0.75\n",
      "524 1.1473588 0.75\n",
      "525 1.1473588 0.75\n",
      "526 1.1473588 0.75\n",
      "527 1.1473569 0.75\n",
      "528 1.1473557 0.75\n",
      "529 1.1473547 0.75\n",
      "530 1.1473547 0.75\n",
      "531 1.1473532 0.75\n",
      "532 1.1473497 0.75\n",
      "533 1.1473455 0.75\n",
      "534 1.1473492 0.75\n",
      "535 1.1473479 0.75\n",
      "536 1.1473502 0.75\n",
      "537 1.1473457 0.75\n",
      "538 1.147344 0.75\n",
      "539 1.1473459 0.75\n",
      "540 1.1473465 0.75\n",
      "541 1.1473441 0.75\n",
      "542 1.1473478 0.75\n",
      "543 1.1473389 0.75\n",
      "544 1.1473411 0.75\n",
      "545 1.1473452 0.75\n",
      "546 1.1473399 0.75\n",
      "547 1.1473413 0.75\n",
      "548 1.1473428 0.75\n",
      "549 1.1473383 0.75\n",
      "550 1.1473397 0.75\n",
      "551 1.1473389 0.75\n",
      "552 1.1473365 0.75\n",
      "553 1.1473411 0.75\n",
      "554 1.147335 0.75\n",
      "555 1.1473366 0.75\n",
      "556 1.1473304 0.75\n",
      "557 1.1473373 0.75\n",
      "558 1.1473334 0.75\n",
      "559 1.1473342 0.75\n",
      "560 1.1473311 0.75\n",
      "561 1.1473337 0.75\n",
      "562 1.1473342 0.75\n",
      "563 1.1473259 0.75\n",
      "564 1.1473283 0.75\n",
      "565 1.1473283 0.75\n",
      "566 1.1473291 0.75\n",
      "567 1.1473331 0.75\n",
      "568 1.1473274 0.75\n",
      "569 1.1473252 0.75\n",
      "570 1.1473212 0.75\n",
      "571 1.1473231 0.75\n",
      "572 1.1473235 0.75\n",
      "573 1.1473259 0.75\n",
      "574 1.1473192 0.75\n",
      "575 1.1473219 0.75\n",
      "576 1.1473215 0.75\n",
      "577 1.1473198 0.75\n",
      "578 1.1473198 0.75\n",
      "579 1.1473185 0.75\n",
      "580 1.1473141 0.75\n",
      "581 1.1473204 0.75\n",
      "582 1.1473147 0.75\n",
      "583 1.1473145 0.75\n",
      "584 1.1473168 0.75\n",
      "585 1.1473147 0.75\n",
      "586 1.1473175 0.75\n",
      "587 1.1473154 0.75\n",
      "588 1.1473122 0.75\n",
      "589 1.1473145 0.75\n",
      "590 1.1473123 0.75\n",
      "591 1.1473145 0.75\n",
      "592 1.1473079 0.75\n",
      "593 1.1473123 0.75\n",
      "594 1.1473101 0.75\n",
      "595 1.1473083 0.75\n",
      "596 1.1473072 0.75\n",
      "597 1.1473078 0.75\n",
      "598 1.1473078 0.75\n",
      "599 1.1473055 0.75\n",
      "601 1.1473061 0.75\n",
      "602 1.1473018 0.75\n",
      "603 1.147306 0.75\n",
      "604 1.1473039 0.75\n",
      "605 1.1472977 0.75\n",
      "606 1.1472987 0.75\n",
      "607 1.1473045 0.75\n",
      "608 1.147297 0.75\n",
      "609 1.1472971 0.75\n",
      "610 1.1472993 0.75\n",
      "611 1.1472987 0.75\n",
      "612 1.1472932 0.75\n",
      "613 1.1472994 0.75\n",
      "614 1.1472956 0.75\n",
      "615 1.1472932 0.75\n",
      "616 1.1472956 0.75\n",
      "617 1.1472946 0.75\n",
      "618 1.147292 0.75\n",
      "619 1.1472903 0.75\n",
      "620 1.1472903 0.75\n",
      "621 1.1472925 0.75\n",
      "622 1.1472902 0.75\n",
      "623 1.1472908 0.75\n",
      "624 1.1472887 0.75\n",
      "625 1.1472887 0.75\n",
      "626 1.1472901 0.75\n",
      "627 1.1472896 0.75\n",
      "628 1.147288 0.75\n",
      "629 1.1472796 0.75\n",
      "630 1.1472862 0.75\n",
      "631 1.1472834 0.75\n",
      "632 1.1472884 0.75\n",
      "633 1.1472842 0.75\n",
      "634 1.147281 0.75\n",
      "635 1.1472856 0.75\n",
      "636 1.1472803 0.75\n",
      "637 1.1472812 0.75\n",
      "638 1.1472765 0.75\n",
      "639 1.1472794 0.75\n",
      "640 1.1472794 0.75\n",
      "641 1.1472756 0.75\n",
      "642 1.1472774 0.75\n",
      "643 1.1472789 0.75\n",
      "644 1.1472788 0.75\n",
      "645 1.1472726 0.75\n",
      "646 1.1472726 0.75\n",
      "647 1.1472727 0.75\n",
      "648 1.1472769 0.75\n",
      "649 1.147275 0.75\n",
      "650 1.1472706 0.75\n",
      "651 1.1472706 0.75\n",
      "652 1.1472691 0.75\n",
      "653 1.1472713 0.75\n",
      "654 1.1472722 0.75\n",
      "655 1.1472726 0.75\n",
      "656 1.1472727 0.75\n",
      "657 1.147266 0.75\n",
      "658 1.1472706 0.75\n",
      "659 1.1472683 0.75\n",
      "660 1.1472639 0.75\n",
      "661 1.1472629 0.75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "662 1.1472652 0.75\n",
      "663 1.1472635 0.75\n",
      "664 1.1472653 0.75\n",
      "665 1.147257 0.75\n",
      "666 1.1472653 0.75\n",
      "667 1.1472615 0.75\n",
      "668 1.1472597 0.75\n",
      "669 1.1472563 0.75\n",
      "670 1.1472559 0.75\n",
      "671 1.1472585 0.75\n",
      "672 1.1472569 0.75\n",
      "673 1.1472569 0.75\n",
      "674 1.1472551 0.75\n",
      "675 1.1472569 0.75\n",
      "676 1.147253 0.75\n",
      "677 1.1472529 0.75\n",
      "678 1.1472536 0.75\n",
      "679 1.1472532 0.75\n",
      "680 1.1472554 0.75\n",
      "681 1.1472515 0.75\n",
      "682 1.1472478 0.75\n",
      "683 1.1472479 0.75\n",
      "684 1.1472484 0.75\n",
      "685 1.1472502 0.75\n",
      "686 1.1472462 0.75\n",
      "687 1.1472516 0.75\n",
      "688 1.1472484 0.75\n",
      "689 1.1472455 0.75\n",
      "690 1.1472447 0.75\n",
      "691 1.1472473 0.75\n",
      "692 1.1472455 0.75\n",
      "693 1.1472455 0.75\n",
      "694 1.1472393 0.75\n",
      "695 1.1472378 0.75\n",
      "696 1.1472417 0.75\n",
      "697 1.1472377 0.75\n",
      "698 1.1472377 0.75\n",
      "699 1.1472384 0.75\n",
      "701 1.1472384 0.75\n",
      "702 1.147237 0.75\n",
      "703 1.1472331 0.75\n",
      "704 1.1472377 0.75\n",
      "705 1.147235 0.75\n",
      "706 1.1472334 0.75\n",
      "707 1.1472355 0.75\n",
      "708 1.147236 0.75\n",
      "709 1.1472279 0.75\n",
      "710 1.1472316 0.75\n",
      "711 1.1472303 0.75\n",
      "712 1.14723 0.75\n",
      "713 1.147231 0.75\n",
      "714 1.1472281 0.75\n",
      "715 1.147231 0.75\n",
      "716 1.1472265 0.75\n",
      "717 1.1472266 0.75\n",
      "718 1.1472298 0.75\n",
      "719 1.147228 0.75\n",
      "720 1.1472236 0.75\n",
      "721 1.1472218 0.75\n",
      "722 1.1472224 0.75\n",
      "723 1.1472241 0.75\n",
      "724 1.147222 0.75\n",
      "725 1.1472219 0.75\n",
      "726 1.1472267 0.75\n",
      "727 1.1472203 0.75\n",
      "728 1.1472181 0.75\n",
      "729 1.1472191 0.75\n",
      "730 1.1472195 0.75\n",
      "731 1.1472218 0.75\n",
      "732 1.1472218 0.75\n",
      "733 1.1472152 0.75\n",
      "734 1.1472135 0.75\n",
      "735 1.1472166 0.75\n",
      "736 1.1472167 0.75\n",
      "737 1.1472145 0.75\n",
      "738 1.1472129 0.75\n",
      "739 1.1472145 0.75\n",
      "740 1.1472129 0.75\n",
      "741 1.147211 0.75\n",
      "742 1.1472083 0.75\n",
      "743 1.1472044 0.75\n",
      "744 1.1472112 0.75\n",
      "745 1.1472067 0.75\n",
      "746 1.1472058 0.75\n",
      "747 1.1472052 0.75\n",
      "748 1.1472082 0.75\n",
      "749 1.1472019 0.75\n",
      "750 1.147206 0.75\n",
      "751 1.1472067 0.75\n",
      "752 1.1472026 0.75\n",
      "753 1.1472048 0.75\n",
      "754 1.1472 0.75\n",
      "755 1.1472014 0.75\n",
      "756 1.1472015 0.75\n",
      "757 1.1472037 0.75\n",
      "758 1.1472036 0.75\n",
      "759 1.1471993 0.75\n",
      "760 1.1471993 0.75\n",
      "761 1.1471976 0.75\n",
      "762 1.1472007 0.75\n",
      "763 1.1471981 0.75\n",
      "764 1.1471945 0.75\n",
      "765 1.1471939 0.75\n",
      "766 1.147192 0.75\n",
      "767 1.1471908 0.75\n",
      "768 1.1471944 0.75\n",
      "769 1.1471864 0.75\n",
      "770 1.1471958 0.75\n",
      "771 1.1471885 0.75\n",
      "772 1.1471891 0.75\n",
      "773 1.147187 0.75\n",
      "774 1.1471914 0.75\n",
      "775 1.1471862 0.75\n",
      "776 1.1471864 0.75\n",
      "777 1.1471883 0.75\n",
      "778 1.1471825 0.75\n",
      "779 1.1471884 0.75\n",
      "780 1.147187 0.75\n",
      "781 1.1471841 0.75\n",
      "782 1.1471877 0.75\n",
      "783 1.1471869 0.75\n",
      "784 1.1471795 0.75\n",
      "785 1.1471785 0.75\n",
      "786 1.14718 0.75\n",
      "787 1.1471777 0.75\n",
      "788 1.1471819 0.75\n",
      "789 1.1471804 0.75\n",
      "790 1.1471801 0.75\n",
      "791 1.1471758 0.75\n",
      "792 1.1471739 0.75\n",
      "793 1.1471763 0.75\n",
      "794 1.1471763 0.75\n",
      "795 1.147171 0.75\n",
      "796 1.1471795 0.75\n",
      "797 1.1471734 0.75\n",
      "798 1.147171 0.75\n",
      "799 1.1471733 0.75\n",
      "801 1.1471747 0.75\n",
      "802 1.1471682 0.75\n",
      "803 1.1471695 0.75\n",
      "804 1.1471683 0.75\n",
      "805 1.1471688 0.75\n",
      "806 1.1471704 0.75\n",
      "807 1.1471683 0.75\n",
      "808 1.1471643 0.75\n",
      "809 1.1471621 0.75\n",
      "810 1.147165 0.75\n",
      "811 1.1471628 0.75\n",
      "812 1.1471641 0.75\n",
      "813 1.147162 0.75\n",
      "814 1.1471634 0.75\n",
      "815 1.1471621 0.75\n",
      "816 1.1471602 0.75\n",
      "817 1.1471559 0.75\n",
      "818 1.1471621 0.75\n",
      "819 1.1471604 0.75\n",
      "820 1.1471604 0.75\n",
      "821 1.1471612 0.75\n",
      "822 1.147161 0.75\n",
      "823 1.1471637 0.75\n",
      "824 1.1471535 0.75\n",
      "825 1.1471527 0.75\n",
      "826 1.1471514 0.75\n",
      "827 1.1471554 0.75\n",
      "828 1.1471514 0.75\n",
      "829 1.1471477 0.75\n",
      "830 1.1471528 0.75\n",
      "831 1.1471498 0.75\n",
      "832 1.1471511 0.75\n",
      "833 1.1471468 0.75\n",
      "834 1.1471447 0.75\n",
      "835 1.1471452 0.75\n",
      "836 1.1471468 0.75\n",
      "837 1.1471491 0.75\n",
      "838 1.1471434 0.75\n",
      "839 1.147143 0.75\n",
      "840 1.147143 0.75\n",
      "841 1.1471422 0.75\n",
      "842 1.1471444 0.75\n",
      "843 1.1471444 0.75\n",
      "844 1.1471423 0.75\n",
      "845 1.1471406 0.75\n",
      "846 1.1471401 0.75\n",
      "847 1.1471384 0.75\n",
      "848 1.147137 0.75\n",
      "849 1.1471409 0.75\n",
      "850 1.1471332 0.75\n",
      "851 1.1471354 0.75\n",
      "852 1.1471378 0.75\n",
      "853 1.1471378 0.75\n",
      "854 1.1471362 0.75\n",
      "855 1.1471356 0.75\n",
      "856 1.1471299 0.75\n",
      "857 1.147132 0.75\n",
      "858 1.1471344 0.75\n",
      "859 1.1471286 0.75\n",
      "860 1.1471367 0.75\n",
      "861 1.1471288 0.75\n",
      "862 1.1471311 0.75\n",
      "863 1.1471292 0.75\n",
      "864 1.1471332 0.75\n",
      "865 1.1471304 0.75\n",
      "866 1.1471272 0.75\n",
      "867 1.1471263 0.75\n",
      "868 1.1471242 0.75\n",
      "869 1.1471255 0.75\n",
      "870 1.1471202 0.75\n",
      "871 1.1471171 0.75\n",
      "872 1.1471208 0.75\n",
      "873 1.1471226 0.75\n",
      "874 1.1471208 0.75\n",
      "875 1.1471226 0.75\n",
      "876 1.1471165 0.75\n",
      "877 1.1471196 0.75\n",
      "878 1.1471157 0.75\n",
      "879 1.147118 0.75\n",
      "880 1.1471225 0.75\n",
      "881 1.1471175 0.75\n",
      "882 1.1471201 0.75\n",
      "883 1.1471096 0.75\n",
      "884 1.1471204 0.75\n",
      "885 1.1471119 0.75\n",
      "886 1.1471168 0.75\n",
      "887 1.147113 0.75\n",
      "888 1.1471113 0.75\n",
      "889 1.1471113 0.75\n",
      "890 1.1471051 0.75\n",
      "891 1.1471095 0.75\n",
      "892 1.1471052 0.75\n",
      "893 1.1471074 0.75\n",
      "894 1.1471075 0.75\n",
      "895 1.147106 0.75\n",
      "896 1.147108 0.75\n",
      "897 1.1471013 0.75\n",
      "898 1.1471045 0.75\n",
      "899 1.147102 0.75\n",
      "901 1.1471028 0.75\n",
      "902 1.1471052 0.75\n",
      "903 1.1470989 0.75\n",
      "904 1.1471018 0.75\n",
      "905 1.147099 0.75\n",
      "906 1.1471038 0.75\n",
      "907 1.1470999 0.75\n",
      "908 1.1471024 0.75\n",
      "909 1.1471022 0.75\n",
      "910 1.1470962 0.75\n",
      "911 1.147096 0.75\n",
      "912 1.1470917 0.75\n",
      "913 1.1470975 0.75\n",
      "914 1.1470922 0.75\n",
      "915 1.1470944 0.75\n",
      "916 1.1470914 0.75\n",
      "917 1.1470914 0.75\n",
      "918 1.1470916 0.75\n",
      "919 1.147092 0.75\n",
      "920 1.1470921 0.75\n",
      "921 1.1470876 0.75\n",
      "922 1.1470922 0.75\n",
      "923 1.1470877 0.75\n",
      "924 1.1470891 0.75\n",
      "925 1.1470891 0.75\n",
      "926 1.147086 0.75\n",
      "927 1.1470886 0.75\n",
      "928 1.1470853 0.75\n",
      "929 1.1470878 0.75\n",
      "930 1.1470847 0.75\n",
      "931 1.147084 0.75\n",
      "932 1.1470855 0.75\n",
      "933 1.1470838 0.75\n",
      "934 1.1470847 0.75\n",
      "935 1.1470758 0.75\n",
      "936 1.1470785 0.75\n",
      "937 1.1470785 0.75\n",
      "938 1.1470762 0.75\n",
      "939 1.1470765 0.75\n",
      "940 1.1470742 0.75\n",
      "941 1.1470786 0.75\n",
      "942 1.1470755 0.75\n",
      "943 1.1470748 0.75\n",
      "944 1.1470757 0.75\n",
      "945 1.147078 0.75\n",
      "946 1.1470761 0.75\n",
      "947 1.1470696 0.75\n",
      "948 1.14707 0.75\n",
      "949 1.1470718 0.75\n",
      "950 1.1470703 0.75\n",
      "951 1.1470685 0.75\n",
      "952 1.1470662 0.75\n",
      "953 1.1470641 0.75\n",
      "954 1.1470695 0.75\n",
      "955 1.1470712 0.75\n",
      "956 1.1470605 0.75\n",
      "957 1.1470635 0.75\n",
      "958 1.1470674 0.75\n",
      "959 1.147062 0.75\n",
      "960 1.1470612 0.75\n",
      "961 1.1470686 0.75\n",
      "962 1.14706 0.75\n",
      "963 1.1470628 0.75\n",
      "964 1.1470627 0.75\n",
      "965 1.1470588 0.75\n",
      "966 1.1470628 0.75\n",
      "967 1.1470628 0.75\n",
      "968 1.1470567 0.75\n",
      "969 1.1470611 0.75\n",
      "970 1.1470572 0.75\n",
      "971 1.1470528 0.75\n",
      "972 1.1470561 0.75\n",
      "973 1.1470543 0.75\n",
      "974 1.1470537 0.75\n",
      "975 1.1470504 0.75\n",
      "976 1.1470543 0.75\n",
      "977 1.1470567 0.75\n",
      "978 1.1470464 0.75\n",
      "979 1.1470535 0.75\n",
      "980 1.1470491 0.75\n",
      "981 1.1470515 0.75\n",
      "982 1.1470519 0.75\n",
      "983 1.1470431 0.75\n",
      "984 1.1470437 0.75\n",
      "985 1.1470454 0.75\n",
      "986 1.1470455 0.75\n",
      "987 1.1470476 0.75\n",
      "988 1.1470467 0.75\n",
      "989 1.1470469 0.75\n",
      "990 1.1470407 0.75\n",
      "991 1.1470453 0.75\n",
      "992 1.1470431 0.75\n",
      "993 1.1470453 0.75\n",
      "994 1.1470348 0.75\n",
      "995 1.1470418 0.75\n",
      "996 1.1470386 0.75\n",
      "997 1.1470329 0.75\n",
      "998 1.1470402 0.75\n",
      "999 1.1470338 0.75\n"
     ]
    }
   ],
   "source": [
    "x_data=[[1,2,1,1],\n",
    "        [2,1,3,2],\n",
    "        [3,1,3,4],\n",
    "        [4,1,5,5],\n",
    "        [1,7,5,5],\n",
    "        [1,2,5,6],\n",
    "        [1,6,6,6],\n",
    "        [1,7,7,7]]\n",
    "\n",
    "y_data=[[0,0,1],\n",
    "        [0,0,1],\n",
    "        [0,0,1],\n",
    "        [0,1,0],\n",
    "        [0,1,0],\n",
    "        [0,1,0],\n",
    "        [1,0,0],\n",
    "        [1,0,0]]\n",
    "\n",
    "X=tf.placeholder('float',[None,4])\n",
    "Y=tf.placeholder('float',[None,3])\n",
    "W=tf.Variable(tf.random_normal([4,3]), name='weight')\n",
    "b=tf.Variable(tf.random_normal([3]), name='bias')\n",
    "\n",
    "hypothesis = tf.sigmoid(tf.matmul(X, W) + b)\n",
    "cost=-tf.reduce_mean(Y*tf.log(hypothesis)+(1-Y)*tf.log(1-hypothesis))\n",
    "\n",
    "train=tf.train.GradientDescentOptimizer(learning_rate=0.000001).minimize(cost)\n",
    "predicted=tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "\n",
    "accuracy=tf.reduce_mean(tf.cast(tf.equal(predicted,Y), dtype=tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(1000):\n",
    "        cos,_,acc = sess.run([cost,train,accuracy], feed_dict={X:x_data, Y:y_data})\n",
    "        if step %100:\n",
    "            print(step,cos,acc)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.9829608\n",
      "2 0.9800156\n",
      "3 0.9770781\n",
      "4 0.9741488\n",
      "5 0.9712274\n",
      "6 0.96831423\n",
      "7 0.9654093\n",
      "8 0.9625124\n",
      "9 0.959624\n",
      "10 0.9567437\n",
      "11 0.953872\n",
      "12 0.9510087\n",
      "13 0.9481539\n",
      "14 0.9453077\n",
      "15 0.9424701\n",
      "16 0.93964124\n",
      "17 0.93682086\n",
      "18 0.93400955\n",
      "19 0.93120694\n",
      "20 0.92841315\n",
      "21 0.9256283\n",
      "22 0.92285264\n",
      "23 0.9200859\n",
      "24 0.91732824\n",
      "25 0.9145798\n",
      "26 0.91184044\n",
      "27 0.90911037\n",
      "28 0.90638953\n",
      "29 0.90367806\n",
      "30 0.900976\n",
      "31 0.89828336\n",
      "32 0.89560026\n",
      "33 0.8929267\n",
      "34 0.8902627\n",
      "35 0.88760823\n",
      "36 0.88496363\n",
      "37 0.88232857\n",
      "38 0.8797035\n",
      "39 0.87708807\n",
      "40 0.87448263\n",
      "41 0.8718871\n",
      "42 0.86930156\n",
      "43 0.86672586\n",
      "44 0.86416036\n",
      "45 0.86160475\n",
      "46 0.8590594\n",
      "47 0.8565242\n",
      "48 0.85399926\n",
      "49 0.8514844\n",
      "50 0.84898\n",
      "51 0.84648573\n",
      "52 0.8440021\n",
      "53 0.8415287\n",
      "54 0.8390658\n",
      "55 0.8366132\n",
      "56 0.83417124\n",
      "57 0.83173984\n",
      "58 0.8293189\n",
      "59 0.82690847\n",
      "60 0.8245088\n",
      "61 0.82211983\n",
      "62 0.8197415\n",
      "63 0.81737375\n",
      "64 0.815017\n",
      "65 0.8126709\n",
      "66 0.8103355\n",
      "67 0.80801123\n",
      "68 0.8056976\n",
      "69 0.80339485\n",
      "70 0.801103\n",
      "71 0.7988221\n",
      "72 0.7965522\n",
      "73 0.7942932\n",
      "74 0.79204524\n",
      "75 0.7898083\n",
      "76 0.7875823\n",
      "77 0.78536725\n",
      "78 0.78316337\n",
      "79 0.7809706\n",
      "80 0.7787887\n",
      "81 0.7766181\n",
      "82 0.7744585\n",
      "83 0.77231014\n",
      "84 0.7701728\n",
      "85 0.76804674\n",
      "86 0.76593184\n",
      "87 0.7638278\n",
      "88 0.7617351\n",
      "89 0.75965357\n",
      "90 0.7575832\n",
      "91 0.75552404\n",
      "92 0.7534759\n",
      "93 0.7514391\n",
      "94 0.74941325\n",
      "95 0.74739885\n",
      "96 0.74539554\n",
      "97 0.74340343\n",
      "98 0.7414225\n",
      "99 0.7394527\n",
      "101 0.7355466\n",
      "102 0.73361015\n",
      "103 0.7316851\n",
      "104 0.7297711\n",
      "105 0.7278682\n",
      "106 0.7259764\n",
      "107 0.72409564\n",
      "108 0.72222614\n",
      "109 0.7203676\n",
      "110 0.71852016\n",
      "111 0.71668375\n",
      "112 0.71485835\n",
      "113 0.71304405\n",
      "114 0.71124053\n",
      "115 0.7094483\n",
      "116 0.7076669\n",
      "117 0.7058964\n",
      "118 0.7041367\n",
      "119 0.70238817\n",
      "120 0.70065033\n",
      "121 0.69892347\n",
      "122 0.69720745\n",
      "123 0.6955021\n",
      "124 0.69380766\n",
      "125 0.6921239\n",
      "126 0.6904509\n",
      "127 0.68878853\n",
      "128 0.6871369\n",
      "129 0.685496\n",
      "130 0.68386555\n",
      "131 0.6822458\n",
      "132 0.68063647\n",
      "133 0.6790378\n",
      "134 0.6774495\n",
      "135 0.67587173\n",
      "136 0.6743044\n",
      "137 0.6727474\n",
      "138 0.67120063\n",
      "139 0.6696643\n",
      "140 0.66813827\n",
      "141 0.6666224\n",
      "142 0.6651168\n",
      "143 0.66362137\n",
      "144 0.662136\n",
      "145 0.6606608\n",
      "146 0.65919554\n",
      "147 0.65774035\n",
      "148 0.6562951\n",
      "149 0.6548598\n",
      "150 0.6534343\n",
      "151 0.65201867\n",
      "152 0.65061283\n",
      "153 0.6492168\n",
      "154 0.64783037\n",
      "155 0.6464536\n",
      "156 0.6450865\n",
      "157 0.6437291\n",
      "158 0.642381\n",
      "159 0.64104253\n",
      "160 0.63971347\n",
      "161 0.63839376\n",
      "162 0.63708335\n",
      "163 0.63578224\n",
      "164 0.6344905\n",
      "165 0.6332078\n",
      "166 0.6319343\n",
      "167 0.6306699\n",
      "168 0.62941444\n",
      "169 0.6281681\n",
      "170 0.62693065\n",
      "171 0.6257022\n",
      "172 0.62448245\n",
      "173 0.6232716\n",
      "174 0.6220694\n",
      "175 0.62087595\n",
      "176 0.61969113\n",
      "177 0.6185149\n",
      "178 0.6173471\n",
      "179 0.61618793\n",
      "180 0.6150371\n",
      "181 0.6138947\n",
      "182 0.61276054\n",
      "183 0.61163485\n",
      "184 0.61051714\n",
      "185 0.6094077\n",
      "186 0.6083064\n",
      "187 0.60721326\n",
      "188 0.6061279\n",
      "189 0.6050505\n",
      "190 0.6039811\n",
      "191 0.60291946\n",
      "192 0.60186565\n",
      "193 0.6008196\n",
      "194 0.59978116\n",
      "195 0.5987505\n",
      "196 0.5977273\n",
      "197 0.5967115\n",
      "198 0.59570336\n",
      "199 0.5947025\n",
      "201 0.5927228\n",
      "202 0.5917438\n",
      "203 0.59077203\n",
      "204 0.5898074\n",
      "205 0.5888497\n",
      "206 0.5878991\n",
      "207 0.5869555\n",
      "208 0.5860188\n",
      "209 0.58508897\n",
      "210 0.5841659\n",
      "211 0.58324957\n",
      "212 0.58234\n",
      "213 0.581437\n",
      "214 0.5805406\n",
      "215 0.57965064\n",
      "216 0.5787672\n",
      "217 0.5778902\n",
      "218 0.57701945\n",
      "219 0.5761552\n",
      "220 0.575297\n",
      "221 0.5744451\n",
      "222 0.5735993\n",
      "223 0.57275957\n",
      "224 0.5719259\n",
      "225 0.5710982\n",
      "226 0.5702765\n",
      "227 0.5694606\n",
      "228 0.5686505\n",
      "229 0.5678462\n",
      "230 0.56704754\n",
      "231 0.56625456\n",
      "232 0.5654672\n",
      "233 0.56468546\n",
      "234 0.5639092\n",
      "235 0.56313837\n",
      "236 0.56237286\n",
      "237 0.5616127\n",
      "238 0.560858\n",
      "239 0.56010836\n",
      "240 0.5593641\n",
      "241 0.5586248\n",
      "242 0.55789065\n",
      "243 0.55716157\n",
      "244 0.5564375\n",
      "245 0.55571836\n",
      "246 0.5550041\n",
      "247 0.5542947\n",
      "248 0.55359\n",
      "249 0.55289006\n",
      "250 0.5521949\n",
      "251 0.55150425\n",
      "252 0.5508183\n",
      "253 0.5501369\n",
      "254 0.54945993\n",
      "255 0.5487875\n",
      "256 0.5481194\n",
      "257 0.5474557\n",
      "258 0.5467964\n",
      "259 0.5461412\n",
      "260 0.5454904\n",
      "261 0.5448437\n",
      "262 0.544201\n",
      "263 0.5435626\n",
      "264 0.5429281\n",
      "265 0.5422976\n",
      "266 0.54167116\n",
      "267 0.5410485\n",
      "268 0.54042983\n",
      "269 0.5398149\n",
      "270 0.53920376\n",
      "271 0.5385964\n",
      "272 0.5379927\n",
      "273 0.5373927\n",
      "274 0.5367962\n",
      "275 0.5362034\n",
      "276 0.53561413\n",
      "277 0.5350284\n",
      "278 0.53444606\n",
      "279 0.53386706\n",
      "280 0.5332915\n",
      "281 0.53271925\n",
      "282 0.5321503\n",
      "283 0.5315846\n",
      "284 0.5310222\n",
      "285 0.5304629\n",
      "286 0.5299068\n",
      "287 0.52935374\n",
      "288 0.5288038\n",
      "289 0.52825695\n",
      "290 0.52771306\n",
      "291 0.52717215\n",
      "292 0.52663416\n",
      "293 0.52609897\n",
      "294 0.5255667\n",
      "295 0.5250373\n",
      "296 0.5245106\n",
      "297 0.5239868\n",
      "298 0.52346563\n",
      "299 0.52294713\n",
      "301 0.5219182\n",
      "302 0.52140766\n",
      "303 0.5208996\n",
      "304 0.5203941\n",
      "305 0.51989114\n",
      "306 0.51939064\n",
      "307 0.5188925\n",
      "308 0.5183969\n",
      "309 0.5179037\n",
      "310 0.5174128\n",
      "311 0.51692426\n",
      "312 0.51643795\n",
      "313 0.51595396\n",
      "314 0.51547223\n",
      "315 0.5149927\n",
      "316 0.51451534\n",
      "317 0.5140401\n",
      "318 0.5135671\n",
      "319 0.5130962\n",
      "320 0.51262736\n",
      "321 0.5121606\n",
      "322 0.5116958\n",
      "323 0.5112331\n",
      "324 0.51077235\n",
      "325 0.5103135\n",
      "326 0.5098566\n",
      "327 0.5094017\n",
      "328 0.5089486\n",
      "329 0.50849736\n",
      "330 0.50804806\n",
      "331 0.50760055\n",
      "332 0.5071548\n",
      "333 0.5067109\n",
      "334 0.5062686\n",
      "335 0.50582814\n",
      "336 0.50538945\n",
      "337 0.5049523\n",
      "338 0.50451696\n",
      "339 0.5040833\n",
      "340 0.50365114\n",
      "341 0.50322074\n",
      "342 0.5027919\n",
      "343 0.5023645\n",
      "344 0.5019388\n",
      "345 0.5015147\n",
      "346 0.50109196\n",
      "347 0.5006708\n",
      "348 0.50025105\n",
      "349 0.4998328\n",
      "350 0.49941605\n",
      "351 0.49900073\n",
      "352 0.49858677\n",
      "353 0.49817428\n",
      "354 0.4977631\n",
      "355 0.4973533\n",
      "356 0.49694493\n",
      "357 0.49653777\n",
      "358 0.49613208\n",
      "359 0.49572763\n",
      "360 0.49532437\n",
      "361 0.49492255\n",
      "362 0.49452195\n",
      "363 0.49412262\n",
      "364 0.4937245\n",
      "365 0.49332765\n",
      "366 0.4929319\n",
      "367 0.49253735\n",
      "368 0.49214405\n",
      "369 0.4917519\n",
      "370 0.49136093\n",
      "371 0.49097103\n",
      "372 0.49058235\n",
      "373 0.49019468\n",
      "374 0.4898082\n",
      "375 0.4894229\n",
      "376 0.48903853\n",
      "377 0.48865533\n",
      "378 0.4882732\n",
      "379 0.4878921\n",
      "380 0.48751202\n",
      "381 0.48713294\n",
      "382 0.48675498\n",
      "383 0.48637795\n",
      "384 0.4860019\n",
      "385 0.48562685\n",
      "386 0.4852529\n",
      "387 0.48487982\n",
      "388 0.48450777\n",
      "389 0.48413658\n",
      "390 0.48376632\n",
      "391 0.48339707\n",
      "392 0.48302874\n",
      "393 0.4826613\n",
      "394 0.48229474\n",
      "395 0.48192915\n",
      "396 0.48156434\n",
      "397 0.4812006\n",
      "398 0.48083758\n",
      "399 0.48047554\n",
      "401 0.4797539\n",
      "402 0.4793944\n",
      "403 0.47903576\n",
      "404 0.47867796\n",
      "405 0.4783209\n",
      "406 0.47796467\n",
      "407 0.47760934\n",
      "408 0.47725475\n",
      "409 0.47690096\n",
      "410 0.47654793\n",
      "411 0.47619572\n",
      "412 0.47584435\n",
      "413 0.47549373\n",
      "414 0.47514382\n",
      "415 0.47479475\n",
      "416 0.4744464\n",
      "417 0.47409883\n",
      "418 0.47375202\n",
      "419 0.47340584\n",
      "420 0.4730605\n",
      "421 0.47271585\n",
      "422 0.472372\n",
      "423 0.4720288\n",
      "424 0.4716863\n",
      "425 0.4713446\n",
      "426 0.4710035\n",
      "427 0.47066328\n",
      "428 0.47032365\n",
      "429 0.46998468\n",
      "430 0.46964642\n",
      "431 0.4693089\n",
      "432 0.46897203\n",
      "433 0.46863586\n",
      "434 0.4683003\n",
      "435 0.4679655\n",
      "436 0.4676313\n",
      "437 0.4672978\n",
      "438 0.46696493\n",
      "439 0.46663278\n",
      "440 0.46630126\n",
      "441 0.46597034\n",
      "442 0.46564016\n",
      "443 0.46531054\n",
      "444 0.46498165\n",
      "445 0.46465334\n",
      "446 0.46432573\n",
      "447 0.4639987\n",
      "448 0.4636723\n",
      "449 0.46334654\n",
      "450 0.4630214\n",
      "451 0.46269685\n",
      "452 0.46237296\n",
      "453 0.4620497\n",
      "454 0.46172702\n",
      "455 0.46140498\n",
      "456 0.4610836\n",
      "457 0.46076277\n",
      "458 0.4604426\n",
      "459 0.46012297\n",
      "460 0.45980403\n",
      "461 0.45948565\n",
      "462 0.45916784\n",
      "463 0.45885062\n",
      "464 0.458534\n",
      "465 0.45821807\n",
      "466 0.4579026\n",
      "467 0.4575878\n",
      "468 0.45727354\n",
      "469 0.45695987\n",
      "470 0.4566469\n",
      "471 0.4563344\n",
      "472 0.4560225\n",
      "473 0.45571125\n",
      "474 0.45540053\n",
      "475 0.45509034\n",
      "476 0.45478082\n",
      "477 0.4544719\n",
      "478 0.45416343\n",
      "479 0.45385557\n",
      "480 0.45354834\n",
      "481 0.4532417\n",
      "482 0.45293558\n",
      "483 0.45263007\n",
      "484 0.4523251\n",
      "485 0.4520208\n",
      "486 0.45171693\n",
      "487 0.45141366\n",
      "488 0.451111\n",
      "489 0.45080894\n",
      "490 0.4505074\n",
      "491 0.45020634\n",
      "492 0.44990593\n",
      "493 0.44960606\n",
      "494 0.44930676\n",
      "495 0.44900805\n",
      "496 0.44870982\n",
      "497 0.4484123\n",
      "498 0.4481152\n",
      "499 0.44781876\n",
      "501 0.44722742\n",
      "502 0.44693267\n",
      "503 0.44663844\n",
      "504 0.44634482\n",
      "505 0.44605172\n",
      "506 0.44575918\n",
      "507 0.44546717\n",
      "508 0.4451757\n",
      "509 0.4448848\n",
      "510 0.44459444\n",
      "511 0.4443047\n",
      "512 0.4440155\n",
      "513 0.44372687\n",
      "514 0.44343877\n",
      "515 0.4431512\n",
      "516 0.4428642\n",
      "517 0.44257778\n",
      "518 0.442292\n",
      "519 0.44200668\n",
      "520 0.441722\n",
      "521 0.44143784\n",
      "522 0.44115424\n",
      "523 0.44087112\n",
      "524 0.44058865\n",
      "525 0.44030672\n",
      "526 0.44002533\n",
      "527 0.4397445\n",
      "528 0.4394643\n",
      "529 0.4391846\n",
      "530 0.43890545\n",
      "531 0.43862683\n",
      "532 0.4383488\n",
      "533 0.43807134\n",
      "534 0.43779445\n",
      "535 0.43751806\n",
      "536 0.43724227\n",
      "537 0.43696705\n",
      "538 0.43669233\n",
      "539 0.4364183\n",
      "540 0.4361447\n",
      "541 0.43587172\n",
      "542 0.4355994\n",
      "543 0.43532753\n",
      "544 0.43505624\n",
      "545 0.43478543\n",
      "546 0.4345153\n",
      "547 0.43424574\n",
      "548 0.43397668\n",
      "549 0.4337082\n",
      "550 0.4334403\n",
      "551 0.43317294\n",
      "552 0.43290612\n",
      "553 0.4326399\n",
      "554 0.4323743\n",
      "555 0.43210918\n",
      "556 0.43184465\n",
      "557 0.43158072\n",
      "558 0.4313174\n",
      "559 0.43105456\n",
      "560 0.4307923\n",
      "561 0.4305306\n",
      "562 0.43026954\n",
      "563 0.430009\n",
      "564 0.42974904\n",
      "565 0.42948967\n",
      "566 0.42923087\n",
      "567 0.42897263\n",
      "568 0.42871493\n",
      "569 0.42845783\n",
      "570 0.42820132\n",
      "571 0.4279453\n",
      "572 0.4276899\n",
      "573 0.42743516\n",
      "574 0.4271809\n",
      "575 0.42692727\n",
      "576 0.4266742\n",
      "577 0.4264217\n",
      "578 0.42616972\n",
      "579 0.4259184\n",
      "580 0.4256676\n",
      "581 0.42541742\n",
      "582 0.4251678\n",
      "583 0.4249187\n",
      "584 0.42467025\n",
      "585 0.42442238\n",
      "586 0.42417508\n",
      "587 0.4239283\n",
      "588 0.4236822\n",
      "589 0.42343664\n",
      "590 0.4231916\n",
      "591 0.4229472\n",
      "592 0.4227034\n",
      "593 0.42246014\n",
      "594 0.42221746\n",
      "595 0.42197537\n",
      "596 0.42173392\n",
      "597 0.42149302\n",
      "598 0.42125267\n",
      "599 0.42101297\n",
      "601 0.42053527\n",
      "602 0.42029727\n",
      "603 0.42005986\n",
      "604 0.419823\n",
      "605 0.4195868\n",
      "606 0.4193512\n",
      "607 0.41911608\n",
      "608 0.41888165\n",
      "609 0.41864777\n",
      "610 0.41841447\n",
      "611 0.4181818\n",
      "612 0.4179497\n",
      "613 0.41771817\n",
      "614 0.41748726\n",
      "615 0.41725683\n",
      "616 0.41702712\n",
      "617 0.41679794\n",
      "618 0.4165693\n",
      "619 0.41634136\n",
      "620 0.41611397\n",
      "621 0.4158871\n",
      "622 0.41566092\n",
      "623 0.41543525\n",
      "624 0.41521025\n",
      "625 0.41498578\n",
      "626 0.4147619\n",
      "627 0.41453868\n",
      "628 0.41431594\n",
      "629 0.41409388\n",
      "630 0.41387233\n",
      "631 0.4136515\n",
      "632 0.41343117\n",
      "633 0.41321144\n",
      "634 0.4129923\n",
      "635 0.41277376\n",
      "636 0.4125558\n",
      "637 0.4123385\n",
      "638 0.41212165\n",
      "639 0.41190556\n",
      "640 0.41169\n",
      "641 0.41147503\n",
      "642 0.4112606\n",
      "643 0.41104683\n",
      "644 0.41083366\n",
      "645 0.41062105\n",
      "646 0.410409\n",
      "647 0.41019762\n",
      "648 0.4099868\n",
      "649 0.40977657\n",
      "650 0.40956688\n",
      "651 0.40935785\n",
      "652 0.4091493\n",
      "653 0.4089415\n",
      "654 0.40873417\n",
      "655 0.4085275\n",
      "656 0.40832144\n",
      "657 0.40811598\n",
      "658 0.407911\n",
      "659 0.40770668\n",
      "660 0.40750292\n",
      "661 0.4072998\n",
      "662 0.4070972\n",
      "663 0.40689525\n",
      "664 0.4066939\n",
      "665 0.4064931\n",
      "666 0.40629292\n",
      "667 0.4060933\n",
      "668 0.40589428\n",
      "669 0.40569586\n",
      "670 0.405498\n",
      "671 0.4053008\n",
      "672 0.40510407\n",
      "673 0.40490806\n",
      "674 0.40471256\n",
      "675 0.40451768\n",
      "676 0.40432328\n",
      "677 0.40412968\n",
      "678 0.40393654\n",
      "679 0.40374395\n",
      "680 0.403552\n",
      "681 0.4033606\n",
      "682 0.40316975\n",
      "683 0.40297955\n",
      "684 0.40278992\n",
      "685 0.40260088\n",
      "686 0.40241235\n",
      "687 0.40222442\n",
      "688 0.4020371\n",
      "689 0.4018504\n",
      "690 0.40166423\n",
      "691 0.40147865\n",
      "692 0.40129367\n",
      "693 0.40110922\n",
      "694 0.40092546\n",
      "695 0.40074217\n",
      "696 0.40055948\n",
      "697 0.4003774\n",
      "698 0.4001958\n",
      "699 0.40001488\n",
      "701 0.39965463\n",
      "702 0.3994754\n",
      "703 0.3992967\n",
      "704 0.39911866\n",
      "705 0.3989411\n",
      "706 0.39876422\n",
      "707 0.39858776\n",
      "708 0.39841193\n",
      "709 0.3982367\n",
      "710 0.398062\n",
      "711 0.39788786\n",
      "712 0.39771432\n",
      "713 0.3975413\n",
      "714 0.39736885\n",
      "715 0.39719704\n",
      "716 0.3970257\n",
      "717 0.39685503\n",
      "718 0.39668483\n",
      "719 0.39651516\n",
      "720 0.39634606\n",
      "721 0.3961776\n",
      "722 0.39600962\n",
      "723 0.39584225\n",
      "724 0.3956754\n",
      "725 0.39550906\n",
      "726 0.39534336\n",
      "727 0.39517814\n",
      "728 0.3950135\n",
      "729 0.39484945\n",
      "730 0.39468592\n",
      "731 0.39452294\n",
      "732 0.39436054\n",
      "733 0.39419863\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "734 0.3940373\n",
      "735 0.39387655\n",
      "736 0.3937162\n",
      "737 0.39355654\n",
      "738 0.3933973\n",
      "739 0.39323866\n",
      "740 0.3930806\n",
      "741 0.39292303\n",
      "742 0.39276603\n",
      "743 0.39260954\n",
      "744 0.39245355\n",
      "745 0.3922982\n",
      "746 0.39214322\n",
      "747 0.39198887\n",
      "748 0.3918351\n",
      "749 0.3916818\n",
      "750 0.39152902\n",
      "751 0.39137676\n",
      "752 0.39122507\n",
      "753 0.39107388\n",
      "754 0.39092317\n",
      "755 0.39077303\n",
      "756 0.3906234\n",
      "757 0.39047432\n",
      "758 0.3903257\n",
      "759 0.3901776\n",
      "760 0.39003\n",
      "761 0.38988298\n",
      "762 0.3897365\n",
      "763 0.38959044\n",
      "764 0.38944486\n",
      "765 0.38929987\n",
      "766 0.3891554\n",
      "767 0.38901138\n",
      "768 0.3888679\n",
      "769 0.38872498\n",
      "770 0.3885825\n",
      "771 0.3884405\n",
      "772 0.38829893\n",
      "773 0.38815802\n",
      "774 0.38801757\n",
      "775 0.38787752\n",
      "776 0.387738\n",
      "777 0.38759902\n",
      "778 0.38746047\n",
      "779 0.38732246\n",
      "780 0.38718498\n",
      "781 0.38704783\n",
      "782 0.38691133\n",
      "783 0.38677526\n",
      "784 0.38663965\n",
      "785 0.38650453\n",
      "786 0.38636994\n",
      "787 0.38623577\n",
      "788 0.38610205\n",
      "789 0.38596892\n",
      "790 0.38583612\n",
      "791 0.38570386\n",
      "792 0.38557205\n",
      "793 0.38544074\n",
      "794 0.38530985\n",
      "795 0.38517955\n",
      "796 0.38504958\n",
      "797 0.38492015\n",
      "798 0.38479114\n",
      "799 0.38466263\n",
      "801 0.3844069\n",
      "802 0.38427976\n",
      "803 0.384153\n",
      "804 0.38402677\n",
      "805 0.38390097\n",
      "806 0.38377562\n",
      "807 0.38365072\n",
      "808 0.3835263\n",
      "809 0.38340223\n",
      "810 0.38327867\n",
      "811 0.38315552\n",
      "812 0.38303292\n",
      "813 0.38291064\n",
      "814 0.38278884\n",
      "815 0.38266748\n",
      "816 0.38254654\n",
      "817 0.38242602\n",
      "818 0.38230592\n",
      "819 0.3821863\n",
      "820 0.38206708\n",
      "821 0.38194823\n",
      "822 0.3818299\n",
      "823 0.381712\n",
      "824 0.38159448\n",
      "825 0.38147736\n",
      "826 0.38136065\n",
      "827 0.38124442\n",
      "828 0.38112855\n",
      "829 0.3810131\n",
      "830 0.38089812\n",
      "831 0.3807835\n",
      "832 0.3806693\n",
      "833 0.38055542\n",
      "834 0.38044205\n",
      "835 0.38032913\n",
      "836 0.38021648\n",
      "837 0.3801043\n",
      "838 0.37999254\n",
      "839 0.37988108\n",
      "840 0.3797701\n",
      "841 0.37965953\n",
      "842 0.37954932\n",
      "843 0.37943944\n",
      "844 0.37933004\n",
      "845 0.37922105\n",
      "846 0.37911236\n",
      "847 0.37900412\n",
      "848 0.3788962\n",
      "849 0.37878874\n",
      "850 0.3786816\n",
      "851 0.3785749\n",
      "852 0.3784685\n",
      "853 0.37836245\n",
      "854 0.37825686\n",
      "855 0.3781516\n",
      "856 0.3780467\n",
      "857 0.3779422\n",
      "858 0.37783805\n",
      "859 0.3777343\n",
      "860 0.37763086\n",
      "861 0.3775278\n",
      "862 0.37742507\n",
      "863 0.3773228\n",
      "864 0.37722075\n",
      "865 0.37711912\n",
      "866 0.3770179\n",
      "867 0.37691692\n",
      "868 0.3768164\n",
      "869 0.3767162\n",
      "870 0.37661627\n",
      "871 0.37651673\n",
      "872 0.37641758\n",
      "873 0.37631872\n",
      "874 0.37622026\n",
      "875 0.37612203\n",
      "876 0.3760242\n",
      "877 0.37592673\n",
      "878 0.37582964\n",
      "879 0.37573278\n",
      "880 0.37563628\n",
      "881 0.37554014\n",
      "882 0.37544432\n",
      "883 0.37534875\n",
      "884 0.37525362\n",
      "885 0.37515885\n",
      "886 0.37506425\n",
      "887 0.37497005\n",
      "888 0.37487617\n",
      "889 0.3747826\n",
      "890 0.37468934\n",
      "891 0.37459636\n",
      "892 0.37450373\n",
      "893 0.37441134\n",
      "894 0.37431937\n",
      "895 0.3742277\n",
      "896 0.3741363\n",
      "897 0.3740452\n",
      "898 0.3739544\n",
      "899 0.37386388\n",
      "901 0.3736838\n",
      "902 0.37359422\n",
      "903 0.37350494\n",
      "904 0.37341595\n",
      "905 0.3733272\n",
      "906 0.37323883\n",
      "907 0.3731506\n",
      "908 0.37306282\n",
      "909 0.37297523\n",
      "910 0.37288797\n",
      "911 0.372801\n",
      "912 0.37271428\n",
      "913 0.37262785\n",
      "914 0.37254173\n",
      "915 0.37245584\n",
      "916 0.37237024\n",
      "917 0.3722849\n",
      "918 0.37219992\n",
      "919 0.37211516\n",
      "920 0.3720306\n",
      "921 0.3719464\n",
      "922 0.37186253\n",
      "923 0.3717788\n",
      "924 0.37169537\n",
      "925 0.37161216\n",
      "926 0.37152922\n",
      "927 0.3714466\n",
      "928 0.37136418\n",
      "929 0.37128216\n",
      "930 0.37120023\n",
      "931 0.37111866\n",
      "932 0.37103724\n",
      "933 0.37095612\n",
      "934 0.37087524\n",
      "935 0.37079468\n",
      "936 0.3707143\n",
      "937 0.3706342\n",
      "938 0.37055433\n",
      "939 0.37047467\n",
      "940 0.3703953\n",
      "941 0.37031615\n",
      "942 0.37023723\n",
      "943 0.37015855\n",
      "944 0.37008017\n",
      "945 0.37000197\n",
      "946 0.369924\n",
      "947 0.36984622\n",
      "948 0.36976877\n",
      "949 0.36969152\n",
      "950 0.36961445\n",
      "951 0.3695377\n",
      "952 0.36946112\n",
      "953 0.36938477\n",
      "954 0.36930865\n",
      "955 0.36923274\n",
      "956 0.36915702\n",
      "957 0.36908165\n",
      "958 0.3690064\n",
      "959 0.3689313\n",
      "960 0.3688566\n",
      "961 0.36878192\n",
      "962 0.36870754\n",
      "963 0.3686334\n",
      "964 0.36855936\n",
      "965 0.36848563\n",
      "966 0.36841214\n",
      "967 0.36833882\n",
      "968 0.36826563\n",
      "969 0.36819276\n",
      "970 0.36812004\n",
      "971 0.36804754\n",
      "972 0.36797523\n",
      "973 0.36790305\n",
      "974 0.3678312\n",
      "975 0.36775953\n",
      "976 0.36768794\n",
      "977 0.3676167\n",
      "978 0.36754557\n",
      "979 0.36747465\n",
      "980 0.36740392\n",
      "981 0.3673333\n",
      "982 0.367263\n",
      "983 0.36719283\n",
      "984 0.36712283\n",
      "985 0.36705303\n",
      "986 0.36698347\n",
      "987 0.3669141\n",
      "988 0.36684483\n",
      "989 0.36677578\n",
      "990 0.36670682\n",
      "991 0.3666382\n",
      "992 0.36656967\n",
      "993 0.36650133\n",
      "994 0.3664332\n",
      "995 0.36636525\n",
      "996 0.36629745\n",
      "997 0.36622977\n",
      "998 0.36616233\n",
      "999 0.366095\n",
      "[[0.19802132 0.6606316  0.14134708]] [1]\n"
     ]
    }
   ],
   "source": [
    "x_data=[[1,2,1,1],\n",
    "        [2,1,3,2],\n",
    "        [3,1,3,4],\n",
    "        [4,1,5,5],\n",
    "        [1,7,5,5],\n",
    "        [1,2,5,6],\n",
    "        [1,6,6,6],\n",
    "        [1,7,7,7]]\n",
    "\n",
    "y_data=[[0,0,1],\n",
    "        [0,0,1],\n",
    "        [0,0,1],\n",
    "        [0,1,0],\n",
    "        [0,1,0],\n",
    "        [0,1,0],\n",
    "        [1,0,0],\n",
    "        [1,0,0]]\n",
    "\n",
    "X=tf.placeholder('float',[None,4])\n",
    "Y=tf.placeholder('float',[None,3])\n",
    "W=tf.Variable(tf.random_normal([4,3]), name='weight')\n",
    "b=tf.Variable(tf.random_normal([3]), name='bias')\n",
    "\n",
    "hypothesis = tf.nn.softmax(tf.matmul(X, W) + b)    #소프트맥스 -> [0.1, 0.8, 0.1] 속성모두 더했을 때 합이 1인 소수값으로 hypothesis(y_hat)를 설정\n",
    "cost=tf.reduce_mean(-tf.reduce_mean(Y*tf.log(hypothesis),axis=1))  \n",
    "\n",
    "optimizer=tf.train.GradientDescentOptimizer(learning_rate=0.001).minimize(cost)\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(1000):\n",
    "        sess.run(optimizer, feed_dict={X:x_data, Y:y_data})\n",
    "        if step %100:\n",
    "            print(step,sess.run(cost, feed_dict={X:x_data,Y:y_data}))\n",
    "        \n",
    "    a=sess.run(hypothesis, feed_dict={X:[[1,11,7,9]]})\n",
    "    print(a,sess.run(tf.argmax(a,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.7470941\n",
      "2 0.7469479\n",
      "3 0.7468015\n",
      "4 0.74665517\n",
      "5 0.74650884\n",
      "6 0.74636257\n",
      "7 0.7462165\n",
      "8 0.74607027\n",
      "9 0.7459242\n",
      "10 0.7457781\n",
      "11 0.74563193\n",
      "12 0.7454859\n",
      "13 0.7453399\n",
      "14 0.74519384\n",
      "15 0.74504787\n",
      "16 0.74490196\n",
      "17 0.7447561\n",
      "18 0.7446102\n",
      "19 0.7444644\n",
      "20 0.74431854\n",
      "21 0.7441728\n",
      "22 0.744027\n",
      "23 0.7438813\n",
      "24 0.7437357\n",
      "25 0.74358994\n",
      "26 0.7434443\n",
      "27 0.74329877\n",
      "28 0.7431532\n",
      "29 0.74300754\n",
      "30 0.7428621\n",
      "31 0.74271667\n",
      "32 0.7425712\n",
      "33 0.7424256\n",
      "34 0.7422802\n",
      "35 0.7421349\n",
      "36 0.7419895\n",
      "37 0.74184424\n",
      "38 0.741699\n",
      "39 0.7415537\n",
      "40 0.74140847\n",
      "41 0.7412634\n",
      "42 0.74111825\n",
      "43 0.7409731\n",
      "44 0.74082804\n",
      "45 0.7406829\n",
      "46 0.74053806\n",
      "47 0.7403931\n",
      "48 0.74024814\n",
      "49 0.7401032\n",
      "50 0.7399582\n",
      "51 0.7398134\n",
      "52 0.7396685\n",
      "53 0.7395237\n",
      "54 0.7393788\n",
      "55 0.73923427\n",
      "56 0.7390894\n",
      "57 0.73894477\n",
      "58 0.7388001\n",
      "59 0.73865545\n",
      "60 0.7385109\n",
      "61 0.73836625\n",
      "62 0.7382217\n",
      "63 0.73807716\n",
      "64 0.73793274\n",
      "65 0.7377882\n",
      "66 0.7376438\n",
      "67 0.7374994\n",
      "68 0.737355\n",
      "69 0.73721063\n",
      "70 0.73706627\n",
      "71 0.736922\n",
      "72 0.7367778\n",
      "73 0.7366336\n",
      "74 0.7364893\n",
      "75 0.7363453\n",
      "76 0.73620105\n",
      "77 0.73605704\n",
      "78 0.7359128\n",
      "79 0.73576885\n",
      "80 0.7356248\n",
      "81 0.73548067\n",
      "82 0.73533684\n",
      "83 0.73519295\n",
      "84 0.73504895\n",
      "85 0.73490506\n",
      "86 0.7347612\n",
      "87 0.73461735\n",
      "88 0.7344735\n",
      "89 0.73432976\n",
      "90 0.734186\n",
      "91 0.73404235\n",
      "92 0.7338986\n",
      "93 0.73375493\n",
      "94 0.7336112\n",
      "95 0.73346764\n",
      "96 0.73332405\n",
      "97 0.7331806\n",
      "98 0.73303705\n",
      "99 0.7328935\n",
      "101 0.7326066\n",
      "102 0.73246324\n",
      "103 0.73231995\n",
      "104 0.7321766\n",
      "105 0.7320333\n",
      "106 0.7318901\n",
      "107 0.7317468\n",
      "108 0.73160356\n",
      "109 0.7314605\n",
      "110 0.7313172\n",
      "111 0.7311741\n",
      "112 0.73103106\n",
      "113 0.730888\n",
      "114 0.73074484\n",
      "115 0.7306019\n",
      "116 0.73045903\n",
      "117 0.730316\n",
      "118 0.730173\n",
      "119 0.7300302\n",
      "120 0.7298873\n",
      "121 0.7297445\n",
      "122 0.72960174\n",
      "123 0.72945887\n",
      "124 0.7293162\n",
      "125 0.7291735\n",
      "126 0.7290308\n",
      "127 0.7288881\n",
      "128 0.72874546\n",
      "129 0.7286029\n",
      "130 0.72846025\n",
      "131 0.7283178\n",
      "132 0.7281752\n",
      "133 0.72803277\n",
      "134 0.7278903\n",
      "135 0.72774786\n",
      "136 0.72760546\n",
      "137 0.7274631\n",
      "138 0.72732085\n",
      "139 0.72717845\n",
      "140 0.7270362\n",
      "141 0.7268939\n",
      "142 0.72675174\n",
      "143 0.72660947\n",
      "144 0.7264673\n",
      "145 0.72632515\n",
      "146 0.72618306\n",
      "147 0.72604096\n",
      "148 0.7258989\n",
      "149 0.7257569\n",
      "150 0.7256148\n",
      "151 0.7254728\n",
      "152 0.72533095\n",
      "153 0.725189\n",
      "154 0.7250471\n",
      "155 0.72490525\n",
      "156 0.7247634\n",
      "157 0.72462153\n",
      "158 0.72447973\n",
      "159 0.72433805\n",
      "160 0.7241963\n",
      "161 0.72405463\n",
      "162 0.7239129\n",
      "163 0.7237713\n",
      "164 0.7236297\n",
      "165 0.7234881\n",
      "166 0.7233465\n",
      "167 0.72320503\n",
      "168 0.7230635\n",
      "169 0.7229221\n",
      "170 0.7227806\n",
      "171 0.7226392\n",
      "172 0.7224978\n",
      "173 0.7223565\n",
      "174 0.7222152\n",
      "175 0.7220739\n",
      "176 0.72193265\n",
      "177 0.7217914\n",
      "178 0.72165024\n",
      "179 0.721509\n",
      "180 0.72136784\n",
      "181 0.72122675\n",
      "182 0.72108567\n",
      "183 0.72094464\n",
      "184 0.7208035\n",
      "185 0.7206625\n",
      "186 0.7205215\n",
      "187 0.72038066\n",
      "188 0.7202397\n",
      "189 0.7200988\n",
      "190 0.71995795\n",
      "191 0.71981704\n",
      "192 0.7196763\n",
      "193 0.7195355\n",
      "194 0.7193948\n",
      "195 0.719254\n",
      "196 0.71911335\n",
      "197 0.7189726\n",
      "198 0.718832\n",
      "199 0.7186914\n",
      "201 0.71841025\n",
      "202 0.71826965\n",
      "203 0.71812916\n",
      "204 0.71798867\n",
      "205 0.71784824\n",
      "206 0.7177077\n",
      "207 0.7175674\n",
      "208 0.717427\n",
      "209 0.71728665\n",
      "210 0.71714634\n",
      "211 0.7170061\n",
      "212 0.7168658\n",
      "213 0.71672565\n",
      "214 0.71658546\n",
      "215 0.71644515\n",
      "216 0.716305\n",
      "217 0.71616495\n",
      "218 0.71602494\n",
      "219 0.71588475\n",
      "220 0.7157448\n",
      "221 0.7156047\n",
      "222 0.7154648\n",
      "223 0.71532476\n",
      "224 0.71518487\n",
      "225 0.71504503\n",
      "226 0.71490514\n",
      "227 0.71476537\n",
      "228 0.71462554\n",
      "229 0.7144857\n",
      "230 0.71434605\n",
      "231 0.7142063\n",
      "232 0.7140666\n",
      "233 0.7139269\n",
      "234 0.7137873\n",
      "235 0.71364766\n",
      "236 0.7135081\n",
      "237 0.71336854\n",
      "238 0.71322894\n",
      "239 0.7130895\n",
      "240 0.71295005\n",
      "241 0.71281064\n",
      "242 0.71267116\n",
      "243 0.7125318\n",
      "244 0.71239245\n",
      "245 0.7122531\n",
      "246 0.71211374\n",
      "247 0.7119745\n",
      "248 0.7118352\n",
      "249 0.71169597\n",
      "250 0.71155685\n",
      "251 0.7114176\n",
      "252 0.7112784\n",
      "253 0.7111393\n",
      "254 0.71100026\n",
      "255 0.71086115\n",
      "256 0.71072215\n",
      "257 0.7105831\n",
      "258 0.7104441\n",
      "259 0.71030515\n",
      "260 0.7101663\n",
      "261 0.71002734\n",
      "262 0.7098884\n",
      "263 0.7097496\n",
      "264 0.70961076\n",
      "265 0.7094719\n",
      "266 0.70933306\n",
      "267 0.70919436\n",
      "268 0.7090558\n",
      "269 0.708917\n",
      "270 0.7087783\n",
      "271 0.7086397\n",
      "272 0.7085011\n",
      "273 0.7083625\n",
      "274 0.70822394\n",
      "275 0.7080854\n",
      "276 0.70794684\n",
      "277 0.7078085\n",
      "278 0.7076701\n",
      "279 0.70753163\n",
      "280 0.7073933\n",
      "281 0.70725495\n",
      "282 0.70711654\n",
      "283 0.7069782\n",
      "284 0.70684\n",
      "285 0.70670176\n",
      "286 0.70656365\n",
      "287 0.70642537\n",
      "288 0.7062872\n",
      "289 0.7061491\n",
      "290 0.70601094\n",
      "291 0.7058729\n",
      "292 0.70573485\n",
      "293 0.70559675\n",
      "294 0.70545876\n",
      "295 0.70532084\n",
      "296 0.7051829\n",
      "297 0.705045\n",
      "298 0.70490694\n",
      "299 0.70476925\n",
      "301 0.70449346\n",
      "302 0.70435566\n",
      "303 0.704218\n",
      "304 0.70408016\n",
      "305 0.7039425\n",
      "306 0.7038048\n",
      "307 0.70366716\n",
      "308 0.70352954\n",
      "309 0.70339185\n",
      "310 0.70325434\n",
      "311 0.7031167\n",
      "312 0.70297927\n",
      "313 0.7028418\n",
      "314 0.7027043\n",
      "315 0.70256674\n",
      "316 0.70242935\n",
      "317 0.702292\n",
      "318 0.7021546\n",
      "319 0.7020173\n",
      "320 0.70188\n",
      "321 0.70174265\n",
      "322 0.7016054\n",
      "323 0.7014681\n",
      "324 0.7013309\n",
      "325 0.7011937\n",
      "326 0.70105666\n",
      "327 0.70091957\n",
      "328 0.7007824\n",
      "329 0.7006454\n",
      "330 0.7005083\n",
      "331 0.7003713\n",
      "332 0.70023435\n",
      "333 0.7000973\n",
      "334 0.6999604\n",
      "335 0.6998235\n",
      "336 0.69968665\n",
      "337 0.6995499\n",
      "338 0.69941306\n",
      "339 0.69927615\n",
      "340 0.69913936\n",
      "341 0.6990026\n",
      "342 0.6988659\n",
      "343 0.6987293\n",
      "344 0.6985926\n",
      "345 0.698456\n",
      "346 0.6983193\n",
      "347 0.69818264\n",
      "348 0.6980461\n",
      "349 0.69790965\n",
      "350 0.69777316\n",
      "351 0.69763666\n",
      "352 0.69750017\n",
      "353 0.6973637\n",
      "354 0.6972273\n",
      "355 0.697091\n",
      "356 0.69695455\n",
      "357 0.6968181\n",
      "358 0.69668186\n",
      "359 0.6965456\n",
      "360 0.6964094\n",
      "361 0.69627315\n",
      "362 0.69613683\n",
      "363 0.69600075\n",
      "364 0.69586456\n",
      "365 0.69572836\n",
      "366 0.6955923\n",
      "367 0.69545615\n",
      "368 0.6953202\n",
      "369 0.69518423\n",
      "370 0.6950482\n",
      "371 0.6949122\n",
      "372 0.69477636\n",
      "373 0.69464046\n",
      "374 0.69450456\n",
      "375 0.6943687\n",
      "376 0.6942329\n",
      "377 0.69409716\n",
      "378 0.6939613\n",
      "379 0.6938256\n",
      "380 0.6936899\n",
      "381 0.69355416\n",
      "382 0.6934185\n",
      "383 0.69328284\n",
      "384 0.69314724\n",
      "385 0.6930116\n",
      "386 0.6928761\n",
      "387 0.6927405\n",
      "388 0.69260514\n",
      "389 0.69246954\n",
      "390 0.6923341\n",
      "391 0.69219863\n",
      "392 0.6920633\n",
      "393 0.6919279\n",
      "394 0.6917925\n",
      "395 0.69165725\n",
      "396 0.6915219\n",
      "397 0.6913865\n",
      "398 0.69125134\n",
      "399 0.69111615\n",
      "401 0.69084567\n",
      "402 0.69071054\n",
      "403 0.6905754\n",
      "404 0.69044024\n",
      "405 0.6903053\n",
      "406 0.6901701\n",
      "407 0.6900352\n",
      "408 0.68990016\n",
      "409 0.6897653\n",
      "410 0.6896302\n",
      "411 0.68949527\n",
      "412 0.68936044\n",
      "413 0.68922555\n",
      "414 0.68909067\n",
      "415 0.68895584\n",
      "416 0.688821\n",
      "417 0.6886863\n",
      "418 0.68855166\n",
      "419 0.6884169\n",
      "420 0.68828213\n",
      "421 0.68814754\n",
      "422 0.6880129\n",
      "423 0.6878783\n",
      "424 0.6877438\n",
      "425 0.68760914\n",
      "426 0.6874746\n",
      "427 0.68734014\n",
      "428 0.6872057\n",
      "429 0.68707114\n",
      "430 0.6869368\n",
      "431 0.6868023\n",
      "432 0.6866679\n",
      "433 0.6865337\n",
      "434 0.6863993\n",
      "435 0.686265\n",
      "436 0.68613076\n",
      "437 0.6859965\n",
      "438 0.68586224\n",
      "439 0.6857281\n",
      "440 0.68559384\n",
      "441 0.6854598\n",
      "442 0.6853257\n",
      "443 0.6851915\n",
      "444 0.68505746\n",
      "445 0.68492347\n",
      "446 0.68478936\n",
      "447 0.68465537\n",
      "448 0.68452144\n",
      "449 0.68438745\n",
      "450 0.6842535\n",
      "451 0.68411964\n",
      "452 0.6839857\n",
      "453 0.68385196\n",
      "454 0.683718\n",
      "455 0.68358433\n",
      "456 0.6834505\n",
      "457 0.6833168\n",
      "458 0.6831832\n",
      "459 0.68304944\n",
      "460 0.6829158\n",
      "461 0.6827822\n",
      "462 0.68264854\n",
      "463 0.68251497\n",
      "464 0.68238145\n",
      "465 0.68224794\n",
      "466 0.6821145\n",
      "467 0.6819809\n",
      "468 0.68184745\n",
      "469 0.68171406\n",
      "470 0.68158066\n",
      "471 0.68144727\n",
      "472 0.6813139\n",
      "473 0.68118054\n",
      "474 0.68104726\n",
      "475 0.68091404\n",
      "476 0.6807807\n",
      "477 0.6806475\n",
      "478 0.68051434\n",
      "479 0.6803811\n",
      "480 0.68024784\n",
      "481 0.6801148\n",
      "482 0.67998165\n",
      "483 0.67984855\n",
      "484 0.6797156\n",
      "485 0.6795824\n",
      "486 0.6794495\n",
      "487 0.6793165\n",
      "488 0.6791835\n",
      "489 0.67905056\n",
      "490 0.6789176\n",
      "491 0.6787847\n",
      "492 0.67865187\n",
      "493 0.678519\n",
      "494 0.67838633\n",
      "495 0.6782535\n",
      "496 0.67812085\n",
      "497 0.67798805\n",
      "498 0.6778553\n",
      "499 0.6777227\n",
      "501 0.67745745\n",
      "502 0.67732483\n",
      "503 0.67719215\n",
      "504 0.67705965\n",
      "505 0.6769271\n",
      "506 0.67679477\n",
      "507 0.6766622\n",
      "508 0.67652977\n",
      "509 0.6763973\n",
      "510 0.676265\n",
      "511 0.67613256\n",
      "512 0.6760002\n",
      "513 0.6758679\n",
      "514 0.67573565\n",
      "515 0.6756033\n",
      "516 0.675471\n",
      "517 0.6753388\n",
      "518 0.6752066\n",
      "519 0.67507446\n",
      "520 0.6749422\n",
      "521 0.6748101\n",
      "522 0.674678\n",
      "523 0.6745459\n",
      "524 0.67441386\n",
      "525 0.67428184\n",
      "526 0.67414975\n",
      "527 0.6740178\n",
      "528 0.67388594\n",
      "529 0.67375404\n",
      "530 0.6736221\n",
      "531 0.67349017\n",
      "532 0.6733583\n",
      "533 0.67322654\n",
      "534 0.6730948\n",
      "535 0.6729629\n",
      "536 0.6728312\n",
      "537 0.67269945\n",
      "538 0.67256784\n",
      "539 0.6724361\n",
      "540 0.67230433\n",
      "541 0.6721728\n",
      "542 0.6720412\n",
      "543 0.6719095\n",
      "544 0.671778\n",
      "545 0.6716465\n",
      "546 0.67151505\n",
      "547 0.6713835\n",
      "548 0.671252\n",
      "549 0.6711206\n",
      "550 0.6709892\n",
      "551 0.6708578\n",
      "552 0.6707264\n",
      "553 0.670595\n",
      "554 0.67046374\n",
      "555 0.67033243\n",
      "556 0.6702011\n",
      "557 0.6700699\n",
      "558 0.6699386\n",
      "559 0.66980743\n",
      "560 0.66967624\n",
      "561 0.669545\n",
      "562 0.6694139\n",
      "563 0.6692829\n",
      "564 0.66915184\n",
      "565 0.6690208\n",
      "566 0.6688899\n",
      "567 0.6687588\n",
      "568 0.6686279\n",
      "569 0.66849697\n",
      "570 0.668366\n",
      "571 0.6682351\n",
      "572 0.6681043\n",
      "573 0.66797346\n",
      "574 0.6678427\n",
      "575 0.66771185\n",
      "576 0.66758114\n",
      "577 0.6674503\n",
      "578 0.66731966\n",
      "579 0.6671889\n",
      "580 0.6670582\n",
      "581 0.6669275\n",
      "582 0.666797\n",
      "583 0.66666645\n",
      "584 0.6665358\n",
      "585 0.6664052\n",
      "586 0.6662747\n",
      "587 0.6661442\n",
      "588 0.6660137\n",
      "589 0.6658833\n",
      "590 0.6657528\n",
      "591 0.6656225\n",
      "592 0.66549206\n",
      "593 0.6653616\n",
      "594 0.6652313\n",
      "595 0.665101\n",
      "596 0.66497076\n",
      "597 0.66484046\n",
      "598 0.6647102\n",
      "599 0.6645801\n",
      "601 0.6643197\n",
      "602 0.6641896\n",
      "603 0.66405946\n",
      "604 0.6639294\n",
      "605 0.66379935\n",
      "606 0.6636692\n",
      "607 0.66353923\n",
      "608 0.6634093\n",
      "609 0.66327924\n",
      "610 0.6631494\n",
      "611 0.66301936\n",
      "612 0.66288954\n",
      "613 0.6627596\n",
      "614 0.66262966\n",
      "615 0.6624999\n",
      "616 0.66237015\n",
      "617 0.6622404\n",
      "618 0.66211057\n",
      "619 0.66198087\n",
      "620 0.6618511\n",
      "621 0.66172147\n",
      "622 0.6615917\n",
      "623 0.6614622\n",
      "624 0.6613325\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625 0.6612031\n",
      "626 0.66107345\n",
      "627 0.660944\n",
      "628 0.6608144\n",
      "629 0.6606849\n",
      "630 0.6605554\n",
      "631 0.6604261\n",
      "632 0.6602966\n",
      "633 0.6601672\n",
      "634 0.6600379\n",
      "635 0.6599085\n",
      "636 0.65977925\n",
      "637 0.6596499\n",
      "638 0.65952057\n",
      "639 0.65939134\n",
      "640 0.6592622\n",
      "641 0.65913296\n",
      "642 0.65900373\n",
      "643 0.6588747\n",
      "644 0.65874547\n",
      "645 0.6586164\n",
      "646 0.6584874\n",
      "647 0.6583582\n",
      "648 0.6582292\n",
      "649 0.65810025\n",
      "650 0.6579712\n",
      "651 0.6578422\n",
      "652 0.65771335\n",
      "653 0.65758437\n",
      "654 0.6574555\n",
      "655 0.65732664\n",
      "656 0.6571978\n",
      "657 0.65706897\n",
      "658 0.6569402\n",
      "659 0.6568114\n",
      "660 0.6566828\n",
      "661 0.656554\n",
      "662 0.6564253\n",
      "663 0.6562966\n",
      "664 0.65616804\n",
      "665 0.6560394\n",
      "666 0.65591073\n",
      "667 0.6557823\n",
      "668 0.6556537\n",
      "669 0.65552515\n",
      "670 0.65539664\n",
      "671 0.6552681\n",
      "672 0.6551397\n",
      "673 0.6550112\n",
      "674 0.65488285\n",
      "675 0.6547544\n",
      "676 0.6546261\n",
      "677 0.6544977\n",
      "678 0.6543694\n",
      "679 0.654241\n",
      "680 0.6541127\n",
      "681 0.65398455\n",
      "682 0.6538563\n",
      "683 0.65372807\n",
      "684 0.6536\n",
      "685 0.6534718\n",
      "686 0.6533437\n",
      "687 0.6532155\n",
      "688 0.6530875\n",
      "689 0.65295947\n",
      "690 0.6528314\n",
      "691 0.6527033\n",
      "692 0.65257543\n",
      "693 0.6524474\n",
      "694 0.65231943\n",
      "695 0.6521915\n",
      "696 0.65206367\n",
      "697 0.65193576\n",
      "698 0.6518079\n",
      "699 0.65168005\n",
      "701 0.65142447\n",
      "702 0.65129673\n",
      "703 0.65116906\n",
      "704 0.6510413\n",
      "705 0.6509136\n",
      "706 0.650786\n",
      "707 0.65065825\n",
      "708 0.6505306\n",
      "709 0.650403\n",
      "710 0.6502754\n",
      "711 0.6501479\n",
      "712 0.6500204\n",
      "713 0.6498929\n",
      "714 0.6497654\n",
      "715 0.64963794\n",
      "716 0.6495105\n",
      "717 0.6493832\n",
      "718 0.6492558\n",
      "719 0.6491283\n",
      "720 0.649001\n",
      "721 0.6488738\n",
      "722 0.64874643\n",
      "723 0.64861923\n",
      "724 0.648492\n",
      "725 0.6483648\n",
      "726 0.64823765\n",
      "727 0.6481104\n",
      "728 0.6479833\n",
      "729 0.6478562\n",
      "730 0.64772904\n",
      "731 0.6476019\n",
      "732 0.6474749\n",
      "733 0.64734787\n",
      "734 0.64722085\n",
      "735 0.64709383\n",
      "736 0.64696693\n",
      "737 0.6468399\n",
      "738 0.646713\n",
      "739 0.6465861\n",
      "740 0.6464593\n",
      "741 0.64633244\n",
      "742 0.64620566\n",
      "743 0.6460788\n",
      "744 0.64595205\n",
      "745 0.6458253\n",
      "746 0.6456986\n",
      "747 0.6455719\n",
      "748 0.64544517\n",
      "749 0.6453185\n",
      "750 0.6451919\n",
      "751 0.64506537\n",
      "752 0.64493877\n",
      "753 0.64481217\n",
      "754 0.64468557\n",
      "755 0.64455914\n",
      "756 0.6444326\n",
      "757 0.64430606\n",
      "758 0.6441797\n",
      "759 0.6440532\n",
      "760 0.64392686\n",
      "761 0.6438005\n",
      "762 0.64367414\n",
      "763 0.64354783\n",
      "764 0.64342153\n",
      "765 0.6432953\n",
      "766 0.643169\n",
      "767 0.64304286\n",
      "768 0.64291656\n",
      "769 0.6427904\n",
      "770 0.64266425\n",
      "771 0.6425381\n",
      "772 0.64241195\n",
      "773 0.6422859\n",
      "774 0.6421598\n",
      "775 0.6420338\n",
      "776 0.6419078\n",
      "777 0.6417816\n",
      "778 0.64165574\n",
      "779 0.6415298\n",
      "780 0.6414038\n",
      "781 0.6412779\n",
      "782 0.64115196\n",
      "783 0.64102614\n",
      "784 0.64090025\n",
      "785 0.6407744\n",
      "786 0.64064866\n",
      "787 0.64052296\n",
      "788 0.6403972\n",
      "789 0.6402714\n",
      "790 0.6401457\n",
      "791 0.64002\n",
      "792 0.63989437\n",
      "793 0.6397688\n",
      "794 0.63964313\n",
      "795 0.6395176\n",
      "796 0.6393919\n",
      "797 0.6392664\n",
      "798 0.63914084\n",
      "799 0.6390154\n",
      "801 0.6387644\n",
      "802 0.6386389\n",
      "803 0.63851345\n",
      "804 0.6383881\n",
      "805 0.63826275\n",
      "806 0.63813746\n",
      "807 0.63801205\n",
      "808 0.6378867\n",
      "809 0.6377615\n",
      "810 0.63763624\n",
      "811 0.637511\n",
      "812 0.63738585\n",
      "813 0.6372606\n",
      "814 0.63713557\n",
      "815 0.63701046\n",
      "816 0.6368853\n",
      "817 0.6367601\n",
      "818 0.6366351\n",
      "819 0.63651013\n",
      "820 0.636385\n",
      "821 0.6362601\n",
      "822 0.63613516\n",
      "823 0.63601017\n",
      "824 0.63588536\n",
      "825 0.63576037\n",
      "826 0.63563544\n",
      "827 0.63551056\n",
      "828 0.6353858\n",
      "829 0.63526094\n",
      "830 0.6351361\n",
      "831 0.6350114\n",
      "832 0.6348866\n",
      "833 0.6347619\n",
      "834 0.63463724\n",
      "835 0.6345125\n",
      "836 0.63438785\n",
      "837 0.6342632\n",
      "838 0.63413864\n",
      "839 0.6340142\n",
      "840 0.63388944\n",
      "841 0.633765\n",
      "842 0.63364047\n",
      "843 0.6335159\n",
      "844 0.63339144\n",
      "845 0.633267\n",
      "846 0.63314253\n",
      "847 0.6330181\n",
      "848 0.6328937\n",
      "849 0.6327694\n",
      "850 0.632645\n",
      "851 0.6325207\n",
      "852 0.63239634\n",
      "853 0.6322721\n",
      "854 0.6321478\n",
      "855 0.63202363\n",
      "856 0.6318994\n",
      "857 0.63177526\n",
      "858 0.6316511\n",
      "859 0.6315269\n",
      "860 0.63140285\n",
      "861 0.63127875\n",
      "862 0.6311547\n",
      "863 0.6310306\n",
      "864 0.63090664\n",
      "865 0.6307826\n",
      "866 0.6306587\n",
      "867 0.6305347\n",
      "868 0.6304108\n",
      "869 0.63028693\n",
      "870 0.630163\n",
      "871 0.6300391\n",
      "872 0.62991536\n",
      "873 0.62979144\n",
      "874 0.6296677\n",
      "875 0.62954396\n",
      "876 0.6294202\n",
      "877 0.6292964\n",
      "878 0.6291728\n",
      "879 0.62904906\n",
      "880 0.6289255\n",
      "881 0.6288019\n",
      "882 0.62867826\n",
      "883 0.6285547\n",
      "884 0.62843114\n",
      "885 0.62830764\n",
      "886 0.6281841\n",
      "887 0.6280606\n",
      "888 0.62793714\n",
      "889 0.62781364\n",
      "890 0.62769026\n",
      "891 0.6275669\n",
      "892 0.62744343\n",
      "893 0.6273201\n",
      "894 0.6271967\n",
      "895 0.6270734\n",
      "896 0.6269501\n",
      "897 0.62682676\n",
      "898 0.62670356\n",
      "899 0.62658036\n",
      "901 0.62633395\n",
      "902 0.62621087\n",
      "903 0.62608767\n",
      "904 0.62596464\n",
      "905 0.6258415\n",
      "906 0.6257185\n",
      "907 0.6255954\n",
      "908 0.62547237\n",
      "909 0.62534934\n",
      "910 0.6252264\n",
      "911 0.62510335\n",
      "912 0.62498045\n",
      "913 0.6248576\n",
      "914 0.6247346\n",
      "915 0.6246118\n",
      "916 0.6244889\n",
      "917 0.6243661\n",
      "918 0.6242433\n",
      "919 0.62412053\n",
      "920 0.6239978\n",
      "921 0.62387514\n",
      "922 0.6237524\n",
      "923 0.6236296\n",
      "924 0.6235071\n",
      "925 0.62338436\n",
      "926 0.62326175\n",
      "927 0.62313914\n",
      "928 0.6230166\n",
      "929 0.62289405\n",
      "930 0.6227715\n",
      "931 0.62264895\n",
      "932 0.62252647\n",
      "933 0.6224041\n",
      "934 0.62228155\n",
      "935 0.6221591\n",
      "936 0.6220367\n",
      "937 0.6219144\n",
      "938 0.621792\n",
      "939 0.62166965\n",
      "940 0.6215474\n",
      "941 0.6214251\n",
      "942 0.6213028\n",
      "943 0.6211806\n",
      "944 0.62105834\n",
      "945 0.62093616\n",
      "946 0.62081397\n",
      "947 0.6206918\n",
      "948 0.62056965\n",
      "949 0.6204476\n",
      "950 0.6203255\n",
      "951 0.62020344\n",
      "952 0.62008137\n",
      "953 0.61995935\n",
      "954 0.61983734\n",
      "955 0.61971533\n",
      "956 0.6195934\n",
      "957 0.6194715\n",
      "958 0.61934954\n",
      "959 0.6192277\n",
      "960 0.6191058\n",
      "961 0.61898404\n",
      "962 0.6188622\n",
      "963 0.61874044\n",
      "964 0.61861867\n",
      "965 0.6184969\n",
      "966 0.61837524\n",
      "967 0.6182535\n",
      "968 0.61813176\n",
      "969 0.6180101\n",
      "970 0.61788845\n",
      "971 0.61776686\n",
      "972 0.61764526\n",
      "973 0.61752367\n",
      "974 0.6174021\n",
      "975 0.61728054\n",
      "976 0.61715907\n",
      "977 0.61703765\n",
      "978 0.6169162\n",
      "979 0.61679476\n",
      "980 0.6166734\n",
      "981 0.6165519\n",
      "982 0.6164306\n",
      "983 0.6163092\n",
      "984 0.6161879\n",
      "985 0.6160666\n",
      "986 0.61594534\n",
      "987 0.61582404\n",
      "988 0.6157028\n",
      "989 0.61558163\n",
      "990 0.6154604\n",
      "991 0.61533916\n",
      "992 0.61521804\n",
      "993 0.6150969\n",
      "994 0.61497587\n",
      "995 0.6148548\n",
      "996 0.61473376\n",
      "997 0.6146127\n",
      "998 0.6144917\n",
      "999 0.6143707\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data=pd.read_csv('data-04-zoo.csv',skiprows=19,names=['hair','feather','eggs','milks','airborne','aquatic','predator','toothed','backbone','breathes','venomous','fins','legs','tail','domestic','capsize','type'])\n",
    "\n",
    "xArr=data.iloc[:,0:16]  # N x 16\n",
    "yArr=data.iloc[:,16]   #class값( N x 1)\n",
    "yArr=yArr[:,np.newaxis]\n",
    "yArr=pd.get_dummies(yArr[:,0])  \n",
    "\n",
    "#normalizing? \n",
    "\n",
    "X=tf.placeholder('float',[None,16])\n",
    "Y=tf.placeholder('float',[None,7])\n",
    "W=tf.Variable(tf.random_normal([16,7]), name='weight')\n",
    "b=tf.Variable(tf.random_normal([7]), name='bias')\n",
    "\n",
    "hypothesis = tf.nn.softmax(tf.matmul(X, W) + b)    #소프트맥스 -> [0.1, 0.8, 0.1] 속성모두 더했을 때 합이 1인 소수값으로 hypothesis(y_hat)를 설정\n",
    "cost=tf.reduce_mean(-tf.reduce_mean(Y*tf.log(hypothesis),axis=1))  \n",
    "\n",
    "optimizer=tf.train.GradientDescentOptimizer(learning_rate=0.001).minimize(cost)\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(1000):\n",
    "        sess.run(optimizer, feed_dict={X:xArr, Y:yArr})\n",
    "        if step %100:\n",
    "            print(step,sess.run(cost, feed_dict={X:xArr,Y:yArr}))\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "소프트맥스를 이용한 엔트로피 계산 및 정확도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"one_hot_13:0\", shape=(?, 1, 7), dtype=float32)\n",
      "Tensor(\"Reshape_13:0\", shape=(?, 7), dtype=float32)\n",
      "0 7.100826 0.01980198\n",
      "200 0.4021022 0.8811881\n",
      "400 0.23024257 0.96039605\n",
      "600 0.16091342 0.990099\n",
      "800 0.12430417 0.990099\n",
      "1000 0.10168605 1.0\n",
      "1200 0.08625628 1.0\n",
      "1400 0.07501477 1.0\n",
      "1600 0.06643672 1.0\n",
      "1800 0.059663035 1.0\n",
      "2000 0.054171436 1.0\n",
      "True Prediction: 0 True Y : 0\n",
      "True Prediction: 0 True Y : 0\n",
      "True Prediction: 3 True Y : 3\n",
      "True Prediction: 0 True Y : 0\n",
      "True Prediction: 0 True Y : 0\n",
      "True Prediction: 0 True Y : 0\n",
      "True Prediction: 0 True Y : 0\n",
      "True Prediction: 3 True Y : 3\n",
      "True Prediction: 3 True Y : 3\n",
      "True Prediction: 0 True Y : 0\n",
      "True Prediction: 0 True Y : 0\n",
      "True Prediction: 1 True Y : 1\n",
      "True Prediction: 3 True Y : 3\n",
      "True Prediction: 6 True Y : 6\n",
      "True Prediction: 6 True Y : 6\n",
      "True Prediction: 6 True Y : 6\n",
      "True Prediction: 1 True Y : 1\n",
      "True Prediction: 0 True Y : 0\n",
      "True Prediction: 3 True Y : 3\n",
      "True Prediction: 0 True Y : 0\n",
      "True Prediction: 1 True Y : 1\n",
      "True Prediction: 1 True Y : 1\n",
      "True Prediction: 0 True Y : 0\n",
      "True Prediction: 1 True Y : 1\n",
      "True Prediction: 5 True Y : 5\n",
      "True Prediction: 4 True Y : 4\n",
      "True Prediction: 4 True Y : 4\n",
      "True Prediction: 0 True Y : 0\n",
      "True Prediction: 0 True Y : 0\n",
      "True Prediction: 0 True Y : 0\n",
      "True Prediction: 5 True Y : 5\n",
      "True Prediction: 0 True Y : 0\n",
      "True Prediction: 0 True Y : 0\n",
      "True Prediction: 1 True Y : 1\n",
      "True Prediction: 3 True Y : 3\n",
      "True Prediction: 0 True Y : 0\n",
      "True Prediction: 0 True Y : 0\n",
      "True Prediction: 1 True Y : 1\n",
      "True Prediction: 3 True Y : 3\n",
      "True Prediction: 5 True Y : 5\n",
      "True Prediction: 5 True Y : 5\n",
      "True Prediction: 1 True Y : 1\n",
      "True Prediction: 5 True Y : 5\n",
      "True Prediction: 1 True Y : 1\n",
      "True Prediction: 0 True Y : 0\n",
      "True Prediction: 0 True Y : 0\n",
      "True Prediction: 6 True Y : 6\n",
      "True Prediction: 0 True Y : 0\n",
      "True Prediction: 0 True Y : 0\n",
      "True Prediction: 0 True Y : 0\n",
      "True Prediction: 0 True Y : 0\n",
      "True Prediction: 5 True Y : 5\n",
      "True Prediction: 4 True Y : 4\n",
      "True Prediction: 6 True Y : 6\n",
      "True Prediction: 0 True Y : 0\n",
      "True Prediction: 0 True Y : 0\n",
      "True Prediction: 1 True Y : 1\n",
      "True Prediction: 1 True Y : 1\n",
      "True Prediction: 1 True Y : 1\n",
      "True Prediction: 1 True Y : 1\n",
      "True Prediction: 3 True Y : 3\n",
      "True Prediction: 3 True Y : 3\n",
      "True Prediction: 2 True Y : 2\n",
      "True Prediction: 0 True Y : 0\n",
      "True Prediction: 0 True Y : 0\n",
      "True Prediction: 0 True Y : 0\n",
      "True Prediction: 0 True Y : 0\n",
      "True Prediction: 0 True Y : 0\n",
      "True Prediction: 0 True Y : 0\n",
      "True Prediction: 0 True Y : 0\n",
      "True Prediction: 0 True Y : 0\n",
      "True Prediction: 1 True Y : 1\n",
      "True Prediction: 6 True Y : 6\n",
      "True Prediction: 3 True Y : 3\n",
      "True Prediction: 0 True Y : 0\n",
      "True Prediction: 0 True Y : 0\n",
      "True Prediction: 2 True Y : 2\n",
      "True Prediction: 6 True Y : 6\n",
      "True Prediction: 1 True Y : 1\n",
      "True Prediction: 1 True Y : 1\n",
      "True Prediction: 2 True Y : 2\n",
      "True Prediction: 6 True Y : 6\n",
      "True Prediction: 3 True Y : 3\n",
      "True Prediction: 1 True Y : 1\n",
      "True Prediction: 0 True Y : 0\n",
      "True Prediction: 6 True Y : 6\n",
      "True Prediction: 3 True Y : 3\n",
      "True Prediction: 1 True Y : 1\n",
      "True Prediction: 5 True Y : 5\n",
      "True Prediction: 4 True Y : 4\n",
      "True Prediction: 2 True Y : 2\n",
      "True Prediction: 2 True Y : 2\n",
      "True Prediction: 3 True Y : 3\n",
      "True Prediction: 0 True Y : 0\n",
      "True Prediction: 0 True Y : 0\n",
      "True Prediction: 1 True Y : 1\n",
      "True Prediction: 0 True Y : 0\n",
      "True Prediction: 5 True Y : 5\n",
      "True Prediction: 0 True Y : 0\n",
      "True Prediction: 6 True Y : 6\n",
      "True Prediction: 1 True Y : 1\n"
     ]
    }
   ],
   "source": [
    "zoo = np.loadtxt('data-04-zoo.csv',delimiter=',',dtype=np.float32)\n",
    "\n",
    "x_data = zoo[:,0:-1]\n",
    "y_data = zoo[:,[-1]]\n",
    "\n",
    "\n",
    "X = tf.placeholder(tf.float32,[None,16])\n",
    "Y = tf.placeholder(tf.int32,[None,1])\n",
    "Y_one_hot = tf.one_hot(Y,7) #3차원의  data로 \n",
    "print(Y_one_hot)\n",
    "Y_one_hot = tf.reshape(Y_one_hot,[-1,7])  #Y_one_hot변수의 끝 차원을 7로, 나머지 차원은 알아서 조정~\n",
    "print(Y_one_hot)  \n",
    "\n",
    "W=tf.Variable(tf.random_normal([16,7]), name='weight')\n",
    "b=tf.Variable(tf.random_normal([7]), name='bias')\n",
    "logits=tf.matmul(X,W)+b\n",
    "hypothesis=tf.nn.softmax(logits)\n",
    "\n",
    "#손실값(y-y_hat) 계산\n",
    "cost_i=tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y_one_hot)   #소프트맥스의 엔트로피계산(손실값(y-y_hat)), 확률 계산을 해주는 함수\n",
    "cost=tf.reduce_mean(cost_i)\n",
    "optimizer=tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "#소프트맥스값으로부터 accuracy도출\n",
    "prediction=tf.argmax(hypothesis,1)  #y_hat행렬에서 1번째(2차원(행 x 열)이라면 열) 차원에서 가장 큰값  -> [[0.7,0.1,0.2],...] -> [0,...]\n",
    "correct_prediction=tf.equal(prediction, tf.argmax(Y_one_hot,1))   #예측값과 라벨값(실제값)과 같다면 True, 아니면, False\n",
    "accuracy=tf.reduce_mean(tf.cast(correct_prediction, tf.float32))  #True,False값을 float32형으로 바꾸고, 그것의 평균값 \n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())   #세션함수를 실행하기위해 필요하 모든 변수들 초기화\n",
    "    for step in range(2001):\n",
    "        sess.run(optimizer, feed_dict={X : x_data, Y : y_data})\n",
    "        if step%200==0:\n",
    "            loss, acc = sess.run([cost,accuracy], feed_dict={X :x_data, Y :y_data})\n",
    "            print(step,loss,acc)\n",
    "            \n",
    "    pred=sess.run(prediction, feed_dict={X:x_data})\n",
    "    for p,y in zip(pred,y_data.flatten()):  #y_data의 인덱스 값을 가져옴. \n",
    "        print('{} Prediction: {} True Y : {}'.format(p==int(y),p,int(y)))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 1)\n",
      "Tensor(\"one_hot_25:0\", shape=(?, 1, 4), dtype=float32)\n",
      "Tensor(\"Reshape_25:0\", shape=(?, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "zoo = np.loadtxt('data-04-zoo.csv',delimiter=',',dtype=np.float32)\n",
    "\n",
    "x_data = zoo[:,0:-1]\n",
    "y_data = zoo[:,[-1]]\n",
    "\n",
    "X = tf.placeholder(tf.float32,[None,16])\n",
    "Y = tf.placeholder(tf.int32,[None,1])\n",
    "print(Y.shape)\n",
    "Y_one_hot = tf.one_hot(Y,4) #3차원의  data로 \n",
    "print(Y_one_hot)\n",
    "Y_one_hot = tf.reshape(Y_one_hot,[-1,2])  #Y_one_hot변수의 끝 차원을 7로, 나머지 차원은 알아서 조정~\n",
    "print(Y_one_hot)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"one_hot_22:0\", shape=(?, 1, 1), dtype=float32)\n",
      "Tensor(\"Reshape_22:0\", shape=(?, 1), dtype=float32)\n",
      "0 0.0 [0 0 0 0] 1.0\n",
      "200 0.0 [0 0 0 0] 1.0\n",
      "400 0.0 [0 0 0 0] 1.0\n",
      "600 0.0 [0 0 0 0] 1.0\n",
      "800 0.0 [0 0 0 0] 1.0\n",
      "1000 0.0 [0 0 0 0] 1.0\n",
      "1200 0.0 [0 0 0 0] 1.0\n",
      "1400 0.0 [0 0 0 0] 1.0\n",
      "1600 0.0 [0 0 0 0] 1.0\n",
      "1800 0.0 [0 0 0 0] 1.0\n",
      "2000 0.0 [0 0 0 0] 1.0\n"
     ]
    }
   ],
   "source": [
    "x_data=np.array([[0,0],\n",
    "        [0,1],\n",
    "        [1,0],\n",
    "        [1,1]])\n",
    "y_data=np.array([[0],\n",
    "        [1],\n",
    "        [1],\n",
    "        [0]])\n",
    "\n",
    "\n",
    "X = tf.placeholder(tf.float32,[None,2])\n",
    "Y = tf.placeholder(tf.int32,[None,1])\n",
    "Y_one_hot = tf.one_hot(Y,1) #3차원의  data로 \n",
    "print(Y_one_hot)\n",
    "Y_one_hot = tf.reshape(Y_one_hot,[-1,1])  #Y_one_hot변수의 끝 차원을 7로, 나머지 차원은 알아서 조정~\n",
    "print(Y_one_hot)  \n",
    "\n",
    "W=tf.Variable(tf.random_normal([2,1]), name='weight')\n",
    "b=tf.Variable(tf.random_normal([1]), name='bias')\n",
    "logits=tf.matmul(X,W)+b\n",
    "hypothesis=tf.nn.softmax(logits)\n",
    "\n",
    "#손실값(y-y_hat) 계산\n",
    "cost_i=tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y_one_hot)   #소프트맥스의 엔트로피계산(손실값(y-y_hat)), 확률 계산을 해주는 함수\n",
    "cost=tf.reduce_mean(cost_i)\n",
    "optimizer=tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "#소프트맥스값으로부터 accuracy도출\n",
    "prediction=tf.argmax(hypothesis,1)  #y_hat행렬에서 1번째(2차원(행 x 열)이라면 열) 차원에서 가장 큰값  -> [[0.7,0.1,0.2],...] -> [0,...]\n",
    "correct_prediction=tf.equal(prediction, tf.argmax(Y_one_hot,1))   #예측값과 라벨값(실제값)과 같다면 True, 아니면, False\n",
    "accuracy=tf.reduce_mean(tf.cast(correct_prediction, tf.float32))  #True,False값을 float32형으로 바꾸고, 그것의 평균값 \n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())   #세션함수를 실행하기위해 필요하 모든 변수들 초기화\n",
    "    for step in range(2001):\n",
    "        sess.run(optimizer, feed_dict={X : x_data, Y : y_data})\n",
    "        if step%200==0:\n",
    "            loss, pre,acc = sess.run([cost,prediction,accuracy], feed_dict={X :x_data, Y :y_data})\n",
    "            print(step,loss,pre,acc)\n",
    "            \n",
    "    #pred=sess.run(prediction, feed_dict={X:x_data})\n",
    "    #for p,y in zip(pred,y_data.flatten()):  #y_data의 인덱스 값을 가져옴. \n",
    "    #    print('{} Prediction: {} True Y : {}'.format(p==int(y),p,int(y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "신경망 구현실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.76302403 0.5\n",
      "200 0.76302403 0.5\n",
      "400 0.76302403 0.5\n",
      "600 0.76302403 0.5\n",
      "800 0.76302403 0.5\n",
      "1000 0.76302403 0.5\n",
      "1200 0.76302403 0.5\n",
      "1400 0.76302403 0.5\n",
      "1600 0.76302403 0.5\n",
      "1800 0.76302403 0.5\n",
      "2000 0.76302403 0.5\n",
      "2200 0.76302403 0.5\n",
      "2400 0.76302403 0.5\n",
      "2600 0.76302403 0.5\n",
      "2800 0.76302403 0.5\n",
      "3000 0.76302403 0.5\n",
      "3200 0.76302403 0.5\n",
      "3400 0.76302403 0.5\n",
      "3600 0.76302403 0.5\n",
      "3800 0.76302403 0.5\n",
      "4000 0.76302403 0.5\n",
      "4200 0.76302403 0.5\n",
      "4400 0.76302403 0.5\n",
      "4600 0.76302403 0.5\n",
      "4800 0.76302403 0.5\n",
      "5000 0.76302403 0.5\n",
      "5200 0.76302403 0.5\n",
      "5400 0.76302403 0.5\n",
      "5600 0.76302403 0.5\n",
      "5800 0.76302403 0.5\n",
      "6000 0.76302403 0.5\n",
      "6200 0.76302403 0.5\n",
      "6400 0.76302403 0.5\n",
      "6600 0.76302403 0.5\n",
      "6800 0.76302403 0.5\n",
      "7000 0.76302403 0.5\n",
      "7200 0.76302403 0.5\n",
      "7400 0.76302403 0.5\n",
      "7600 0.76302403 0.5\n",
      "7800 0.76302403 0.5\n",
      "8000 0.76302403 0.5\n",
      "8200 0.76302403 0.5\n",
      "8400 0.76302403 0.5\n",
      "8600 0.76302403 0.5\n",
      "8800 0.76302403 0.5\n",
      "9000 0.76302403 0.5\n",
      "9200 0.76302403 0.5\n",
      "9400 0.76302403 0.5\n",
      "9600 0.76302403 0.5\n",
      "9800 0.76302403 0.5\n",
      "10000 0.76302403 0.5\n",
      "\n",
      "hypothesis :  [[0.39584363]\n",
      " [0.26931787]\n",
      " [0.40443945]\n",
      " [0.28183642]] \n",
      "predicted :  [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]] \n",
      "accuracy :  0.5\n"
     ]
    }
   ],
   "source": [
    "#왜 안돼 ;;;;;;;;;;;;;;;;;;;;;;;;; \n",
    "x_data=[[0,0],\n",
    "        [0,1],\n",
    "        [1,0],\n",
    "        [1,1]]\n",
    "\n",
    "y_data=[[0],\n",
    "        [1],\n",
    "        [1],\n",
    "        [0]]\n",
    "\n",
    "\n",
    "#input layer = (X,Y)\n",
    "tf.set_random_seed(777)\n",
    "X = tf.placeholder(tf.float32,shape=[None,2])\n",
    "Y = tf.placeholder(tf.float32,shape=[None,1])\n",
    "\n",
    "#hiddenlayer = layer1 \n",
    "W1=tf.Variable(tf.random_normal([2,3]), name='weight')    #W1=[[w1,w2],[w3,w4]] (hiddenlayer의 노드의 개수 = 2) -> (x1*w1+x2*w3)+b1\n",
    "b1=tf.Variable(tf.random_normal([3]), name='bias')\n",
    "layer1=tf.sigmoid(tf.matmul(X,W1)+b1)     #  X*W1+b1= layer1 -> f(layer1*W2+b2)\n",
    "                                          # layer1=1.0/(1+tf.exp(-(tf.matmul(X,W1)+b))))   #로지스틱 함수\n",
    "\n",
    "#output layer = hypothesis\n",
    "W2=tf.Variable(tf.random_normal([3,1]), name='weight')  #W2=[[w5],[w6]] -> \n",
    "b2=tf.Variable(tf.random_normal([1]), name='bias')\n",
    "hypothesis=tf.sigmoid(tf.matmul(layer1,W2)+b2)     \n",
    "\n",
    "cost= -tf.reduce_mean(Y*tf.log(hypothesis)+(1-Y)*tf.log(1-hypothesis))   #엔트로피\n",
    "\n",
    "train=tf.train.GradientDescentOptimizer(learning_rate=0.005).minimize(cost)  #그래디언트옵티마이저로 cost값을 가장 최소화하는 \n",
    "\n",
    "predicted=tf.cast(hypothesis > 0.5, dtype=tf.float32)  # 시그모이드함수에서 나온값이 0.5보다 크면 1(float32형), 작으면, 0(float32형)\n",
    "\n",
    "accuracy=tf.reduce_mean(tf.cast(tf.equal(predicted,Y), dtype=tf.float32))   \n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for step in range(10001):\n",
    "        cos, acc = sess.run([cost, accuracy], feed_dict={X:x_data,Y:y_data})\n",
    "        if step %200 ==0:\n",
    "            print(step, cos,acc)\n",
    "            \n",
    "    h,o,a=sess.run([hypothesis,predicted,accuracy],feed_dict={X:x_data,Y:y_data,})\n",
    "    \n",
    "    print('\\nhypothesis : ',h,\n",
    "         '\\npredicted : ',o,\n",
    "         '\\naccuracy : ',a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.8015749\n",
      "500 0.69902253\n",
      "1000 0.69180465\n",
      "1500 0.69113827\n",
      "2000 0.69083613\n",
      "2500 0.69054115\n",
      "3000 0.6902311\n",
      "3500 0.6899021\n",
      "4000 0.68955153\n",
      "4500 0.6891761\n",
      "5000 0.6887727\n",
      "5500 0.68833756\n",
      "6000 0.6878669\n",
      "6500 0.6873566\n",
      "7000 0.6868018\n",
      "7500 0.6861974\n",
      "8000 0.68553793\n",
      "8500 0.68481725\n",
      "9000 0.6840286\n",
      "9500 0.68316495\n",
      "10000 0.6822182\n",
      "10500 0.6811799\n",
      "11000 0.6800408\n",
      "11500 0.67879117\n",
      "12000 0.6774205\n",
      "12500 0.67591774\n",
      "13000 0.67427146\n",
      "13500 0.67246974\n",
      "14000 0.6705005\n",
      "14500 0.6683514\n",
      "15000 0.6660106\n",
      "15500 0.6634666\n",
      "16000 0.6607085\n",
      "16500 0.65772665\n",
      "17000 0.65451276\n",
      "17500 0.65106046\n",
      "18000 0.64736545\n",
      "18500 0.6434262\n",
      "19000 0.63924325\n",
      "19500 0.6348207\n",
      "20000 0.63016516\n",
      "20500 0.6252861\n",
      "21000 0.62019545\n",
      "21500 0.6149074\n",
      "22000 0.60943735\n",
      "22500 0.6038021\n",
      "23000 0.5980186\n",
      "23500 0.59210145\n",
      "24000 0.58606625\n",
      "24500 0.5799261\n",
      "25000 0.5736908\n",
      "25500 0.5673692\n",
      "26000 0.5609657\n",
      "26500 0.5544826\n",
      "27000 0.54791856\n",
      "27500 0.5412686\n",
      "28000 0.53452593\n",
      "28500 0.527681\n",
      "29000 0.5207217\n",
      "29500 0.5136336\n",
      "30000 0.5064009\n",
      "30500 0.49900556\n",
      "31000 0.49142957\n",
      "31500 0.48365197\n",
      "32000 0.47565475\n",
      "32500 0.46741855\n",
      "33000 0.45892733\n",
      "33500 0.4501649\n",
      "34000 0.44111913\n",
      "34500 0.43178254\n",
      "35000 0.42215163\n",
      "35500 0.41222975\n",
      "36000 0.4020262\n",
      "36500 0.39155763\n",
      "37000 0.380848\n",
      "37500 0.36992776\n",
      "38000 0.35883456\n",
      "38500 0.34761298\n",
      "39000 0.33631098\n",
      "39500 0.3249799\n",
      "40000 0.31367403\n",
      "40500 0.3024473\n",
      "41000 0.29135194\n",
      "41500 0.28043693\n",
      "42000 0.2697487\n",
      "42500 0.25932777\n",
      "43000 0.24920787\n",
      "43500 0.23941776\n",
      "44000 0.22997943\n",
      "44500 0.22090878\n",
      "45000 0.21221597\n",
      "45500 0.20390558\n",
      "46000 0.19597791\n",
      "46500 0.1884291\n",
      "47000 0.18125176\n",
      "47500 0.17443618\n",
      "48000 0.16797045\n",
      "48500 0.16184077\n",
      "49000 0.1560329\n",
      "49500 0.1505317\n",
      "50000 0.14532167\n",
      "50500 0.14038762\n",
      "51000 0.135714\n",
      "51500 0.13128616\n",
      "52000 0.12708983\n",
      "52500 0.12311126\n",
      "53000 0.119337216\n",
      "53500 0.11575529\n",
      "54000 0.11235387\n",
      "54500 0.10912153\n",
      "55000 0.10604806\n",
      "55500 0.10312365\n",
      "56000 0.100339115\n",
      "56500 0.0976859\n",
      "57000 0.09515613\n",
      "57500 0.092742205\n",
      "58000 0.09043724\n",
      "58500 0.08823474\n",
      "59000 0.08612855\n",
      "59500 0.084113196\n",
      "60000 0.08218348\n",
      "60500 0.08033426\n",
      "61000 0.0785612\n",
      "61500 0.07685994\n",
      "62000 0.07522625\n",
      "62500 0.073656894\n",
      "63000 0.07214822\n",
      "63500 0.07069695\n",
      "64000 0.06930008\n",
      "64500 0.06795483\n",
      "65000 0.06665844\n",
      "65500 0.065408394\n",
      "66000 0.06420253\n",
      "66500 0.06303839\n",
      "67000 0.061914355\n",
      "67500 0.060828116\n",
      "68000 0.059777968\n",
      "68500 0.058762427\n",
      "69000 0.05777947\n",
      "69500 0.056827888\n",
      "70000 0.055906326\n",
      "70500 0.055013336\n",
      "71000 0.054147463\n",
      "71500 0.05330771\n",
      "72000 0.052492842\n",
      "72500 0.05170202\n",
      "73000 0.050933998\n",
      "73500 0.050188094\n",
      "74000 0.04946307\n",
      "74500 0.048758384\n",
      "75000 0.04807302\n",
      "75500 0.047406305\n",
      "76000 0.04675748\n",
      "76500 0.04612585\n",
      "77000 0.045510627\n",
      "77500 0.04491133\n",
      "78000 0.044327363\n",
      "78500 0.043757796\n",
      "79000 0.04320298\n",
      "79500 0.042661652\n",
      "80000 0.042133335\n",
      "80500 0.04161774\n",
      "81000 0.04111456\n",
      "81500 0.040623173\n",
      "82000 0.040143497\n",
      "82500 0.03967443\n",
      "83000 0.03921638\n",
      "83500 0.038768537\n",
      "84000 0.03833044\n",
      "84500 0.037902575\n",
      "85000 0.037483536\n",
      "85500 0.037073724\n",
      "86000 0.036672823\n",
      "86500 0.036280155\n",
      "87000 0.03589597\n",
      "87500 0.035519503\n",
      "88000 0.035150602\n",
      "88500 0.03478977\n",
      "89000 0.034435872\n",
      "89500 0.034089178\n",
      "90000 0.03374905\n",
      "90500 0.03341575\n",
      "91000 0.033088665\n",
      "91500 0.0327683\n",
      "92000 0.03245391\n",
      "92500 0.03214538\n",
      "93000 0.031842694\n",
      "93500 0.031545386\n",
      "94000 0.031253558\n",
      "94500 0.030966971\n",
      "95000 0.030686032\n",
      "95500 0.030409697\n",
      "96000 0.030138377\n",
      "96500 0.02987187\n",
      "97000 0.029609803\n",
      "97500 0.029352415\n",
      "98000 0.029099159\n",
      "98500 0.028850473\n",
      "99000 0.028606322\n",
      "99500 0.028365705\n",
      "100000 0.02812914\n",
      "\n",
      "Hypothesis:  [[0.00751489 0.00977883]\n",
      " [0.97006595 0.96849656]\n",
      " [0.9741144  0.9748664 ]\n",
      " [0.04595849 0.04546249]] \n",
      "Predicted:  [[0. 0.]\n",
      " [1. 1.]\n",
      " [1. 1.]\n",
      " [0. 0.]] \n",
      "Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "x_data = [[0,0],\n",
    "          [0,1],\n",
    "          [1,0],\n",
    "          [1,1]]\n",
    "y_data = [[0],\n",
    "          [1],\n",
    "          [1],\n",
    "          [0]]\n",
    "\n",
    "#XOR 문제  -> nural network\n",
    "\n",
    "X=tf.placeholder(tf.float32, shape=[None,2]) #input node : data 속성의 개수\n",
    "Y=tf.placeholder(tf.float32, shape=[None,1]) #output node : label의 개수\n",
    "\n",
    "W1=tf.Variable(tf.random_normal([2,3]),name='weight') #[2,2] = w1,2,3,4 \n",
    "b1=tf.Variable(tf.random_normal([3]),name='bias')                    \n",
    "layer1 = tf.sigmoid(tf.matmul(X,W1)+b1) #sigmoid통과\n",
    "\n",
    "W2=tf.Variable(tf.random_normal([3,2]),name='weight') #[2,2] = w1,2,3,4 \n",
    "b2=tf.Variable(tf.random_normal([2]),name='bias')                       \n",
    "layer2 = tf.sigmoid(tf.matmul(X,W1)+b1) #sigmoid통과\n",
    "\n",
    "W3=tf.Variable(tf.random_normal([2,1]),name='weight') #[2,1] = w5,6    \n",
    "b3=tf.Variable(tf.random_normal([1]),name='bias')\n",
    "hypothesis=tf.sigmoid(tf.matmul(layer1,W2)+b2) #matmul:layer1*W2 #sigmoid 한번 더 통과\n",
    "\n",
    "\n",
    "cost = -tf.reduce_mean(Y*tf.log(hypothesis)+(1-Y)*tf.log(1-hypothesis))\n",
    "\n",
    "\n",
    "train=tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
    "predicted = tf.cast(hypothesis>0.5,dtype=tf.float32) \n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted,Y),dtype=tf.float32)) \n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(100001):\n",
    "        cost_val,_=sess.run([cost,train],feed_dict={X:x_data, Y:y_data})\n",
    "        if step % 500==0:\n",
    "            print(step,cost_val) \n",
    "            \n",
    "    h,c,a=sess.run([hypothesis,predicted,accuracy],feed_dict={X:x_data,Y:y_data})\n",
    "    print(\"\\nHypothesis: \",h,\n",
    "          \"\\nPredicted: \",c,\n",
    "          \"\\nAccuracy: \",a) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.7720155\n",
      "200 0.71944475\n",
      "400 0.7065798\n",
      "600 0.7026918\n",
      "800 0.7008691\n",
      "1000 0.69963014\n",
      "1200 0.69864285\n",
      "1400 0.6978183\n",
      "1600 0.69712096\n",
      "1800 0.69652915\n",
      "2000 0.69602656\n",
      "2200 0.6955992\n",
      "2400 0.69523597\n",
      "2600 0.694927\n",
      "2800 0.694664\n",
      "3000 0.6944402\n",
      "3200 0.69424963\n",
      "3400 0.6940873\n",
      "3600 0.6939491\n",
      "3800 0.6938312\n",
      "4000 0.69373083\n",
      "4200 0.69364524\n",
      "4400 0.6935723\n",
      "4600 0.69351006\n",
      "4800 0.69345695\n",
      "5000 0.6934117\n",
      "5200 0.6933729\n",
      "5400 0.69334\n",
      "5600 0.6933118\n",
      "5800 0.6932878\n",
      "6000 0.6932672\n",
      "6200 0.6932498\n",
      "6400 0.6932348\n",
      "6600 0.69322205\n",
      "6800 0.69321114\n",
      "7000 0.69320184\n",
      "7200 0.69319385\n",
      "7400 0.69318706\n",
      "7600 0.6931813\n",
      "7800 0.6931763\n",
      "8000 0.693172\n",
      "8200 0.6931684\n",
      "8400 0.6931653\n",
      "8600 0.69316274\n",
      "8800 0.6931604\n",
      "9000 0.6931585\n",
      "9200 0.6931569\n",
      "9400 0.69315547\n",
      "9600 0.6931542\n",
      "9800 0.6931532\n",
      "10000 0.69315237\n",
      "\n",
      "Hypothesis:  [[0.502605  ]\n",
      " [0.500328  ]\n",
      " [0.50048965]\n",
      " [0.49821255]] \n",
      "Predicted:  [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]] \n",
      "Accuracy:  0.75\n"
     ]
    }
   ],
   "source": [
    "x_data = [[0,0],\n",
    "          [0,1],\n",
    "          [1,0],\n",
    "          [1,1]]\n",
    "y_data = [[0],\n",
    "          [1],\n",
    "          [1],\n",
    "          [0]]\n",
    "\n",
    "#XOR 문제  -> nural network\n",
    "\n",
    "X=tf.placeholder(tf.float32, shape=[None,2]) #input node : data 속성의 개수\n",
    "Y=tf.placeholder(tf.float32, shape=[None,1]) #output node : label의 개수\n",
    "\n",
    "W=tf.Variable(tf.random_normal([2,1]),name='weight') #[2,2] = w1,2,3,4 \n",
    "b=tf.Variable(tf.random_normal([1]),name='bias')                    \n",
    "\n",
    "hypothesis=tf.sigmoid(tf.matmul(X,W)+b) #matmul:layer1*W2 #sigmoid 한번 더 통과\n",
    "cost = -tf.reduce_mean(Y*tf.log(hypothesis)+(1-Y)*tf.log(1-hypothesis))\n",
    "\n",
    "train=tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
    "predicted = tf.cast(hypothesis>0.5,dtype=tf.float32) \n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted,Y),dtype=tf.float32)) \n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(10001):\n",
    "        cost_val,_=sess.run([cost,train],feed_dict={X:x_data, Y:y_data})\n",
    "                                                    \n",
    "        if step % 200==0:\n",
    "            print(step,cost_val) \n",
    "            \n",
    "    h,c,a=sess.run([hypothesis,predicted,accuracy],feed_dict={X:x_data,Y:y_data})\n",
    "    print(\"\\nHypothesis: \",h,\n",
    "          \"\\nPredicted: \",c,\n",
    "          \"\\nAccuracy: \",a) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.8894654\n",
      "200 0.69911885\n",
      "400 0.6948195\n",
      "600 0.6938361\n",
      "800 0.6929045\n",
      "1000 0.6919739\n",
      "1200 0.69103837\n",
      "1400 0.69009256\n",
      "1600 0.68913144\n",
      "1800 0.6881503\n",
      "2000 0.6871433\n",
      "2200 0.68610597\n",
      "2400 0.68503284\n",
      "2600 0.6839185\n",
      "2800 0.6827572\n",
      "3000 0.68154347\n",
      "3200 0.68027115\n",
      "3400 0.6789339\n",
      "3600 0.67752516\n",
      "3800 0.67603797\n",
      "4000 0.6744644\n",
      "4200 0.6727969\n",
      "4400 0.67102665\n",
      "4600 0.6691444\n",
      "4800 0.6671401\n",
      "5000 0.665003\n",
      "5200 0.66272074\n",
      "5400 0.66028\n",
      "5600 0.65766585\n",
      "5800 0.65486157\n",
      "6000 0.6518482\n",
      "6200 0.64860415\n",
      "6400 0.6451049\n",
      "6600 0.6413226\n",
      "6800 0.637226\n",
      "7000 0.63278025\n",
      "7200 0.6279479\n",
      "7400 0.62268955\n",
      "7600 0.61696583\n",
      "7800 0.61074066\n",
      "8000 0.6039824\n",
      "8200 0.59666693\n",
      "8400 0.5887773\n",
      "8600 0.5803022\n",
      "8800 0.57123274\n",
      "9000 0.56156015\n",
      "9200 0.55127406\n",
      "9400 0.5403635\n",
      "9600 0.5288201\n",
      "9800 0.5166414\n",
      "10000 0.5038352\n",
      "\n",
      "Hypothesis:  [[0.38921678]\n",
      " [0.48202723]\n",
      " [0.74409986]\n",
      " [0.39148313]] \n",
      "Predicted:  [[0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]] \n",
      "Accuracy:  0.75\n",
      "0 0.8926536\n",
      "200 0.7163993\n",
      "400 0.7062924\n",
      "600 0.6973221\n",
      "800 0.6890396\n",
      "1000 0.6812018\n",
      "1200 0.67359954\n",
      "1400 0.6660502\n",
      "1600 0.6583928\n",
      "1800 0.65048504\n",
      "2000 0.64220136\n",
      "2200 0.63343287\n",
      "2400 0.62408686\n",
      "2600 0.61408615\n",
      "2800 0.60336804\n",
      "3000 0.5918822\n",
      "3200 0.5795909\n",
      "3400 0.5664682\n",
      "3600 0.5525022\n",
      "3800 0.5376974\n",
      "4000 0.52207804\n",
      "4200 0.50569135\n",
      "4400 0.48860857\n",
      "4600 0.47092545\n",
      "4800 0.45276046\n",
      "5000 0.43425044\n",
      "5200 0.4155454\n",
      "5400 0.39680183\n",
      "5600 0.37817547\n",
      "5800 0.3598149\n",
      "6000 0.3418558\n",
      "6200 0.3244153\n",
      "6400 0.30759236\n",
      "6600 0.29146275\n",
      "6800 0.27608186\n",
      "7000 0.26148504\n",
      "7200 0.24768965\n",
      "7400 0.23469722\n",
      "7600 0.22249663\n",
      "7800 0.21106625\n",
      "8000 0.20037687\n",
      "8200 0.19039366\n",
      "8400 0.18107828\n",
      "8600 0.17239049\n",
      "8800 0.1642896\n",
      "9000 0.15673517\n",
      "9200 0.14968792\n",
      "9400 0.14311045\n",
      "9600 0.13696717\n",
      "9800 0.13122489\n",
      "10000 0.12585244\n",
      "\n",
      "Hypothesis:  [[0.11371288]\n",
      " [0.88992333]\n",
      " [0.8697014 ]\n",
      " [0.11870797]] \n",
      "Predicted:  [[0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]] \n",
      "Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "x_data = [[0,0],\n",
    "          [0,1],\n",
    "          [1,0],\n",
    "          [1,1]]\n",
    "y_data = [[0],\n",
    "          [1],\n",
    "          [1],\n",
    "          [0]]\n",
    "\n",
    "#XOR 문제  -> nural network\n",
    "i=5\n",
    "while(i):  #레이어의 수 : 2개, layer1의 노드수 : 10개, layer2의 노드수 : 10개\n",
    "\n",
    "    X=tf.placeholder(tf.float32, shape=[None,2]) #input node : data 속성의 개수\n",
    "    Y=tf.placeholder(tf.float32, shape=[None,1]) #output node : label의 개수\n",
    "\n",
    "    W1=tf.Variable(tf.random_normal([2,10]),name='weight') #[2,2] = w1,2,3,4 \n",
    "    b1=tf.Variable(tf.random_normal([10]),name='bias')                    \n",
    "\n",
    "    layer1=tf.sigmoid(tf.matmul(X,W1)+b1)   # (N x 2) x (2 x 4) -> (N x 4)\n",
    "\n",
    "    W2=tf.Variable(tf.random_normal([10,10]),name='weight')  \n",
    "    b2=tf.Variable(tf.random_normal([10]),name='bias') \n",
    "\n",
    "    layer2=tf.sigmoid(tf.matmul(layer1,W2)+b2)   # (N x 4) x (4 x 5) -> (N x 5)\n",
    "\n",
    "    W3=tf.Variable(tf.random_normal([10,1]),name='weight')\n",
    "    b3=tf.Variable(tf.random_normal([1]),name='bias')\n",
    "\n",
    "    hypothesis=tf.sigmoid(tf.matmul(layer2,W3)+b3)  \n",
    "\n",
    "    cost = -tf.reduce_mean(Y*tf.log(hypothesis)+(1-Y)*tf.log(1-hypothesis))\n",
    "\n",
    "    train=tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
    "    predicted = tf.cast(hypothesis>0.5,dtype=tf.float32) \n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted,Y),dtype=tf.float32)) \n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        for step in range(10001):\n",
    "            cost_val,_=sess.run([cost,train],feed_dict={X:x_data, Y:y_data})\n",
    "\n",
    "            if step % 200==0:\n",
    "                print(step,cost_val) \n",
    "\n",
    "        h,c,a=sess.run([hypothesis,predicted,accuracy],feed_dict={X:x_data,Y:y_data})\n",
    "        print(\"\\nHypothesis: \",h,\n",
    "              \"\\nPredicted: \",c,\n",
    "              \"\\nAccuracy: \",a) \n",
    "        if a==1:\n",
    "            break\n",
    "        else:\n",
    "            i-=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer3=tf.sigmoid(tf.matmul(layer2,W3)+b3)  # (N x 3) x (3 x 2) -> (N x 2)\n",
    "\n",
    "W4=tf.Variable(tf.random_normal([10,10]),name='weight')\n",
    "b4=tf.Variable(tf.random_normal([10]),name='bias')\n",
    "\n",
    "layer4=tf.sigmoid(tf.matmul(layer3,W4)+b4)  # (N x 3) x (3 x 4) -> (N x 4)\n",
    "\n",
    "W5=tf.Variable(tf.random_normal([10,10]),name='weight')\n",
    "b5=tf.Variable(tf.random_normal([10]), name='bias')\n",
    "\n",
    "layer5=tf.sigmoid(tf.matmul(layer4,W5)+b5)\n",
    "\n",
    "W6=tf.Variable(tf.random_normal([10,10]), name='weight')\n",
    "b6=tf.Variable(tf.random_normal([10]), name='bias')\n",
    "\n",
    "layer6=tf.sigmoid(tf.matmul(layer5,W6)+b6)\n",
    "W7=tf.Variable(tf.random_normal([10,1]), name='weight')\n",
    "b7=tf.Variable(tf.random_normal([1]), name='bias')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
