{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outs = [3, 4, -1, 1, 0.3333333333333333]\n"
     ]
    }
   ],
   "source": [
    "a=tf.constant(3)  #constant : 노드\n",
    "b=tf.constant(4)\n",
    "c=tf.subtract(a,b)  \n",
    "d=tf.pow(c,2)\n",
    "e=tf.divide(d,a)  # 곱 : multiply, 합 : add, 차 : subtract, 할 : divide\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    fetches=[a,b,c,d,e]   #a,b,c,d,e를 위해 필요한 연산을 메모리에 올\n",
    "    outs=sess.run(fetches)  #a~e까지의 값을 가져와서 outs에 넣음\n",
    "    print(\"outs = {}\".format(outs))  #outs의 값 출력\n",
    "\n",
    "#sess=tf.Session()\n",
    "#output=sess.run(e)\n",
    "#sess.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3)\n"
     ]
    }
   ],
   "source": [
    "a=tf.constant([[1,2,3],[4,5,6]])\n",
    "print(a.get_shape())  # 2 x 3 metrix만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3,)\n"
     ]
    }
   ],
   "source": [
    "x=tf.constant([1,0,1])\n",
    "print(x.get_shape())  #1차원배열이지만, 1은 생략됨\n",
    "#b=tf.matmul(a,x)  # (2x3) 행렬곱 (3x) 이므로 오류가 발생"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 1)\n",
      "matmul : \n",
      " [[ 4]\n",
      " [10]]\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "numpy와 tensorflow비교 (newaxis vs expand_dims())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3,)\n",
      "matmul : [[ 4]\n",
      " [10]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "m1=np.array([1,0,1])\n",
    "print(m1.shape)  #m1의 차원값을 출력\n",
    "\n",
    "m1=m1[:,np.newaxis]  #1차원의 열을 어거지로 생성\n",
    "b=tf.matmul(a,m1)  #행렬곱 시행\n",
    "print(\"matmul : {}\".format(b.eval()))  #행렬곱한 행렬요소값을 리턴"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.Session() vs InteractiveSession()\n",
    "1) tf.Session() : 항상 with절과 함께 사용(default 세션을 지정해주지 않기 때문) -> .eval()함수사용 불가 (with문이 없을 때)\n",
    "    \n",
    "2) InteractiveSession() : 자동으로 터미널에 default session을 할당 -> .eval()함수 사용가능\n",
    "\n",
    "3) sess=tf.InteractiveSession()\n",
    "   seses.run() : 세션 실행문\n",
    "   \n",
    "4) sess.close() : 세션 종료문"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 1)\n",
      "matmul : \n",
      " [[ 4]\n",
      " [10]]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\venv\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py:1750: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    }
   ],
   "source": [
    "x=tf.expand_dims(x,1)  #x에 3x1로 dimension을 확장\n",
    "print(x.get_shape()) \n",
    "b=tf.matmul(a,x)    #\n",
    "sess=tf.InteractiveSession()  \n",
    "print('matmul : \\n {}\\n'.format(b.eval()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre run : \n",
      "<tf.Variable 'var_19:0' shape=(1, 5) dtype=float32_ref>\n",
      "\n",
      "\n",
      "post_var : \n",
      "[[-2.6804054   0.8892643   1.6956359  -0.66288364  0.3425908 ]]\n",
      "<tf.Variable 'var_19:0' shape=(1, 5) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "val=tf.random_normal((1,5),0,1)  #1행 5열로 값을 랜덤하게 가져옴.(평균 : 0, 표준편차 : 1로)\n",
    "var=tf.Variable(val, name='var')\n",
    "print('pre run : \\n{}\\n'.format(var))\n",
    "\n",
    "init=tf.global_variables_initializer()  #초기화 값 \n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    post_var=sess.run(var)\n",
    "    \n",
    "print('\\npost_var : \\n{}'.format(post_var))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "딥러닝에서 데이터에 대한 학습이 이루어질 때 학습할 데이터들을 입력해줘야 한다. 예를 들어, 2장에서 잠깐 살펴보았던, MNIST 데이터를 학습한다고 할때 입력값으로 MNIST 이미지 데이터를 입력값으로 넣어 줘야 했다. 텐서플로에서는 입력값을 넣어주기 위해 플레이스홀더(placeholder)라는 것이 있다. 플레이스홀더는 데이터를 입력받는 비어있는 변수라고 생각할 수 있다. 먼저 그래프를 구성하고, 그 그래프가 실행되는 시점에 입력 데이터를 넣어주는 데 사용한다.\n",
    "\n",
    "플레이스홀더는 shape 인수를 유동적으로 지정할 수 있다. 예를 들어, None으로 지정되면 이 플레이스홀더는 모든 크기의 데이터를 받을 수 있다. 주로 배치단위(batch size)의 샘플 데이터 개수에 해당 되는 부분(데이터의 행)은 None을 사용하고, 데이터 Feature의 길이(데이터의 열)는 고정된 값을 사용한다. \n",
    "\n",
    "ph = tf.placeholder(tf.float32, shape=(None, 10))\n",
    "플레이스홀더를 정의하면 반드시 그래프 실행 단계에서 입력값을 넣어줘야 하며, 그렇지 않을 경우 에러가 나타난다. 입력 데이터는 딕셔너리(dictionary)형태로 session.run()메소드를 통해 전달된다. 딕셔너리의 키(key)는 플레이스홀더 변수 이름에 해당하며 값(value)은 list 또는 NumPy 배열이다.\n",
    "\n",
    "sess.run(s, feed_dict={ph: data})\n",
    "\n",
    "https://excelsior-cjh.tistory.com/151"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xwb=[[ 3.966744  ]\n",
      " [-0.12428355]\n",
      " [-0.7186617 ]\n",
      " [-0.01700443]\n",
      " [-0.6235158 ]]\n",
      "x=[[ 0.28152335 -1.0873142  -0.10008131  0.9490972  -0.17908643  1.9063516\n",
      "  -1.1211256   2.1829665  -0.69462043 -0.5508108 ]\n",
      " [-0.7696937  -1.4199489  -0.45710313  1.0524241   0.33085787 -0.8002776\n",
      "   0.8853537   0.06422591  1.1357985  -0.18612957]\n",
      " [ 0.79037726 -0.0365244  -0.57014227 -0.6687889  -0.87560284  0.5462008\n",
      "  -0.33357745  0.33013386  0.09076046 -0.7838045 ]\n",
      " [-1.2250643   0.47747952  0.36636657  1.1028085   0.44931766 -0.98258036\n",
      "  -1.2166041   1.0916824  -1.7302413  -0.31013316]\n",
      " [ 2.1144855  -0.7097177   0.24388897 -1.346157    2.1827571   1.302949\n",
      "  -0.86375755  1.1857052  -1.5114818   0.3294574 ]]\n",
      "y=[[-1.2681309 ]\n",
      " [-0.4167219 ]\n",
      " [-0.06693975]\n",
      " [ 0.20285742]\n",
      " [ 0.578951  ]\n",
      " [ 1.3877394 ]\n",
      " [-1.6439235 ]\n",
      " [ 0.39964703]\n",
      " [ 1.1751881 ]\n",
      " [-0.41833964]]\n",
      "s=\n",
      "3.9667439460754395\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_data = np.random.randn(5, 10)\n",
    "w_data = np.random.randn(10, 1)\n",
    "\n",
    "with tf.Graph().as_default():  #그래프 실행\n",
    "    x=tf.placeholder(tf.float32, shape=(5,10))  # 5 x 10 행렬\n",
    "    w=tf.placeholder(tf.float32, shape=(10,1))  # 10 x 1 행렬\n",
    "    b=tf.fill((5,1),-1.)  # 5 x 1 행렬을 만들고, -1로 채워라\n",
    "    xw=tf.matmul(x,w)\n",
    "    \n",
    "    xwb=xw+b\n",
    "    with tf.Session() as sess:\n",
    "        outs=sess.run([xwb,x,w,s],feed_dict={x : x_data,w : w_data})\n",
    "        \n",
    "    \n",
    "print(\"xwb={}\\nx={}\\ny={}\\ns=\\n{}\\n\".format(outs[0],outs[1],outs[2],outs[3]))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "yMat=ws*xMat+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3) (1, 2)\n",
      "[[1 2 3]\n",
      " [1 3 4]]\n",
      "[[2]\n",
      " [3]]\n",
      "[[ 2  5  7]\n",
      " [ 5 13 18]\n",
      " [ 7 18 25]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "matrix([[ 0.5],\n",
       "        [-0.5],\n",
       "        [-1.5]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def standRegree(xArr, yArr):  \n",
    "    xMat=np.mat(xArr)  #(2 x 2)\n",
    "    yMat=np.mat(yArr).T  #차원 맞추기 (2x1)\n",
    "    print(xMat)\n",
    "    print(yMat)\n",
    "    \n",
    "    \n",
    "    xTx=xMat.T*xMat  #2*1\n",
    "    print(xTx)\n",
    "    if np.linalg.det(xTx)==0.0: #ad-bc==0이라면\n",
    "        print(\"This matrix is singular\")\n",
    "        return\n",
    "    \n",
    "    ws=xTx.I*(xMat.T*yMat)  # (2x2)((2x2)(2x1))  -> (2x2)(2x1) -> (2x1)\n",
    "    return ws\n",
    "\n",
    "xArr=np.array([[1,1],[1,2]])  # (bias값,x값)   #(b=1),x=1 -> y=2, (b=1),x=2 -> y=3 \n",
    "yArr=np.array([[2,3]])        # (y값...)\n",
    "\n",
    "xArr=np.array([[1,2,3],[1,3,4]])\n",
    "yArr=np.array([[2,3]])\n",
    "print(xArr.shape,yArr.shape)\n",
    "standRegree(xArr,yArr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [array([[0.025, 0.065, 0.09 ]], dtype=float32)]\n",
      "50 [array([[0.11737743, 0.33145082, 0.44882843]], dtype=float32)]\n",
      "100 [array([[0.10898961, 0.3376941 , 0.44668385]], dtype=float32)]\n",
      "150 [array([[0.10075645, 0.34381738, 0.44457403]], dtype=float32)]\n",
      "200 [array([[0.09267656, 0.34982663, 0.4425035 ]], dtype=float32)]\n",
      "250 [array([[0.0847471 , 0.35572404, 0.44047153]], dtype=float32)]\n",
      "300 [array([[0.07696518, 0.36151174, 0.4384773 ]], dtype=float32)]\n",
      "350 [array([[0.06932814, 0.3671918 , 0.4365202 ]], dtype=float32)]\n",
      "400 [array([[0.06183323, 0.37276605, 0.43459952]], dtype=float32)]\n",
      "450 [array([[0.05447784, 0.3782366 , 0.43271458]], dtype=float32)]\n",
      "500 [array([[0.04725936, 0.38360524, 0.43086475]], dtype=float32)]\n",
      "550 [array([[0.04017525, 0.3888739 , 0.4290494 ]], dtype=float32)]\n",
      "600 [array([[0.03322301, 0.39404452, 0.42726782]], dtype=float32)]\n",
      "650 [array([[0.02640015, 0.39911887, 0.42551944]], dtype=float32)]\n",
      "700 [array([[0.0197043 , 0.40409887, 0.42380354]], dtype=float32)]\n",
      "750 [array([[0.01313305, 0.40898615, 0.4221196 ]], dtype=float32)]\n",
      "800 [array([[0.00668413, 0.41378245, 0.42046696]], dtype=float32)]\n",
      "850 [array([[3.5524409e-04, 4.1848949e-01, 4.1884515e-01]], dtype=float32)]\n",
      "900 [array([[-0.00585585,  0.42310888,  0.41725352]], dtype=float32)]\n",
      "950 [array([[-0.01195133,  0.4276423 ,  0.41569147]], dtype=float32)]\n",
      "1000 [array([[-0.01793336,  0.4320913 ,  0.41415855]], dtype=float32)]\n",
      "1050 [array([[-0.02380403,  0.43645748,  0.41265413]], dtype=float32)]\n",
      "1100 [array([[-0.02956542,  0.4407424 ,  0.41117775]], dtype=float32)]\n",
      "1150 [array([[-0.03521958,  0.4449475 ,  0.40972888]], dtype=float32)]\n",
      "1200 [array([[-0.0407685 ,  0.44907442,  0.40830693]], dtype=float32)]\n",
      "1250 [array([[-0.04621411,  0.4531244 ,  0.4069115 ]], dtype=float32)]\n",
      "1300 [array([[-0.05155841,  0.4570993 ,  0.40554193]], dtype=float32)]\n",
      "1350 [array([[-0.05680322,  0.461     ,  0.4041979 ]], dtype=float32)]\n",
      "1400 [array([[-0.0619504 ,  0.46482816,  0.40287885]], dtype=float32)]\n",
      "1450 [array([[-0.06700175,  0.4685852 ,  0.40158427]], dtype=float32)]\n",
      "1500 [array([[-0.07195917,  0.47227216,  0.4003139 ]], dtype=float32)]\n",
      "1550 [array([[-0.07682424,  0.47589055,  0.3990672 ]], dtype=float32)]\n",
      "1600 [array([[-0.08159875,  0.47944158,  0.3978436 ]], dtype=float32)]\n",
      "1650 [array([[-0.0862844 ,  0.48292652,  0.39664283]], dtype=float32)]\n",
      "1700 [array([[-0.09088284,  0.48634648,  0.39546448]], dtype=float32)]\n",
      "1750 [array([[-0.09539563,  0.48970285,  0.39430797]], dtype=float32)]\n",
      "1800 [array([[-0.0998245 ,  0.49299678,  0.39317304]], dtype=float32)]\n",
      "1850 [array([[-0.1041709 ,  0.4962293 ,  0.39205924]], dtype=float32)]\n",
      "1900 [array([[-0.1084364 ,  0.49940166,  0.3909662 ]], dtype=float32)]\n",
      "1950 [array([[-0.1126225 ,  0.502515  ,  0.38989347]], dtype=float32)]\n",
      "2000 [array([[-0.11673067,  0.5055705 ,  0.38884068]], dtype=float32)]\n",
      "2050 [array([[-0.12076233,  0.508569  ,  0.3878075 ]], dtype=float32)]\n",
      "2100 [array([[-0.12471899,  0.5115117 ,  0.38679358]], dtype=float32)]\n",
      "2150 [array([[-0.12860197,  0.5143996 ,  0.38579848]], dtype=float32)]\n",
      "2200 [array([[-0.13241272,  0.51723385,  0.38482195]], dtype=float32)]\n",
      "2250 [array([[-0.1361525 ,  0.52001524,  0.38386363]], dtype=float32)]\n",
      "2300 [array([[-0.13982272,  0.5227449 ,  0.3829231 ]], dtype=float32)]\n",
      "2350 [array([[-0.14342462,  0.5254237 ,  0.38200012]], dtype=float32)]\n",
      "2400 [array([[-0.14695951,  0.52805275,  0.38109425]], dtype=float32)]\n",
      "2450 [array([[-0.15042855,  0.53063273,  0.3802053 ]], dtype=float32)]\n",
      "2500 [array([[-0.15383308,  0.53316474,  0.3793329 ]], dtype=float32)]\n",
      "2550 [array([[-0.1571742,  0.5356496,  0.3784767]], dtype=float32)]\n",
      "2600 [array([[-0.16045313,  0.53808826,  0.3776365 ]], dtype=float32)]\n",
      "2650 [array([[-0.16367105,  0.5404815 ,  0.37681192]], dtype=float32)]\n",
      "2700 [array([[-0.16682906,  0.5428302 ,  0.3760026 ]], dtype=float32)]\n",
      "2750 [array([[-0.16992831,  0.54513526,  0.3752084 ]], dtype=float32)]\n",
      "2800 [array([[-0.1729698,  0.5473973,  0.374429 ]], dtype=float32)]\n",
      "2850 [array([[-0.17595473,  0.54961735,  0.37366405]], dtype=float32)]\n",
      "2900 [array([[-0.17888412,  0.5517961 ,  0.37291333]], dtype=float32)]\n",
      "2950 [array([[-0.18175896,  0.55393416,  0.37217665]], dtype=float32)]\n",
      "3000 [array([[-0.18458028,  0.5560324 ,  0.3714537 ]], dtype=float32)]\n",
      "3050 [array([[-0.18734908,  0.55809164,  0.3707442 ]], dtype=float32)]\n",
      "3100 [array([[-0.19006637,  0.56011254,  0.3700479 ]], dtype=float32)]\n",
      "3150 [array([[-0.19273306,  0.5620959 ,  0.3693645 ]], dtype=float32)]\n",
      "3200 [array([[-0.19535014,  0.56404227,  0.3686939 ]], dtype=float32)]\n",
      "3250 [array([[-0.19791849,  0.5659524 ,  0.3680357 ]], dtype=float32)]\n",
      "3300 [array([[-0.20043904,  0.567827  ,  0.36738983]], dtype=float32)]\n",
      "3350 [array([[-0.20291269,  0.56966674,  0.36675596]], dtype=float32)]\n",
      "3400 [array([[-0.2053403 ,  0.57147217,  0.3661339 ]], dtype=float32)]\n",
      "3450 [array([[-0.20772271,  0.573244  ,  0.36552343]], dtype=float32)]\n",
      "3500 [array([[-0.21006076,  0.5749829 ,  0.36492428]], dtype=float32)]\n",
      "3550 [array([[-0.21235532,  0.5766893 ,  0.36433637]], dtype=float32)]\n",
      "3600 [array([[-0.21460716,  0.578364  ,  0.36375934]], dtype=float32)]\n",
      "3650 [array([[-0.2168171 ,  0.58000755,  0.3631931 ]], dtype=float32)]\n",
      "3700 [array([[-0.21898594,  0.5816206 ,  0.3626373 ]], dtype=float32)]\n",
      "3750 [array([[-0.22111434,  0.5832035 ,  0.3620919 ]], dtype=float32)]\n",
      "3800 [array([[-0.22320312,  0.58475703,  0.36155665]], dtype=float32)]\n",
      "3850 [array([[-0.22525306,  0.5862816 ,  0.36103135]], dtype=float32)]\n",
      "3900 [array([[-0.22726482,  0.5877778 ,  0.36051586]], dtype=float32)]\n",
      "3950 [array([[-0.22923896,  0.58924603,  0.36000994]], dtype=float32)]\n",
      "4000 [array([[-0.23117661,  0.59068704,  0.35951346]], dtype=float32)]\n",
      "4050 [array([[-0.2330781 ,  0.59210116,  0.35902625]], dtype=float32)]\n",
      "4100 [array([[-0.23494424,  0.5934891 ,  0.35854805]], dtype=float32)]\n",
      "4150 [array([[-0.23677562,  0.59485114,  0.35807872]], dtype=float32)]\n",
      "4200 [array([[-0.23857291,  0.59618783,  0.35761818]], dtype=float32)]\n",
      "4250 [array([[-0.24033679,  0.5974996 ,  0.35716617]], dtype=float32)]\n",
      "4300 [array([[-0.2420678 ,  0.5987869 ,  0.35672268]], dtype=float32)]\n",
      "4350 [array([[-0.24376658,  0.6000503 ,  0.3562874 ]], dtype=float32)]\n",
      "4400 [array([[-0.24543373,  0.6012901 ,  0.3558603 ]], dtype=float32)]\n",
      "4450 [array([[-0.24706985,  0.6025069 ,  0.35544103]], dtype=float32)]\n",
      "4500 [array([[-0.24867554,  0.603701  ,  0.3550296 ]], dtype=float32)]\n",
      "4550 [array([[-0.25025135,  0.604873  ,  0.35462582]], dtype=float32)]\n",
      "4600 [array([[-0.2517978,  0.606023 ,  0.3542296]], dtype=float32)]\n",
      "4650 [array([[-0.25331548,  0.6071517 ,  0.35384074]], dtype=float32)]\n",
      "4700 [array([[-0.2548049 ,  0.6082593 ,  0.35345912]], dtype=float32)]\n",
      "4750 [array([[-0.2562666 ,  0.60934645,  0.3530846 ]], dtype=float32)]\n",
      "4800 [array([[-0.25770107,  0.6104133 ,  0.35271698]], dtype=float32)]\n",
      "4850 [array([[-0.2591089 ,  0.61146027,  0.35235626]], dtype=float32)]\n",
      "4900 [array([[-0.2604905 ,  0.61248773,  0.3520023 ]], dtype=float32)]\n",
      "4950 [array([[-0.2618464 ,  0.613496  ,  0.35165492]], dtype=float32)]\n",
      "5000 [array([[-0.26317695,  0.61448556,  0.35131398]], dtype=float32)]\n",
      "5050 [array([[-0.26448283,  0.61545676,  0.35097933]], dtype=float32)]\n",
      "5100 [array([[-0.26576433,  0.6164098 ,  0.35065103]], dtype=float32)]\n",
      "5150 [array([[-0.26702204,  0.61734504,  0.3503288 ]], dtype=float32)]\n",
      "5200 [array([[-0.26825637,  0.61826295,  0.35001257]], dtype=float32)]\n",
      "5250 [array([[-0.26946777,  0.6191639 ,  0.34970218]], dtype=float32)]\n",
      "5300 [array([[-0.27065653,  0.6200478 ,  0.34939766]], dtype=float32)]\n",
      "5350 [array([[-0.27182317,  0.6209155 ,  0.34909874]], dtype=float32)]\n",
      "5400 [array([[-0.2729681,  0.6217669,  0.3488054]], dtype=float32)]\n",
      "5450 [array([[-0.27409172,  0.6226026 ,  0.34851745]], dtype=float32)]\n",
      "5500 [array([[-0.27519444,  0.6234226 ,  0.34823495]], dtype=float32)]\n",
      "5550 [array([[-0.27627662,  0.6242274 ,  0.3479577 ]], dtype=float32)]\n",
      "5600 [array([[-0.27733862,  0.62501717,  0.3476856 ]], dtype=float32)]\n",
      "5650 [array([[-0.2783809 ,  0.6257922 ,  0.34741858]], dtype=float32)]\n",
      "5700 [array([[-0.27940378,  0.62655276,  0.3471566 ]], dtype=float32)]\n",
      "5750 [array([[-0.2804077 ,  0.62729925,  0.34689945]], dtype=float32)]\n",
      "5800 [array([[-0.28139284,  0.62803185,  0.34664708]], dtype=float32)]\n",
      "5850 [array([[-0.28235963,  0.6287507 ,  0.34639943]], dtype=float32)]\n",
      "5900 [array([[-0.28330848,  0.6294564 ,  0.34615633]], dtype=float32)]\n",
      "5950 [array([[-0.28423965,  0.63014877,  0.3459178 ]], dtype=float32)]\n",
      "6000 [array([[-0.28515354,  0.6308283 ,  0.3456837 ]], dtype=float32)]\n",
      "6050 [array([[-0.28605038,  0.63149524,  0.34545395]], dtype=float32)]\n",
      "6100 [array([[-0.28693056,  0.6321498 ,  0.34522846]], dtype=float32)]\n",
      "6150 [array([[-0.28779435,  0.63279223,  0.34500715]], dtype=float32)]\n",
      "6200 [array([[-0.28864202,  0.63342255,  0.34478998]], dtype=float32)]\n",
      "6250 [array([[-0.28947392,  0.634041  ,  0.34457693]], dtype=float32)]\n",
      "6300 [array([[-0.29029033,  0.63464826,  0.3443677 ]], dtype=float32)]\n",
      "6350 [array([[-0.29109156,  0.635244  ,  0.34416246]], dtype=float32)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6400 [array([[-0.29187793,  0.6358286 ,  0.34396115]], dtype=float32)]\n",
      "6450 [array([[-0.2926496,  0.6364024,  0.3437635]], dtype=float32)]\n",
      "6500 [array([[-0.29340696,  0.6369655 ,  0.3435695 ]], dtype=float32)]\n",
      "6550 [array([[-0.29415017,  0.63751817,  0.34337914]], dtype=float32)]\n",
      "6600 [array([[-0.2948796 ,  0.63806057,  0.3431923 ]], dtype=float32)]\n",
      "6650 [array([[-0.29559538,  0.6385928 ,  0.34300894]], dtype=float32)]\n",
      "6700 [array([[-0.29629785,  0.6391152 ,  0.342829  ]], dtype=float32)]\n",
      "6750 [array([[-0.2969873 ,  0.63962793,  0.34265232]], dtype=float32)]\n",
      "6800 [array([[-0.29766393,  0.64013094,  0.34247908]], dtype=float32)]\n",
      "6850 [array([[-0.29832798,  0.64062464,  0.34230903]], dtype=float32)]\n",
      "6900 [array([[-0.29897964,  0.64110917,  0.34214213]], dtype=float32)]\n",
      "6950 [array([[-0.29961914,  0.64158463,  0.34197834]], dtype=float32)]\n",
      "7000 [array([[-0.30024672,  0.6420513 ,  0.34181762]], dtype=float32)]\n",
      "7050 [array([[-0.30086267,  0.6425092 ,  0.34165987]], dtype=float32)]\n",
      "7100 [array([[-0.30146712,  0.64295876,  0.34150508]], dtype=float32)]\n",
      "7150 [array([[-0.30206037,  0.64339995,  0.3413531 ]], dtype=float32)]\n",
      "7200 [array([[-0.30264252,  0.6438327 ,  0.34120402]], dtype=float32)]\n",
      "7250 [array([[-0.30321383,  0.6442575 ,  0.34105766]], dtype=float32)]\n",
      "7300 [array([[-0.3037745 ,  0.6446744 ,  0.34091407]], dtype=float32)]\n",
      "7350 [array([[-0.30432478,  0.6450835 ,  0.34077314]], dtype=float32)]\n",
      "7400 [array([[-0.30486482,  0.6454851 ,  0.34063482]], dtype=float32)]\n",
      "7450 [array([[-0.3053948 ,  0.64587903,  0.34049916]], dtype=float32)]\n",
      "7500 [array([[-0.30591488,  0.6462658 ,  0.34036592]], dtype=float32)]\n",
      "7550 [array([[-0.30642536,  0.6466451 ,  0.3402353 ]], dtype=float32)]\n",
      "7600 [array([[-0.30692628,  0.64701754,  0.3401071 ]], dtype=float32)]\n",
      "7650 [array([[-0.30741787,  0.6473831 ,  0.33998114]], dtype=float32)]\n",
      "7700 [array([[-0.30790034,  0.6477417 ,  0.33985764]], dtype=float32)]\n",
      "7750 [array([[-0.3083738,  0.6480937,  0.3397364]], dtype=float32)]\n",
      "7800 [array([[-0.3088384 ,  0.6484393 ,  0.33961734]], dtype=float32)]\n",
      "7850 [array([[-0.30929443,  0.64877826,  0.33950058]], dtype=float32)]\n",
      "7900 [array([[-0.309742  ,  0.64911103,  0.33938608]], dtype=float32)]\n",
      "7950 [array([[-0.3101812 ,  0.6494376 ,  0.33927354]], dtype=float32)]\n",
      "8000 [array([[-0.31061226,  0.64975804,  0.33916315]], dtype=float32)]\n",
      "8050 [array([[-0.31103528,  0.6500726 ,  0.33905485]], dtype=float32)]\n",
      "8100 [array([[-0.3114504,  0.6503812,  0.3389485]], dtype=float32)]\n",
      "8150 [array([[-0.3118578 ,  0.65068406,  0.3388442 ]], dtype=float32)]\n",
      "8200 [array([[-0.31225762,  0.6509813 ,  0.33874187]], dtype=float32)]\n",
      "8250 [array([[-0.31265002,  0.6512729 ,  0.33864138]], dtype=float32)]\n",
      "8300 [array([[-0.31303513,  0.6515592 ,  0.33854282]], dtype=float32)]\n",
      "8350 [array([[-0.31341302,  0.65184015,  0.3384461 ]], dtype=float32)]\n",
      "8400 [array([[-0.31378394,  0.6521158 ,  0.33835122]], dtype=float32)]\n",
      "8450 [array([[-0.31414795,  0.6523863 ,  0.33825803]], dtype=float32)]\n",
      "8500 [array([[-0.31450522,  0.6526519 ,  0.33816653]], dtype=float32)]\n",
      "8550 [array([[-0.31485578,  0.65291256,  0.3380768 ]], dtype=float32)]\n",
      "8600 [array([[-0.31519985,  0.6531682 ,  0.3379888 ]], dtype=float32)]\n",
      "8650 [array([[-0.31553754,  0.6534194 ,  0.33790222]], dtype=float32)]\n",
      "8700 [array([[-0.31586888,  0.65366554,  0.33781758]], dtype=float32)]\n",
      "8750 [array([[-0.31619406,  0.6539074 ,  0.3377342 ]], dtype=float32)]\n",
      "8800 [array([[-0.31651318,  0.6541445 ,  0.33765262]], dtype=float32)]\n",
      "8850 [array([[-0.31682637,  0.65437734,  0.3375724 ]], dtype=float32)]\n",
      "8900 [array([[-0.31713372,  0.65460575,  0.3374938 ]], dtype=float32)]\n",
      "8950 [array([[-0.3174354 ,  0.6548301 ,  0.33741656]], dtype=float32)]\n",
      "9000 [array([[-0.3177314 ,  0.65505004,  0.33734077]], dtype=float32)]\n",
      "9050 [array([[-0.31802192,  0.6552659 ,  0.33726647]], dtype=float32)]\n",
      "9100 [array([[-0.31830707,  0.6554781 ,  0.33719334]], dtype=float32)]\n",
      "9150 [array([[-0.31858686,  0.65568584,  0.3371218 ]], dtype=float32)]\n",
      "9200 [array([[-0.31886145,  0.65588987,  0.3370516 ]], dtype=float32)]\n",
      "9250 [array([[-0.31913093,  0.6560904 ,  0.3369826 ]], dtype=float32)]\n",
      "9300 [array([[-0.31939542,  0.6562868 ,  0.33691496]], dtype=float32)]\n",
      "9350 [array([[-0.319655  ,  0.6564796 ,  0.33684856]], dtype=float32)]\n",
      "9400 [array([[-0.3199097 ,  0.65666896,  0.3367834 ]], dtype=float32)]\n",
      "9450 [array([[-0.32015964,  0.6568548 ,  0.3367194 ]], dtype=float32)]\n",
      "9500 [array([[-0.32040498,  0.65703726,  0.33665657]], dtype=float32)]\n",
      "9550 [array([[-0.32064578,  0.65721625,  0.33659494]], dtype=float32)]\n",
      "9600 [array([[-0.32088205,  0.65739185,  0.3365344 ]], dtype=float32)]\n",
      "9650 [array([[-0.32111385,  0.6575642 ,  0.336475  ]], dtype=float32)]\n",
      "9700 [array([[-0.32134148,  0.65773326,  0.3364168 ]], dtype=float32)]\n",
      "9750 [array([[-0.32156485,  0.65789914,  0.33635974]], dtype=float32)]\n",
      "9800 [array([[-0.321784  ,  0.65806216,  0.33630362]], dtype=float32)]\n",
      "9850 [array([[-0.32199907,  0.65822196,  0.3362486 ]], dtype=float32)]\n",
      "9900 [array([[-0.32221016,  0.6583788 ,  0.33619463]], dtype=float32)]\n",
      "9950 [array([[-0.3224173 ,  0.65853286,  0.33614156]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "#x_data = np.array([[1,1], [2,2]])\n",
    "#print(dim(x_data))\n",
    "#y_data = np.array([[2,3]])\n",
    "\n",
    "xArr=np.array([[1,2,3],[1,3,4]])\n",
    "yArr=np.array([[2,3]])\n",
    "\n",
    "with tf.Graph().as_default():  #그래프 실행\n",
    "    x=tf.placeholder(tf.float32, shape=[None,3])  # 독립변수   [? x 2]\n",
    "    y=tf.placeholder(tf.float32, shape=None)      # 종속변수   \n",
    "    w=tf.Variable([[0,0,0]],dtype=tf.float32, name='weight')  #가중치(구해야되는 값)\n",
    "    #b=tf.Variable(0,dtype=tf.float32, name='bias')           #bias값\n",
    "    #loss=tf.Variable(tf.zeros([1.,2.]),name='loss')\n",
    "    \n",
    "    y_hat=tf.matmul(w,tf.transpose(x))  # y^=ax+b\n",
    "    loss=tf.reduce_mean(tf.square(y-y_hat)) # (y-y^)*(y-y^)이 최소인 값이 loss에 들어감\n",
    "    \n",
    "    optimizer=tf.train.GradientDescentOptimizer(learning_rate=0.005)   #GradientDescent 기법 사용\n",
    "    train=optimizer.minimize(loss)   #GradientDescent의 최소값인 값이 train에 들어감(접선의 기울기가 0에 가장 가까울 때)  #이때, train의 형태는무엇?\n",
    "    \n",
    "    init=tf.global_variables_initializer()\n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        sess.run(init)   #일상의 습관, 약속같은 문장임.\n",
    "        for step in range(10000):   #100번 반복\n",
    "            sess.run(train, feed_dict={x:xArr,y:yArr})  #x_data,y_data가 씨 \n",
    "            if (step%50==0):\n",
    "                print(step,sess.run([w]))\n",
    "            \n",
    "    #print(step,sess.run([w,b]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-13-4f64171da795>, line 33)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-13-4f64171da795>\"\u001b[1;36m, line \u001b[1;32m33\u001b[0m\n\u001b[1;33m    \u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "#x_data = np.array([[1,1], [2,2]])\n",
    "#print(dim(x_data))\n",
    "#y_data = np.array([[2,3]])\n",
    "\n",
    "xArr=np.array([[2,3],[3,4]])\n",
    "yArr=np.array([[2,3]])\n",
    "\n",
    "\n",
    "with tf.Graph().as_default():  #그래프 실행\n",
    "    x=tf.placeholder(tf.float32, shape=[None,2])  # 독립변수   [? x 2]\n",
    "    y=tf.placeholder(tf.float32, shape=None)      # 종속변수   \n",
    "    w=tf.Variable([[0,0]],dtype=tf.float32, name='weight')  #가중치(구해야되는 값)\n",
    "    b=tf.Variable(0,dtype=tf.float32, name='bias')           #bias값(구해야 되는 값)\n",
    "    #loss=tf.Variable(tf.zeros([1.,2.]),name='loss')\n",
    "    \n",
    "    y_hat=tf.matmul(w,tf.transpose(x))+b  # y^=ax+b\n",
    "    loss=tf.reduce_mean(tf.square(y-y_hat)) # (y-y^)*(y-y^)이 최소인 값이 loss에 들어감\n",
    "    \n",
    "    optimizer=tf.train.GradientDescentOptimizer(learning_rate=0.005)   #GradientDescent 기법 사용\n",
    "    train=optimizer.minimize(loss)   #GradientDescent의 최소값인 값이 train에 들어감(접선의 기울기가 0에 가장 가까울 때)  #이때, train의 형태는무엇?\n",
    "    \n",
    "    init=tf.global_variables_initializer()\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)   #일상의 습관, 약속같은 문장임.(실행합니다~의 뜻)\n",
    "        for step in range(10000):   #100번 반복\n",
    "            sess.run(train, feed_dict={x:xArr,y:yArr})  #x_data,y_data가 씨 \n",
    "            if (step%50==0):\n",
    "                #print(step,sess.run([w,b]))\n",
    "            \n",
    "    #print(step,sess.run([w,b]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [array([[0.38250476, 0.18734524]], dtype=float32)]\n",
      "50 [array([[3.2287223, 1.13213  ]], dtype=float32)]\n",
      "100 [array([[3.394249 , 0.8234837]], dtype=float32)]\n",
      "150 [array([[3.5121586, 0.5981025]], dtype=float32)]\n",
      "200 [array([[3.5977948, 0.4344055]], dtype=float32)]\n",
      "250 [array([[3.6599927 , 0.31551114]], dtype=float32)]\n",
      "300 [array([[3.7051675 , 0.22915773]], dtype=float32)]\n",
      "350 [array([[3.7379782 , 0.16643816]], dtype=float32)]\n",
      "400 [array([[3.7618086 , 0.12088532]], dtype=float32)]\n",
      "450 [array([[3.7791169 , 0.08779977]], dtype=float32)]\n",
      "500 [array([[3.7916882 , 0.06376942]], dtype=float32)]\n",
      "550 [array([[3.8008184 , 0.04631631]], dtype=float32)]\n",
      "600 [array([[3.8074503 , 0.03363976]], dtype=float32)]\n",
      "650 [array([[3.8122666, 0.0244326]], dtype=float32)]\n",
      "700 [array([[3.8157651 , 0.01774527]], dtype=float32)]\n",
      "750 [array([[3.8183057 , 0.01288856]], dtype=float32)]\n",
      "800 [array([[3.8201516 , 0.00936063]], dtype=float32)]\n",
      "850 [array([[3.8214917 , 0.00679868]], dtype=float32)]\n",
      "900 [array([[3.8224652 , 0.00493788]], dtype=float32)]\n",
      "950 [array([[3.8231721e+00, 3.5863335e-03]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "df = pd.DataFrame(np.loadtxt('ex1.txt', dtype='float'))\n",
    "xArr = df[[0,1]].values\n",
    "yArr = df[[2]].values\n",
    "\n",
    "with tf.Graph().as_default():  #그래프 실행\n",
    "    x=tf.placeholder(tf.float32, shape=[None,2])  # 독립변수   [? x 2]\n",
    "    y=tf.placeholder(tf.float32, shape=None)      # 종속변수   \n",
    "    w=tf.Variable([[0,0]],dtype=tf.float32, name='weight')  #가중치(구해야되는 값)\n",
    "    b=tf.Variable(1,dtype=tf.float32, name='bias')           #bias값(구해야 되는 값)\n",
    "    #loss=tf.Variable(tf.zeros([1.,2.]),name='loss')\n",
    "    \n",
    "    y_hat=tf.matmul(w,tf.transpose(x))  # y^=ax+b\n",
    "    loss=tf.reduce_mean(tf.square(y-y_hat)) # (y-y^)*(y-y^)이 최소인 값이 loss에 들어감\n",
    "    \n",
    "    optimizer=tf.train.GradientDescentOptimizer(learning_rate=0.05)   #GradientDescent 기법 사용\n",
    "    train=optimizer.minimize(loss)   #GradientDescent의 최소값인 값이 train에 들어감(접선의 기울기가 0에 가장 가까울 때)  #이때, train의 형태는무엇?\n",
    "    \n",
    "    init=tf.global_variables_initializer()\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)   #일상의 습관, 약속같은 문장임.(실행합니다~의 뜻)\n",
    "        for step in range(1000):   #100번 반복\n",
    "            sess.run(train, feed_dict={x:xArr,y:yArr})  #x_data,y_data가 씨 \n",
    "            if (step%50==0):\n",
    "                print(step,sess.run([w]))\n",
    "            \n",
    "    #print(step,sess.run([w,b]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [array([[0.07650094, 0.03746904]], dtype=float32)]\n",
      "5000 [array([[3.8236527e+00, 2.6670189e-03]], dtype=float32)]\n",
      "10000 [array([[3.8250241e+00, 3.7248963e-05]], dtype=float32)]\n",
      "15000 [array([[3.8250246e+00, 3.6176574e-05]], dtype=float32)]\n",
      "20000 [array([[3.8250246e+00, 3.6176574e-05]], dtype=float32)]\n",
      "25000 [array([[3.8250246e+00, 3.6176574e-05]], dtype=float32)]\n",
      "30000 [array([[3.8250246e+00, 3.6176574e-05]], dtype=float32)]\n",
      "35000 [array([[3.8250246e+00, 3.6176574e-05]], dtype=float32)]\n",
      "40000 [array([[3.8250246e+00, 3.6176574e-05]], dtype=float32)]\n",
      "45000 [array([[3.8250246e+00, 3.6176574e-05]], dtype=float32)]\n",
      "50000 [array([[3.8250246e+00, 3.6176574e-05]], dtype=float32)]\n",
      "55000 [array([[3.8250246e+00, 3.6176574e-05]], dtype=float32)]\n",
      "60000 [array([[3.8250246e+00, 3.6176574e-05]], dtype=float32)]\n",
      "65000 [array([[3.8250246e+00, 3.6176574e-05]], dtype=float32)]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-105-045f1d9ec6e9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m             \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mx_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0my_data\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m5000\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\venv\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    954\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 956\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    957\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\venv\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1178\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1180\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1181\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1182\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\venv\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1357\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1359\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1360\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1361\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\venv\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1363\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1364\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1365\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1366\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1367\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\venv\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1348\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[1;32m-> 1350\u001b[1;33m                                       target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1352\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\venv\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1441\u001b[0m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[0;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1443\u001b[1;33m                                             run_metadata)\n\u001b[0m\u001b[0;32m   1444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1445\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "# load data\n",
    "df = pd.DataFrame(np.loadtxt('ex1.txt', dtype='float'))\n",
    "\n",
    "# split data\n",
    "x_data = df[[0,1]].values\n",
    "y_data = df[[2]].values\n",
    "\n",
    "# define graph\n",
    "with tf.Graph().as_default():\n",
    "    x = tf.placeholder(tf.float32, shape=[None,2])\n",
    "    y = tf.placeholder(tf.float32, shape=None)\n",
    "    \n",
    "    w = tf.Variable([[0,0]], dtype=tf.float32, name='weight')\n",
    "    \n",
    "    y_hat = tf.matmul(w, tf.transpose(x))\n",
    "    \n",
    "    loss = tf.reduce_mean(tf.square(y-y_hat))\n",
    "    \n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "    train  = optimizer.minimize(loss)\n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        for step in range(100000):\n",
    "            sess.run(train, feed_dict={x:x_data, y:y_data})\n",
    "            if (step % 5000 == 0):\n",
    "                print(step, sess.run([w]))\n",
    "        print(step, sess.run([w]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.      , 0.635975],\n",
       "       [1.      , 0.552438],\n",
       "       [1.      , 0.855922],\n",
       "       [1.      , 0.083386],\n",
       "       [1.      , 0.975802],\n",
       "       [1.      , 0.181269],\n",
       "       [1.      , 0.129156],\n",
       "       [1.      , 0.605648],\n",
       "       [1.      , 0.301625],\n",
       "       [1.      , 0.698805],\n",
       "       [1.      , 0.226419],\n",
       "       [1.      , 0.51929 ],\n",
       "       [1.      , 0.354424],\n",
       "       [1.      , 0.11838 ],\n",
       "       [1.      , 0.512811],\n",
       "       [1.      , 0.236795],\n",
       "       [1.      , 0.353509],\n",
       "       [1.      , 0.481447],\n",
       "       [1.      , 0.060509],\n",
       "       [1.      , 0.17409 ],\n",
       "       [1.      , 0.806818],\n",
       "       [1.      , 0.531462],\n",
       "       [1.      , 0.853167],\n",
       "       [1.      , 0.304804],\n",
       "       [1.      , 0.612021],\n",
       "       [1.      , 0.62088 ],\n",
       "       [1.      , 0.580245],\n",
       "       [1.      , 0.742443],\n",
       "       [1.      , 0.11077 ],\n",
       "       [1.      , 0.742687],\n",
       "       [1.      , 0.57439 ],\n",
       "       [1.      , 0.986378],\n",
       "       [1.      , 0.294867],\n",
       "       [1.      , 0.472125],\n",
       "       [1.      , 0.872321],\n",
       "       [1.      , 0.843537],\n",
       "       [1.      , 0.864577],\n",
       "       [1.      , 0.341874],\n",
       "       [1.      , 0.09798 ],\n",
       "       [1.      , 0.757874],\n",
       "       [1.      , 0.877656],\n",
       "       [1.      , 0.457993],\n",
       "       [1.      , 0.475341],\n",
       "       [1.      , 0.848391],\n",
       "       [1.      , 0.746059],\n",
       "       [1.      , 0.153462],\n",
       "       [1.      , 0.694256],\n",
       "       [1.      , 0.498712],\n",
       "       [1.      , 0.02358 ],\n",
       "       [1.      , 0.976826],\n",
       "       [1.      , 0.624004],\n",
       "       [1.      , 0.47222 ],\n",
       "       [1.      , 0.390551],\n",
       "       [1.      , 0.021349],\n",
       "       [1.      , 0.173488],\n",
       "       [1.      , 0.971028],\n",
       "       [1.      , 0.595302],\n",
       "       [1.      , 0.097638],\n",
       "       [1.      , 0.745972],\n",
       "       [1.      , 0.67639 ],\n",
       "       [1.      , 0.488949],\n",
       "       [1.      , 0.982873],\n",
       "       [1.      , 0.29606 ],\n",
       "       [1.      , 0.228008],\n",
       "       [1.      , 0.671059],\n",
       "       [1.      , 0.379419],\n",
       "       [1.      , 0.28517 ],\n",
       "       [1.      , 0.236314],\n",
       "       [1.      , 0.629803],\n",
       "       [1.      , 0.770272],\n",
       "       [1.      , 0.493052],\n",
       "       [1.      , 0.631592],\n",
       "       [1.      , 0.965676],\n",
       "       [1.      , 0.598675],\n",
       "       [1.      , 0.351997],\n",
       "       [1.      , 0.342001],\n",
       "       [1.      , 0.661424],\n",
       "       [1.      , 0.140912],\n",
       "       [1.      , 0.373574],\n",
       "       [1.      , 0.223166],\n",
       "       [1.      , 0.908785],\n",
       "       [1.      , 0.915102],\n",
       "       [1.      , 0.41094 ],\n",
       "       [1.      , 0.754921],\n",
       "       [1.      , 0.764453],\n",
       "       [1.      , 0.101534],\n",
       "       [1.      , 0.780368],\n",
       "       [1.      , 0.819868],\n",
       "       [1.      , 0.17399 ],\n",
       "       [1.      , 0.330472],\n",
       "       [1.      , 0.162656],\n",
       "       [1.      , 0.476283],\n",
       "       [1.      , 0.636391],\n",
       "       [1.      , 0.758737],\n",
       "       [1.      , 0.778372],\n",
       "       [1.      , 0.936287],\n",
       "       [1.      , 0.510904],\n",
       "       [1.      , 0.515737],\n",
       "       [1.      , 0.437823],\n",
       "       [1.      , 0.828607],\n",
       "       [1.      , 0.5561  ],\n",
       "       [1.      , 0.038209],\n",
       "       [1.      , 0.321993],\n",
       "       [1.      , 0.067288],\n",
       "       [1.      , 0.774989],\n",
       "       [1.      , 0.566077],\n",
       "       [1.      , 0.796314],\n",
       "       [1.      , 0.7466  ],\n",
       "       [1.      , 0.360778],\n",
       "       [1.      , 0.397321],\n",
       "       [1.      , 0.062142],\n",
       "       [1.      , 0.37925 ],\n",
       "       [1.      , 0.248238],\n",
       "       [1.      , 0.682561],\n",
       "       [1.      , 0.355393],\n",
       "       [1.      , 0.889051],\n",
       "       [1.      , 0.733806],\n",
       "       [1.      , 0.153949],\n",
       "       [1.      , 0.036104],\n",
       "       [1.      , 0.388577],\n",
       "       [1.      , 0.274481],\n",
       "       [1.      , 0.319401],\n",
       "       [1.      , 0.431653],\n",
       "       [1.      , 0.960398],\n",
       "       [1.      , 0.08366 ],\n",
       "       [1.      , 0.122098],\n",
       "       [1.      , 0.415299],\n",
       "       [1.      , 0.854192],\n",
       "       [1.      , 0.925574],\n",
       "       [1.      , 0.109306],\n",
       "       [1.      , 0.805161],\n",
       "       [1.      , 0.344474],\n",
       "       [1.      , 0.769116],\n",
       "       [1.      , 0.182003],\n",
       "       [1.      , 0.225972],\n",
       "       [1.      , 0.413088],\n",
       "       [1.      , 0.964444],\n",
       "       [1.      , 0.203334],\n",
       "       [1.      , 0.285574],\n",
       "       [1.      , 0.850209],\n",
       "       [1.      , 0.061561],\n",
       "       [1.      , 0.426935],\n",
       "       [1.      , 0.389376],\n",
       "       [1.      , 0.096918],\n",
       "       [1.      , 0.148938],\n",
       "       [1.      , 0.893738],\n",
       "       [1.      , 0.195527],\n",
       "       [1.      , 0.407248],\n",
       "       [1.      , 0.224357],\n",
       "       [1.      , 0.045963],\n",
       "       [1.      , 0.944647],\n",
       "       [1.      , 0.756552],\n",
       "       [1.      , 0.432098],\n",
       "       [1.      , 0.990511],\n",
       "       [1.      , 0.649699],\n",
       "       [1.      , 0.584879],\n",
       "       [1.      , 0.785934],\n",
       "       [1.      , 0.029945],\n",
       "       [1.      , 0.075747],\n",
       "       [1.      , 0.408408],\n",
       "       [1.      , 0.583851],\n",
       "       [1.      , 0.497759],\n",
       "       [1.      , 0.421301],\n",
       "       [1.      , 0.14032 ],\n",
       "       [1.      , 0.546465],\n",
       "       [1.      , 0.843181],\n",
       "       [1.      , 0.29539 ],\n",
       "       [1.      , 0.825059],\n",
       "       [1.      , 0.946343],\n",
       "       [1.      , 0.350404],\n",
       "       [1.      , 0.042787],\n",
       "       [1.      , 0.352487],\n",
       "       [1.      , 0.590736],\n",
       "       [1.      , 0.120748],\n",
       "       [1.      , 0.14314 ],\n",
       "       [1.      , 0.511926],\n",
       "       [1.      , 0.496358],\n",
       "       [1.      , 0.382802],\n",
       "       [1.      , 0.252464],\n",
       "       [1.      , 0.845894],\n",
       "       [1.      , 0.132023],\n",
       "       [1.      , 0.442301],\n",
       "       [1.      , 0.266889],\n",
       "       [1.      , 0.008575],\n",
       "       [1.      , 0.897632],\n",
       "       [1.      , 0.533171],\n",
       "       [1.      , 0.285243],\n",
       "       [1.      , 0.377258],\n",
       "       [1.      , 0.486995],\n",
       "       [1.      , 0.305993],\n",
       "       [1.      , 0.277528],\n",
       "       [1.      , 0.750899],\n",
       "       [1.      , 0.694756],\n",
       "       [1.      , 0.870158],\n",
       "       [1.      , 0.276457],\n",
       "       [1.      , 0.017761],\n",
       "       [1.      , 0.802046],\n",
       "       [1.      , 0.559275],\n",
       "       [1.      , 0.941305],\n",
       "       [1.      , 0.856877]])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "# load data\n",
    "df = pd.DataFrame(np.loadtxt('ex1.txt', dtype='float'))\n",
    "\n",
    "# split data\n",
    "x_data = df[[0,1]].values\n",
    "y_data = df[[2]].values\n",
    "x_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-1ae59fc6b0d5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenfromtxt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'ex1.txt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'ascii'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mxArr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0myArr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "a=np.genfromtxt('ex1.txt', encoding='ascii')\n",
    "xArr = df[[0,1]].values\n",
    "yArr = df[[2]].values\n",
    "\n",
    "with tf.Graph().as_default():  #그래프 실행\n",
    "    x=tf.placeholder(tf.float32, shape=[None,2])  # 독립변수   [? x 2]\n",
    "    y=tf.placeholder(tf.float32, shape=None)      # 종속변수   \n",
    "    w=tf.Variable([[0,0]],dtype=tf.float32, name='weight')  #가중치(구해야되는 값)\n",
    "    b=tf.Variable(1,dtype=tf.float32, name='bias')           #bias값(구해야 되는 값)\n",
    "    #loss=tf.Variable(tf.zeros([1.,2.]),name='loss')\n",
    "    \n",
    "    y_hat=tf.matmul(w,tf.transpose(x))+b  # y^=ax+b\n",
    "    loss=tf.reduce_mean(tf.square(y-y_hat)) # (y-y^)*(y-y^)이 최소인 값이 loss에 들어감\n",
    "    \n",
    "    optimizer=tf.train.GradientDescentOptimizer(learning_rate=0.05)   #GradientDescent 기법 사용\n",
    "    train=optimizer.minimize(loss)   #GradientDescent의 최소값인 값이 train에 들어감(접선의 기울기가 0에 가장 가까울 때)  #이때, train의 형태는무엇?\n",
    "    \n",
    "    init=tf.global_variables_initializer()\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)   #일상의 습관, 약속같은 문장임.(실행합니다~의 뜻)\n",
    "        for step in range(1000):   #100번 반복\n",
    "            sess.run(train, feed_dict={x:xArr,y:yArr})  #x_data,y_data가 씨 \n",
    "            if (step%50==0):\n",
    "                print(step,sess.run([w]))\n",
    "            \n",
    "    #print(step,sess.run([w,b]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [array([[0.38250482, 0.20070985]], dtype=float32)]\n",
      "50 [array([[3.0327291, 1.613054 ]], dtype=float32)]\n",
      "100 [array([[3.028234 , 1.6295248]], dtype=float32)]\n",
      "150 [array([[3.0226228, 1.6402599]], dtype=float32)]\n",
      "200 [array([[3.0185444, 1.6480554]], dtype=float32)]\n",
      "250 [array([[3.0155828, 1.6537174]], dtype=float32)]\n",
      "300 [array([[3.013431 , 1.6578296]], dtype=float32)]\n",
      "350 [array([[3.0118687, 1.6608171]], dtype=float32)]\n",
      "400 [array([[3.0107336, 1.6629865]], dtype=float32)]\n",
      "450 [array([[3.0099094, 1.664562 ]], dtype=float32)]\n",
      "500 [array([[3.009311 , 1.6657062]], dtype=float32)]\n",
      "550 [array([[3.0088763, 1.6665372]], dtype=float32)]\n",
      "600 [array([[3.0085604, 1.6671406]], dtype=float32)]\n",
      "650 [array([[3.0083315, 1.6675788]], dtype=float32)]\n",
      "700 [array([[3.0081651, 1.6678969]], dtype=float32)]\n",
      "750 [array([[3.008044, 1.668128]], dtype=float32)]\n",
      "800 [array([[3.0079565, 1.668296 ]], dtype=float32)]\n",
      "850 [array([[3.0078924, 1.6684177]], dtype=float32)]\n",
      "900 [array([[3.0078466, 1.6685061]], dtype=float32)]\n",
      "950 [array([[3.007813, 1.66857 ]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "xArr=np.genfromtxt('ex1.txt', encoding='ascii', usecols=(0,1))\n",
    "yArr=np.genfromtxt('ex1.txt', encoding='ascii', usecols=(2))\n",
    "\n",
    "with tf.Graph().as_default():  #그래프 실행\n",
    "    x=tf.placeholder(tf.float32, shape=[None,2])  # 독립변수   [? x 2]\n",
    "    y=tf.placeholder(tf.float32, shape=None)      # 종속변수   \n",
    "    w=tf.Variable([[0,0]],dtype=tf.float32, name='weight')  #가중치(구해야되는 값)\n",
    "    #b=tf.Variable(1,dtype=tf.float32, name='bias')           #bias값(구해야 되는 값)\n",
    "    #loss=tf.Variable(tf.zeros([1.,2.]),name='loss')\n",
    "    \n",
    "    y_hat=tf.matmul(w,tf.transpose(x))  # y^=ax+b\n",
    "    loss=tf.reduce_mean(tf.square(y-y_hat)) # (y-y^)*(y-y^)이 최소인 값이 loss에 들어감\n",
    "    \n",
    "    optimizer=tf.train.GradientDescentOptimizer(learning_rate=0.05)   #GradientDescent 기법 사용\n",
    "    train=optimizer.minimize(loss)   #GradientDescent의 최소값인 값이 train에 들어감(접선의 기울기가 0에 가장 가까울 때)  #이때, train의 형태는무엇?\n",
    "    \n",
    "    init=tf.global_variables_initializer()\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)   #일상의 습관, 약속같은 문장임.(실행합니다~의 뜻)\n",
    "        for step in range(1000):   #100번 반복\n",
    "            sess.run(train, feed_dict={x:xArr,y:yArr})  #x_data,y_data가 씨 \n",
    "            if (step%50==0):\n",
    "                print(step,sess.run([w]))\n",
    "            \n",
    "    #print(step,sess.run([w,b]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 2)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "xArr=np.genfromtxt('ex1.txt', encoding='ascii', usecols=(0,1))\n",
    "yArr=np.genfromtxt('ex1.txt', encoding='ascii', usecols=(2))\n",
    "xArr\n",
    "yArr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [array([[0.38250482, 0.20070985]], dtype=float32)]\n",
      "50 [array([[3.0327291, 1.613054 ]], dtype=float32)]\n",
      "100 [array([[3.028234 , 1.6295248]], dtype=float32)]\n",
      "150 [array([[3.0226228, 1.6402599]], dtype=float32)]\n",
      "200 [array([[3.0185444, 1.6480554]], dtype=float32)]\n",
      "250 [array([[3.0155828, 1.6537174]], dtype=float32)]\n",
      "300 [array([[3.013431 , 1.6578296]], dtype=float32)]\n",
      "350 [array([[3.0118687, 1.6608171]], dtype=float32)]\n",
      "400 [array([[3.0107336, 1.6629865]], dtype=float32)]\n",
      "450 [array([[3.0099094, 1.664562 ]], dtype=float32)]\n",
      "500 [array([[3.009311 , 1.6657062]], dtype=float32)]\n",
      "550 [array([[3.0088763, 1.6665372]], dtype=float32)]\n",
      "600 [array([[3.0085604, 1.6671406]], dtype=float32)]\n",
      "650 [array([[3.0083315, 1.6675788]], dtype=float32)]\n",
      "700 [array([[3.0081651, 1.6678969]], dtype=float32)]\n",
      "750 [array([[3.008044, 1.668128]], dtype=float32)]\n",
      "800 [array([[3.0079565, 1.668296 ]], dtype=float32)]\n",
      "850 [array([[3.0078924, 1.6684177]], dtype=float32)]\n",
      "900 [array([[3.0078466, 1.6685061]], dtype=float32)]\n",
      "950 [array([[3.007813, 1.66857 ]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "xArr=np.genfromtxt('ex1.txt', encoding='ascii', usecols=(0,1))\n",
    "yArr=np.genfromtxt('ex1.txt', encoding='ascii', usecols=(2))\n",
    "\n",
    "with tf.Graph().as_default():  #그래프 실행\n",
    "    x=tf.placeholder(tf.float32, shape=[None,2])  # 독립변수   [? x 2]\n",
    "    y=tf.placeholder(tf.float32, shape=None)      # 종속변수   \n",
    "    w=tf.Variable([[0,0]],dtype=tf.float32, name='weight')  #가중치(구해야되는 값)\n",
    "    #b=tf.Variable(1,dtype=tf.float32, name='bias')           #bias값(구해야 되는 값)\n",
    "    #loss=tf.Variable(tf.zeros([1.,2.]),name='loss')\n",
    "    \n",
    "    y_hat=tf.matmul(w,tf.transpose(x))  # y^=ax+b\n",
    "    loss=tf.reduce_mean(tf.square(y-y_hat)) # (y-y^)*(y-y^)이 최소인 값이 loss에 들어감\n",
    "    \n",
    "    optimizer=tf.train.GradientDescentOptimizer(learning_rate=0.05)   #GradientDescent 기법 사용\n",
    "    train=optimizer.minimize(loss)   #GradientDescent의 최소값인 값이 train에 들어감(접선의 기울기가 0에 가장 가까울 때)  #이때, train의 형태는무엇?\n",
    "    \n",
    "    init=tf.global_variables_initializer()\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)   #일상의 습관, 약속같은 문장임.(실행합니다~의 뜻)\n",
    "        for step in range(1000):   #100번 반복\n",
    "            sess.run(train, feed_dict={x:xArr,y:yArr})  #x_data,y_data가 씨 \n",
    "            if (step%50==0):\n",
    "                print(step,sess.run([w]))\n",
    "            \n",
    "    #print(step,sess.run([w,b]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [0.3825048, array([[0.20070985]], dtype=float32)]\n",
      "50 [3.0327291, array([[1.613054]], dtype=float32)]\n",
      "100 [3.028234, array([[1.6295248]], dtype=float32)]\n",
      "150 [3.0226228, array([[1.6402599]], dtype=float32)]\n",
      "200 [3.0185444, array([[1.6480554]], dtype=float32)]\n",
      "250 [3.0155828, array([[1.6537174]], dtype=float32)]\n",
      "300 [3.013431, array([[1.6578296]], dtype=float32)]\n",
      "350 [3.0118687, array([[1.6608171]], dtype=float32)]\n",
      "400 [3.0107336, array([[1.6629865]], dtype=float32)]\n",
      "450 [3.0099094, array([[1.664562]], dtype=float32)]\n",
      "500 [3.009311, array([[1.6657062]], dtype=float32)]\n",
      "550 [3.0088763, array([[1.6665372]], dtype=float32)]\n",
      "600 [3.0085604, array([[1.6671406]], dtype=float32)]\n",
      "650 [3.0083315, array([[1.6675788]], dtype=float32)]\n",
      "700 [3.0081651, array([[1.6678969]], dtype=float32)]\n",
      "750 [3.008044, array([[1.668128]], dtype=float32)]\n",
      "800 [3.0079565, array([[1.668296]], dtype=float32)]\n",
      "850 [3.0078924, array([[1.6684177]], dtype=float32)]\n",
      "900 [3.0078466, array([[1.6685061]], dtype=float32)]\n",
      "950 [3.007813, array([[1.66857]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "xArr=np.genfromtxt('ex1.txt', encoding='ascii', usecols=(1))\n",
    "xArr=xArr[:,np.newaxis]  #차원추가\n",
    "yArr=np.genfromtxt('ex1.txt', encoding='ascii', usecols=(2))\n",
    "\n",
    "with tf.Graph().as_default():  #그래프 실행\n",
    "    x=tf.placeholder(tf.float32, shape=[None,1])  # 독립변수   [? x 2]\n",
    "    y=tf.placeholder(tf.float32, shape=None)      # 종속변수   \n",
    "    w=tf.Variable([[0]],dtype=tf.float32, name='weight')  #가중치(구해야되는 값)\n",
    "    b=tf.Variable(0,dtype=tf.float32, name='bias')           #bias값(구해야 되는 값)\n",
    "    #loss=tf.Variable(tf.zeros([1.,2.]),name='loss')\n",
    "    \n",
    "    y_hat=tf.matmul(w,tf.transpose(x))+b  # y^=ax+b\n",
    "    loss=tf.reduce_mean(tf.square(y-y_hat)) # (y-y^)*(y-y^)이 최소인 값이 loss에 들어감\n",
    "    \n",
    "    optimizer=tf.train.GradientDescentOptimizer(learning_rate=0.05)   #GradientDescent 기법 사용\n",
    "    train=optimizer.minimize(loss)   #GradientDescent의 최소값인 값이 train에 들어감(접선의 기울기가 0에 가장 가까울 때)  #이때, train의 형태는무엇?\n",
    "    \n",
    "    init=tf.global_variables_initializer()\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)   #일상의 습관, 약속같은 문장임.(실행합니다~의 뜻)\n",
    "        for step in range(1000):   #100번 반복\n",
    "            sess.run(train, feed_dict={x:xArr,y:yArr})  #x_data,y_data가 씨 \n",
    "            if (step%50==0):\n",
    "                print(step,sess.run([w]))\n",
    "            \n",
    "    #print(step,sess.run([w,b]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data-01-test-score.csv파일의 선형회귀분석 하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [array([[1.315829 , 1.3188332, 1.3553208]], dtype=float32), 0.016245835]\n",
      "5000 [array([[0.3492308 , 0.54053587, 1.1245258 ]], dtype=float32), -0.029788628]\n",
      "10000 [array([[0.34875056, 0.54090136, 1.1251323 ]], dtype=float32), -0.07112182]\n",
      "15000 [array([[0.34877452, 0.5409823 , 1.1255217 ]], dtype=float32), -0.112050466]\n",
      "20000 [array([[0.34879842, 0.5410608 , 1.1259075 ]], dtype=float32), -0.15257601]\n",
      "25000 [array([[0.34882203, 0.5411388 , 1.1262895 ]], dtype=float32), -0.19270214]\n",
      "30000 [array([[0.34884536, 0.54121643, 1.1266688 ]], dtype=float32), -0.23243277]\n",
      "35000 [array([[0.34886804, 0.54129267, 1.1270425 ]], dtype=float32), -0.27177283]\n",
      "40000 [array([[0.3488915, 0.5413699, 1.1274128]], dtype=float32), -0.3107251]\n",
      "45000 [array([[0.3489179 , 0.54144824, 1.1277755 ]], dtype=float32), -0.34929097]\n",
      "50000 [array([[0.3489411 , 0.54152185, 1.1281358 ]], dtype=float32), -0.38747954]\n",
      "55000 [array([[0.34896034, 0.54159284, 1.1285025 ]], dtype=float32), -0.42528918]\n",
      "60000 [array([[0.34898165, 0.54166555, 1.1288605 ]], dtype=float32), -0.46272963]\n",
      "65000 [array([[0.3490038 , 0.54173785, 1.1292143 ]], dtype=float32), -0.49980032]\n",
      "70000 [array([[0.34902567, 0.54180807, 1.1295649 ]], dtype=float32), -0.5365181]\n",
      "75000 [array([[0.34904736, 0.5418788 , 1.12991   ]], dtype=float32), -0.5728544]\n",
      "80000 [array([[0.34906918, 0.54194915, 1.1302527 ]], dtype=float32), -0.6088269]\n",
      "85000 [array([[0.34909028, 0.5420188 , 1.1305916 ]], dtype=float32), -0.6444546]\n",
      "90000 [array([[0.3491109, 0.5420869, 1.1309278]], dtype=float32), -0.6797392]\n",
      "95000 [array([[0.3491318 , 0.54215497, 1.1312605 ]], dtype=float32), -0.71468073]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "dataset=pd.read_csv('data-01-test-score.csv')\n",
    "yArr=dataset.iloc[:,3]\n",
    "#yArr=yArr[:,np.newaxis]\n",
    "xArr=dataset.iloc[:,[0,1,2]]\n",
    "\n",
    "with tf.Graph().as_default():  #그래프 실행\n",
    "    tf.set_random_seed(777)\n",
    "    x=tf.placeholder(tf.float32, shape=[None,3])  # 독립변수   [? x 2]\n",
    "    y=tf.placeholder(tf.float32, shape=None)      # 종속변수   \n",
    "    w=tf.Variable([[0,0,0]],dtype=tf.float32, name='weight')  #가중치(구해야되는 값)\n",
    "    b=tf.Variable(0,dtype=tf.float32, name='bias')           #bias값(구해야 되는 값)\n",
    "    #loss=tf.Variable(tf.zeros([1.,2.]),name='loss')\n",
    "    \n",
    "    y_hat=tf.matmul(w,tf.transpose(x))+b  # y^=ax+b\n",
    "    loss=tf.reduce_mean(tf.square(y-y_hat)) # (y-y^)*(y-y^)이 최소인 값이 loss에 들어감\n",
    "    \n",
    "    optimizer=tf.train.GradientDescentOptimizer(learning_rate=0.00005)   #GradientDescent 기법 사용\n",
    "    train=optimizer.minimize(loss)   #GradientDescent의 최소값인 값이 train에 들어감(접선의 기울기가 0에 가장 가까울 때)  #이때, train의 형태는무엇?\n",
    "    \n",
    "    init=tf.global_variables_initializer()\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)   #일상의 습관, 약속같은 문장임.(실행합니다~의 뜻)\n",
    "        for step in range(100000):   #100번 반복\n",
    "            sess.run(train, feed_dict={x:xArr,y:yArr})  #x_data,y_data가 씨 \n",
    "            if (step%5000==0):\n",
    "                print(step,sess.run([w,b]))\n",
    "            \n",
    "    #print(step,sess.run([w,b]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "강사님것 -> random_seed()이용해서 초기값을 어느정도 범위내에 고정시킴 + cost값(loss값) 도출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "dataset=pd.read_csv('data-01-test-score.csv')\n",
    "yArr=dataset.iloc[:,3]\n",
    "yArr=yArr[:,np.newaxis]\n",
    "xArr=dataset.iloc[:,[0,1,2]]\n",
    "\n",
    "with tf.Graph().as_default():  #그래프 실행\n",
    "    tf.set_random_seed(777)\n",
    "    x=tf.placeholder(tf.float32, shape=[None,3])      # 독립변수   [? x 2]\n",
    "    y=tf.placeholder(tf.float32, shape=[None,1])      # 종속변수   \n",
    "    w=tf.Variable(tf.random_normal([3,1]), name='weight')       #가중치(구해야되는 값)   -> 초기값 : 랜덤으로 [3 x 1]형태로 가져옴 -> 실행할때마다 cost값이 달라짐.\n",
    "    b=tf.Variable(tf.random_normal([1]), name='bias')           #bias값(구해야 되는 값) -> 초기값 : 랜덤으로 [1 x 1]형태로 가져옴 -> 실행할때마다 cost값이 달라짐.\n",
    "    #loss=tf.Variable(tf.zeros([1.,2.]),name='loss')\n",
    "    \n",
    "    y_hat=tf.matmul(x,w)+b  # y^=ax+b\n",
    "    loss=tf.reduce_mean(tf.square(y-y_hat)) # (y-y^)*(y-y^)이 최소인 값이 loss에 들어감\n",
    "    \n",
    "    optimizer=tf.train.GradientDescentOptimizer(learning_rate=0.00005)   #GradientDescent 기법 사용\n",
    "    train=optimizer.minimize(loss)   #GradientDescent의 최소값인 값이 train에 들어감(접선의 기울기가 0에 가장 가까울 때) \n",
    "    \n",
    "    init=tf.global_variables_initializer()   # Variable형태로 저장한 변수(W,b)를 초기화\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)   #일상의 습관, 약속같은 문장임.(실행합니다~의 뜻)\n",
    "        for step in range(100000):   #여러번 반복\n",
    "            cost, hypothesis, _ = sess.run([loss,y_hat,train], feed_dict={x:xArr,y:yArr})  #xArr,yArr로 부터 loss,y_hat,traindata를 채워넣고 cost,hypothesis라는 이름으로 저장\n",
    "            #sess.run([loss,y_hat,train], feed_dict={x:xArr,y:yArr})\n",
    "            if (step%5000==0):\n",
    "                print(step,cost,train_t,sess.run([w,b]))   # \n",
    "            \n",
    "    #print(step,sess.run([w,b]))\n",
    "    print('Your score will be ',sess.run(y_hat, feed_dict={x:[[60,70,110],[90,100,80]]})) #이런식으로 test_dataset을 넣어서 y값을 도출할수도 있음.\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 21199.48 [array([[ 1.9658394 ],\n",
      "       [ 1.8437815 ],\n",
      "       [-0.01536262]], dtype=float32), array([0.47287357], dtype=float32)]\n",
      "50000 6.319288 [array([[0.3486862 ],\n",
      "       [0.54069275],\n",
      "       [1.1241057 ]], dtype=float32), array([0.03662538], dtype=float32)]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your score will be  [[182.54562]\n",
      " [175.42012]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "dataset=pd.read_csv('data-01-test-score.csv')\n",
    "yArr=dataset.iloc[:,3]\n",
    "yArr=yArr[:,np.newaxis]\n",
    "xArr=dataset.iloc[:,[0,1,2]]\n",
    "\n",
    "with tf.Graph().as_default():  #그래프 실행\n",
    "    tf.set_random_seed(777)\n",
    "    x=tf.placeholder(tf.float32, shape=[None,3])      # 독립변수   [? x 2]\n",
    "    y=tf.placeholder(tf.float32, shape=[None,1])      # 종속변수   \n",
    "    w=tf.Variable(tf.random_normal([3,1]), name='weight')       #가중치 (구해야되는 값)   -> 초기값 : 랜덤으로 [3 x 1]형태로 가져옴 -> 실행할때마다 cost값이 달라짐.\n",
    "    b=tf.Variable(tf.random_normal([1]), name='bias')           #bias값(구해야 되는 값) -> 초기값 : 랜덤으로 [1 x 1]형태로 가져옴 -> 실행할때마다 cost값이 달라짐.\n",
    "    #loss=tf.Variable(tf.zeros([1.,2.]),name='loss')\n",
    "    \n",
    "    y_hat=tf.matmul(x,w)+b  # y^=ax+b\n",
    "    loss=tf.reduce_mean(tf.square(y-y_hat)) # (y-y^)*(y-y^)이 최소인 값이 loss에 들어감\n",
    "    \n",
    "    optimizer=tf.train.GradientDescentOptimizer(learning_rate=0.00005)   #GradientDescent 기법 사용\n",
    "    train=optimizer.minimize(loss)   #GradientDescent의 최소값인 값이 train에 들어감(접선의 기울기가 0에 가장 가까울 때) \n",
    "    \n",
    "    init=tf.global_variables_initializer()   # Variable형태로 저장한 변수(W,b)를 초기화\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)   #일상의 습관, 약속같은 문장임.(실행합니다~의 뜻)\n",
    "        cost_history=[]\n",
    "        for step in range(100000):   #여러번 반복\n",
    "            cost, hypothesis, _ = sess.run([loss,y_hat,train], feed_dict={x:xArr,y:yArr})  #xArr,yArr로 부터 loss,y_hat,traindata를 채워넣고 cost,hypothesis라는 이름으로 저장\n",
    "            cost_history.append(cost)\n",
    "            #sess.run([loss,y_hat,train], feed_dict={x:xArr,y:yArr})\n",
    "            if (step%50000==0):\n",
    "                print(step,cost,sess.run([w,b]))   # \n",
    "        plt.plot(cost_history)  #이뿌당\n",
    "        plt.show()\n",
    "#print(step,sess.run([w,b]))\n",
    "        print('Your score will be ',sess.run(y_hat, feed_dict={x:[[60,70,110],[90,100,80]]})) #이런식으로 test_dataset을 넣어서 y값을 도출할수도 있음.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [array([[-1.2865348 ],\n",
      "       [-0.35789633],\n",
      "       [ 0.36992085],\n",
      "       [ 0.19344088],\n",
      "       [ 0.0999492 ],\n",
      "       [-0.7864803 ],\n",
      "       [-0.01597854],\n",
      "       [ 0.40899926]], dtype=float32), array([-0.29494035], dtype=float32)]\n",
      "500 [array([[-1.2865348 ],\n",
      "       [-0.35789633],\n",
      "       [ 0.36992085],\n",
      "       [ 0.19344088],\n",
      "       [ 0.0999492 ],\n",
      "       [-0.7864803 ],\n",
      "       [-0.01597854],\n",
      "       [ 0.40899926]], dtype=float32), array([-0.29494035], dtype=float32)]\n",
      "1000 [array([[-1.2865348 ],\n",
      "       [-0.35789633],\n",
      "       [ 0.36992085],\n",
      "       [ 0.19344088],\n",
      "       [ 0.0999492 ],\n",
      "       [-0.7864803 ],\n",
      "       [-0.01597854],\n",
      "       [ 0.40899926]], dtype=float32), array([-0.29494035], dtype=float32)]\n",
      "1500 [array([[-1.2865348 ],\n",
      "       [-0.35789633],\n",
      "       [ 0.36992085],\n",
      "       [ 0.19344088],\n",
      "       [ 0.0999492 ],\n",
      "       [-0.7864803 ],\n",
      "       [-0.01597854],\n",
      "       [ 0.40899926]], dtype=float32), array([-0.29494035], dtype=float32)]\n",
      "2000 [array([[-1.2865348 ],\n",
      "       [-0.35789633],\n",
      "       [ 0.36992085],\n",
      "       [ 0.19344088],\n",
      "       [ 0.0999492 ],\n",
      "       [-0.7864803 ],\n",
      "       [-0.01597854],\n",
      "       [ 0.40899926]], dtype=float32), array([-0.29494035], dtype=float32)]\n",
      "2500 [array([[-1.2865348 ],\n",
      "       [-0.35789633],\n",
      "       [ 0.36992085],\n",
      "       [ 0.19344088],\n",
      "       [ 0.0999492 ],\n",
      "       [-0.7864803 ],\n",
      "       [-0.01597854],\n",
      "       [ 0.40899926]], dtype=float32), array([-0.29494035], dtype=float32)]\n",
      "3000 [array([[-1.2865348 ],\n",
      "       [-0.35789633],\n",
      "       [ 0.36992085],\n",
      "       [ 0.19344088],\n",
      "       [ 0.0999492 ],\n",
      "       [-0.7864803 ],\n",
      "       [-0.01597854],\n",
      "       [ 0.40899926]], dtype=float32), array([-0.29494035], dtype=float32)]\n",
      "3500 [array([[-1.2865348 ],\n",
      "       [-0.35789633],\n",
      "       [ 0.36992085],\n",
      "       [ 0.19344088],\n",
      "       [ 0.0999492 ],\n",
      "       [-0.7864803 ],\n",
      "       [-0.01597854],\n",
      "       [ 0.40899926]], dtype=float32), array([-0.29494035], dtype=float32)]\n",
      "4000 [array([[-1.2865348 ],\n",
      "       [-0.35789633],\n",
      "       [ 0.36992085],\n",
      "       [ 0.19344088],\n",
      "       [ 0.0999492 ],\n",
      "       [-0.7864803 ],\n",
      "       [-0.01597854],\n",
      "       [ 0.40899926]], dtype=float32), array([-0.29494035], dtype=float32)]\n",
      "4500 [array([[-1.2865348 ],\n",
      "       [-0.35789633],\n",
      "       [ 0.36992085],\n",
      "       [ 0.19344088],\n",
      "       [ 0.0999492 ],\n",
      "       [-0.7864803 ],\n",
      "       [-0.01597854],\n",
      "       [ 0.40899926]], dtype=float32), array([-0.29494035], dtype=float32)]\n",
      "5000 [array([[-1.2865348 ],\n",
      "       [-0.35789633],\n",
      "       [ 0.36992085],\n",
      "       [ 0.19344088],\n",
      "       [ 0.0999492 ],\n",
      "       [-0.7864803 ],\n",
      "       [-0.01597854],\n",
      "       [ 0.40899926]], dtype=float32), array([-0.29494035], dtype=float32)]\n",
      "5500 [array([[-1.2865348 ],\n",
      "       [-0.35789633],\n",
      "       [ 0.36992085],\n",
      "       [ 0.19344088],\n",
      "       [ 0.0999492 ],\n",
      "       [-0.7864803 ],\n",
      "       [-0.01597854],\n",
      "       [ 0.40899926]], dtype=float32), array([-0.29494035], dtype=float32)]\n",
      "6000 [array([[-1.2865348 ],\n",
      "       [-0.35789633],\n",
      "       [ 0.36992085],\n",
      "       [ 0.19344088],\n",
      "       [ 0.0999492 ],\n",
      "       [-0.7864803 ],\n",
      "       [-0.01597854],\n",
      "       [ 0.40899926]], dtype=float32), array([-0.29494035], dtype=float32)]\n",
      "6500 [array([[-1.2865348 ],\n",
      "       [-0.35789633],\n",
      "       [ 0.36992085],\n",
      "       [ 0.19344088],\n",
      "       [ 0.0999492 ],\n",
      "       [-0.7864803 ],\n",
      "       [-0.01597854],\n",
      "       [ 0.40899926]], dtype=float32), array([-0.29494035], dtype=float32)]\n",
      "7000 [array([[-1.2865348 ],\n",
      "       [-0.35789633],\n",
      "       [ 0.36992085],\n",
      "       [ 0.19344088],\n",
      "       [ 0.0999492 ],\n",
      "       [-0.7864803 ],\n",
      "       [-0.01597854],\n",
      "       [ 0.40899926]], dtype=float32), array([-0.29494035], dtype=float32)]\n",
      "7500 [array([[-1.2865348 ],\n",
      "       [-0.35789633],\n",
      "       [ 0.36992085],\n",
      "       [ 0.19344088],\n",
      "       [ 0.0999492 ],\n",
      "       [-0.7864803 ],\n",
      "       [-0.01597854],\n",
      "       [ 0.40899926]], dtype=float32), array([-0.29494035], dtype=float32)]\n",
      "8000 [array([[-1.2865348 ],\n",
      "       [-0.35789633],\n",
      "       [ 0.36992085],\n",
      "       [ 0.19344088],\n",
      "       [ 0.0999492 ],\n",
      "       [-0.7864803 ],\n",
      "       [-0.01597854],\n",
      "       [ 0.40899926]], dtype=float32), array([-0.29494035], dtype=float32)]\n",
      "8500 [array([[-1.2865348 ],\n",
      "       [-0.35789633],\n",
      "       [ 0.36992085],\n",
      "       [ 0.19344088],\n",
      "       [ 0.0999492 ],\n",
      "       [-0.7864803 ],\n",
      "       [-0.01597854],\n",
      "       [ 0.40899926]], dtype=float32), array([-0.29494035], dtype=float32)]\n",
      "9000 [array([[-1.2865348 ],\n",
      "       [-0.35789633],\n",
      "       [ 0.36992085],\n",
      "       [ 0.19344088],\n",
      "       [ 0.0999492 ],\n",
      "       [-0.7864803 ],\n",
      "       [-0.01597854],\n",
      "       [ 0.40899926]], dtype=float32), array([-0.29494035], dtype=float32)]\n",
      "9500 [array([[-1.2865348 ],\n",
      "       [-0.35789633],\n",
      "       [ 0.36992085],\n",
      "       [ 0.19344088],\n",
      "       [ 0.0999492 ],\n",
      "       [-0.7864803 ],\n",
      "       [-0.01597854],\n",
      "       [ 0.40899926]], dtype=float32), array([-0.29494035], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "#concrete.csv\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "conc=pd.read_csv('concrete.csv')\n",
    "\n",
    "xArr=conc.iloc[:,:8]\n",
    "yArr=conc.iloc[:,8]\n",
    "yArr=yArr[:,np.newaxis]\n",
    "\n",
    "with tf.Graph().as_default():  #그래프 실행\n",
    "    \n",
    "    x=tf.placeholder(tf.float32, shape=[None,8])      # 독립변수   [? x 2]\n",
    "    y=tf.placeholder(tf.float32, shape=[None,1])      # 종속변수   \n",
    "    w=tf.Variable(tf.random_normal([8,1]), name='weight')       #가중치(구해야되는 값)   -> 초기값 : 랜덤으로 [3 x 1]형태로 가져옴 -> 실행할때마다 cost값이 달라짐.\n",
    "    b=tf.Variable(tf.random_normal([1]), name='bias')           #bias값(구해야 되는 값) -> 초기값 : 랜덤으로 [1 x 1]형태로 가져옴 -> 실행할때마다 cost값이 달라짐.\n",
    "    #loss=tf.Variable(tf.zeros([1.,2.]),name='loss')\n",
    "    \n",
    "    y_hat=tf.matmul(x,w)+b  # y^=ax+b\n",
    "    loss=tf.reduce_mean(tf.square(y-y_hat)) # (y-y^)*(y-y^)이 최소인 값이 loss에 들어감\n",
    "    \n",
    "    optimizer=tf.train.GradientDescentOptimizer(learning_rate=0.0005)   #GradientDescent 기법 사용\n",
    "    train=optimizer.minimize(loss)   #GradientDescent의 최소값인 값이 train에 들어감(접선의 기울기가 0에 가장 가까울 때) \n",
    "    \n",
    "    init=tf.global_variables_initializer()   # Variable형태로 저장한 변수(W,b)를 초기화\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)   #일상의 습관, 약속같은 문장임.(실행합니다~의 뜻)\n",
    "        cost_history=[]\n",
    "        for step in range(10000):   #여러번 반복\n",
    "            #cost, hypothesis, _ = sess.run([loss,y_hat,train], feed_dict={x:xArr,y:yArr})  #xArr,yArr로 부터 loss,y_hat,traindata를 채워넣고 cost,hypothesis라는 이름으로 저장\n",
    "            #cost_history.append(cost)\n",
    "            #sess.run([cost,hypothesis], feed_dict={x:xArr,y:yArr})\n",
    "            if (step%500==0):\n",
    "                print(step,sess.run([w,b]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "conc=pd.read_csv('concrete.csv')\n",
    "\n",
    "xArr=conc.iloc[:,:8]\n",
    "yArr=conc.iloc[:,8]\n",
    "yArr=yArr[:,np.newaxis]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [array([[-0.60569537],\n",
      "       [-0.06847277],\n",
      "       [-0.8021823 ],\n",
      "       [ 1.1429056 ],\n",
      "       [-0.68776125],\n",
      "       [-0.9491823 ]], dtype=float32), array([0.7861743], dtype=float32)]\n",
      "500 [array([[-0.60569537],\n",
      "       [-0.06847277],\n",
      "       [-0.8021823 ],\n",
      "       [ 1.1429056 ],\n",
      "       [-0.68776125],\n",
      "       [-0.9491823 ]], dtype=float32), array([0.7861743], dtype=float32)]\n",
      "1000 [array([[-0.60569537],\n",
      "       [-0.06847277],\n",
      "       [-0.8021823 ],\n",
      "       [ 1.1429056 ],\n",
      "       [-0.68776125],\n",
      "       [-0.9491823 ]], dtype=float32), array([0.7861743], dtype=float32)]\n",
      "1500 [array([[-0.60569537],\n",
      "       [-0.06847277],\n",
      "       [-0.8021823 ],\n",
      "       [ 1.1429056 ],\n",
      "       [-0.68776125],\n",
      "       [-0.9491823 ]], dtype=float32), array([0.7861743], dtype=float32)]\n",
      "2000 [array([[-0.60569537],\n",
      "       [-0.06847277],\n",
      "       [-0.8021823 ],\n",
      "       [ 1.1429056 ],\n",
      "       [-0.68776125],\n",
      "       [-0.9491823 ]], dtype=float32), array([0.7861743], dtype=float32)]\n",
      "2500 [array([[-0.60569537],\n",
      "       [-0.06847277],\n",
      "       [-0.8021823 ],\n",
      "       [ 1.1429056 ],\n",
      "       [-0.68776125],\n",
      "       [-0.9491823 ]], dtype=float32), array([0.7861743], dtype=float32)]\n",
      "3000 [array([[-0.60569537],\n",
      "       [-0.06847277],\n",
      "       [-0.8021823 ],\n",
      "       [ 1.1429056 ],\n",
      "       [-0.68776125],\n",
      "       [-0.9491823 ]], dtype=float32), array([0.7861743], dtype=float32)]\n",
      "3500 [array([[-0.60569537],\n",
      "       [-0.06847277],\n",
      "       [-0.8021823 ],\n",
      "       [ 1.1429056 ],\n",
      "       [-0.68776125],\n",
      "       [-0.9491823 ]], dtype=float32), array([0.7861743], dtype=float32)]\n",
      "4000 [array([[-0.60569537],\n",
      "       [-0.06847277],\n",
      "       [-0.8021823 ],\n",
      "       [ 1.1429056 ],\n",
      "       [-0.68776125],\n",
      "       [-0.9491823 ]], dtype=float32), array([0.7861743], dtype=float32)]\n",
      "4500 [array([[-0.60569537],\n",
      "       [-0.06847277],\n",
      "       [-0.8021823 ],\n",
      "       [ 1.1429056 ],\n",
      "       [-0.68776125],\n",
      "       [-0.9491823 ]], dtype=float32), array([0.7861743], dtype=float32)]\n",
      "5000 [array([[-0.60569537],\n",
      "       [-0.06847277],\n",
      "       [-0.8021823 ],\n",
      "       [ 1.1429056 ],\n",
      "       [-0.68776125],\n",
      "       [-0.9491823 ]], dtype=float32), array([0.7861743], dtype=float32)]\n",
      "5500 [array([[-0.60569537],\n",
      "       [-0.06847277],\n",
      "       [-0.8021823 ],\n",
      "       [ 1.1429056 ],\n",
      "       [-0.68776125],\n",
      "       [-0.9491823 ]], dtype=float32), array([0.7861743], dtype=float32)]\n",
      "6000 [array([[-0.60569537],\n",
      "       [-0.06847277],\n",
      "       [-0.8021823 ],\n",
      "       [ 1.1429056 ],\n",
      "       [-0.68776125],\n",
      "       [-0.9491823 ]], dtype=float32), array([0.7861743], dtype=float32)]\n",
      "6500 [array([[-0.60569537],\n",
      "       [-0.06847277],\n",
      "       [-0.8021823 ],\n",
      "       [ 1.1429056 ],\n",
      "       [-0.68776125],\n",
      "       [-0.9491823 ]], dtype=float32), array([0.7861743], dtype=float32)]\n",
      "7000 [array([[-0.60569537],\n",
      "       [-0.06847277],\n",
      "       [-0.8021823 ],\n",
      "       [ 1.1429056 ],\n",
      "       [-0.68776125],\n",
      "       [-0.9491823 ]], dtype=float32), array([0.7861743], dtype=float32)]\n",
      "7500 [array([[-0.60569537],\n",
      "       [-0.06847277],\n",
      "       [-0.8021823 ],\n",
      "       [ 1.1429056 ],\n",
      "       [-0.68776125],\n",
      "       [-0.9491823 ]], dtype=float32), array([0.7861743], dtype=float32)]\n",
      "8000 [array([[-0.60569537],\n",
      "       [-0.06847277],\n",
      "       [-0.8021823 ],\n",
      "       [ 1.1429056 ],\n",
      "       [-0.68776125],\n",
      "       [-0.9491823 ]], dtype=float32), array([0.7861743], dtype=float32)]\n",
      "8500 [array([[-0.60569537],\n",
      "       [-0.06847277],\n",
      "       [-0.8021823 ],\n",
      "       [ 1.1429056 ],\n",
      "       [-0.68776125],\n",
      "       [-0.9491823 ]], dtype=float32), array([0.7861743], dtype=float32)]\n",
      "9000 [array([[-0.60569537],\n",
      "       [-0.06847277],\n",
      "       [-0.8021823 ],\n",
      "       [ 1.1429056 ],\n",
      "       [-0.68776125],\n",
      "       [-0.9491823 ]], dtype=float32), array([0.7861743], dtype=float32)]\n",
      "9500 [array([[-0.60569537],\n",
      "       [-0.06847277],\n",
      "       [-0.8021823 ],\n",
      "       [ 1.1429056 ],\n",
      "       [-0.68776125],\n",
      "       [-0.9491823 ]], dtype=float32), array([0.7861743], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "#Insurance.csv\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "Ins=pd.read_csv('insurance.csv')\n",
    "\n",
    "#Ins.shape\n",
    "xArr=Ins.iloc[:,:6]\n",
    "yArr=Ins.iloc[:,6]\n",
    "yArr=yArr[:,np.newaxis]\n",
    "\n",
    "with tf.Graph().as_default():  #그래프 실행\n",
    "    #tf.set_random_seed(777)   #seed를 어떻게???\n",
    "    x=tf.placeholder(tf.float32, shape=[None,6])      # 독립변수   [? x 2]\n",
    "    y=tf.placeholder(tf.float32, shape=[None,1])      # 종속변수   \n",
    "    w=tf.Variable(tf.random_normal([6,1]),mean=0 name='weight')       #가중치(구해야되는 값)   -> 초기값 : 랜덤으로 [3 x 1]형태로 가져옴 -> 실행할때마다 cost값이 달라짐.\n",
    "    b=tf.Variable(tf.random_normal([1]), name='bias')           #bias값(구해야 되는 값) -> 초기값 : 랜덤으로 [1 x 1]형태로 가져옴 -> 실행할때마다 cost값이 달라짐.\n",
    "    #loss=tf.Variable(tf.zeros([1.,2.]),name='loss')\n",
    "    \n",
    "    y_hat=tf.matmul(x,w)+b  # y^=ax+b\n",
    "    loss=tf.reduce_mean(tf.square(y-y_hat)) # (y-y^)*(y-y^)이 최소인 값이 loss에 들어감\n",
    "    \n",
    "    optimizer=tf.train.GradientDescentOptimizer(learning_rate=0.0005)   #GradientDescent 기법 사용\n",
    "    train=optimizer.minimize(loss)   #GradientDescent의 최소값인 값이 train에 들어감(접선의 기울기가 0에 가장 가까울 때) \n",
    "    \n",
    "    init=tf.global_variables_initializer()   # Variable형태로 저장한 변수(W,b)를 초기화\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)   #일상의 습관, 약속같은 문장임.(실행합니다~의 뜻)\n",
    "        cost_history=[]\n",
    "        for step in range(10000):   #여러번 반복\n",
    "            #cost, hypothesis, _ = sess.run([loss,y_hat,train], feed_dict={x:xArr,y:yArr})  #xArr,yArr로 부터 loss,y_hat,traindata를 채워넣고 cost,hypothesis라는 이름으로 저장\n",
    "            #cost_history.append(cost)\n",
    "            #sess.run([cost,hypothesis], feed_dict={x:xArr,y:yArr})\n",
    "            if (step%500==0):\n",
    "                print(step,sess.run([w,b]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<로지스틱 회귀>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.79079574 None\n",
      "200 0.4990554 None\n",
      "400 0.39152715 None\n",
      "600 0.34447083 None\n",
      "800 0.31934026 None\n",
      "1000 0.30325052 None\n",
      "1200 0.29139137 None\n",
      "1400 0.28174585 None\n",
      "1600 0.27338496 None\n",
      "1800 0.2658473 None\n",
      "2000 0.25888893 None\n",
      "2200 0.25237307 None\n",
      "2400 0.24621807 None\n",
      "2600 0.24037246 None\n",
      "2800 0.23480123 None\n",
      "3000 0.2294789 None\n",
      "3200 0.2243859 None\n",
      "3400 0.21950643 None\n",
      "3600 0.21482666 None\n",
      "3800 0.2103348 None\n",
      "4000 0.20602016 None\n",
      "4200 0.2018727 None\n",
      "4400 0.19788368 None\n",
      "4600 0.19404458 None\n",
      "4800 0.19034766 None\n",
      "5000 0.18678552 None\n",
      "5200 0.18335135 None\n",
      "5400 0.18003891 None\n",
      "5600 0.17684199 None\n",
      "5800 0.1737551 None\n",
      "6000 0.17077278 None\n",
      "6200 0.16789007 None\n",
      "6400 0.16510244 None\n",
      "6600 0.16240524 None\n",
      "6800 0.15979442 None\n",
      "7000 0.15726607 None\n",
      "7200 0.15481637 None\n",
      "7400 0.15244193 None\n",
      "7600 0.15013935 None\n",
      "7800 0.14790557 None\n",
      "8000 0.1457376 None\n",
      "8200 0.14363264 None\n",
      "8400 0.14158812 None\n",
      "8600 0.13960142 None\n",
      "8800 0.13767028 None\n",
      "9000 0.13579232 None\n",
      "9200 0.13396552 None\n",
      "9400 0.13218786 None\n",
      "9600 0.13045731 None\n",
      "9800 0.12877218 None\n",
      "10000 0.12713058 None\n",
      "\n",
      "hypothesis :  [[0.021927  ]\n",
      " [0.14484914]\n",
      " [0.25940505]\n",
      " [0.8029977 ]\n",
      " [0.9523703 ]\n",
      " [0.9845403 ]] \n",
      "predicted :  [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] \n",
      "accuracy :  1.0\n"
     ]
    }
   ],
   "source": [
    "x_data=[[1,2],\n",
    "        [2,3],\n",
    "        [3,1],\n",
    "        [4,3],\n",
    "        [5,3],\n",
    "        [6,2]]\n",
    "y_data=[[0],\n",
    "        [0],\n",
    "        [0],\n",
    "        [1],\n",
    "        [1],\n",
    "        [1]]\n",
    "\n",
    "\n",
    "#print('y_data.shape={}'.format(y_data.shape))\n",
    "X=tf.placeholder(tf.float32, shape=[None,2])\n",
    "Y=tf.placeholder(tf.float32, shape=[None,1])\n",
    "W=tf.Variable(tf.random_normal([2,1]), name='weight')\n",
    "b=tf.Variable(tf.random_normal([1]), name='bias')\n",
    "\n",
    "hypothesis=tf.sigmoid(tf.matmul(X,W)+b) # == hypohesis=1.0/(1+tf.exp(-(tf.matmul(X,W)+b))))   #로지스틱 함수\n",
    "\n",
    "cost= -tf.reduce_mean(Y*tf.log(hypothesis)+(1-Y)*tf.log(1-hypothesis))   #엔트로피\n",
    "\n",
    "train=tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)  #그래디언트옵티마이저로 cost값을 가장 최소화하는 \n",
    "\n",
    "predicted=tf.cast(hypothesis > 0.5, dtype=tf.float32)  # 시그모이드함수에서 나온값이 0.5보다 크면 1(float32형), 작으면, 0(float32형)\n",
    "\n",
    "accuracy=tf.reduce_mean(tf.cast(tf.equal(predicted,Y), dtype=tf.float32))   \n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(10001):\n",
    "        cos, val =sess.run([cost, train], feed_dict={X:x_data,Y:y_data})\n",
    "        \n",
    "        if step %200 ==0:\n",
    "            print(step, cos,val)\n",
    "            \n",
    "    h,o,a=sess.run([hypothesis,predicted,accuracy],feed_dict={X:x_data,Y:y_data,})\n",
    "    \n",
    "    print('\\nhypothesis : ',h,\n",
    "         '\\npredicted : ',o,\n",
    "         '\\naccuracy : ',a)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<diabetes.csv>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.9364585107560245\n",
      "2000 0.5671873993438072\n",
      "4000 0.5214567413880581\n",
      "6000 0.499684248254026\n",
      "8000 0.4884471812121777\n",
      "10000 0.48221406233799563\n",
      "12000 0.4785462745645465\n",
      "14000 0.47628416857779665\n",
      "16000 0.47483622038513645\n",
      "18000 0.47388167551219257\n",
      "20000 0.4732373511650803\n",
      "22000 0.47279399977327113\n",
      "24000 0.47248407935138176\n",
      "26000 0.47226456364858926\n",
      "28000 0.4721073497073516\n",
      "30000 0.47199369129862606\n",
      "32000 0.47191085792111614\n",
      "34000 0.47185007067638607\n",
      "36000 0.47180519484125444\n",
      "38000 0.47177189368161454\n",
      "40000 0.4717470703928512\n",
      "42000 0.471728493925722\n",
      "44000 0.4717145444045994\n",
      "46000 0.4717040376366788\n",
      "48000 0.47169610271588885\n",
      "50000 0.4716900957540064\n",
      "52000 0.47168553849627387\n",
      "54000 0.4716820742693722\n",
      "56000 0.47167943612471125\n",
      "58000 0.47167742364227827\n",
      "60000 0.4716758859367963\n",
      "62000 0.47167470913974463\n",
      "64000 0.4716738071337105\n",
      "66000 0.47167311466471856\n",
      "68000 0.4716725822029339\n",
      "70000 0.47167217209520634\n",
      "72000 0.47167185567634656\n",
      "74000 0.47167161109468536\n",
      "76000 0.4716714216716239\n",
      "78000 0.4716712746615959\n",
      "80000 0.4716711603130722\n",
      "82000 0.47167107115642376\n",
      "84000 0.4716710014630927\n",
      "86000 0.47167094683434885\n",
      "88000 0.4716709038882323\n",
      "90000 0.4716708700209864\n",
      "92000 0.4716708432250852\n",
      "94000 0.471670821950294\n",
      "96000 0.47167080499749847\n",
      "98000 0.4716707914374953\n",
      "100000 0.47167078054882294\n",
      "\n",
      "hypothesis :  [[0.35373602]\n",
      " [0.95648617]\n",
      " [0.19488767]\n",
      " [0.96235732]\n",
      " [0.06873642]\n",
      " [0.83418661]\n",
      " [0.95319619]\n",
      " [0.57315518]\n",
      " [0.17775401]\n",
      " [0.59379891]\n",
      " [0.75426447]\n",
      " [0.10993983]\n",
      " [0.27317003]\n",
      " [0.18987277]\n",
      " [0.78131645]\n",
      " [0.38063188]\n",
      " [0.79507879]\n",
      " [0.75782705]\n",
      " [0.81941085]\n",
      " [0.57674267]\n",
      " [0.7224395 ]\n",
      " [0.05972294]\n",
      " [0.722232  ]\n",
      " [0.67689484]\n",
      " [0.28612236]\n",
      " [0.96237085]\n",
      " [0.61989626]\n",
      " [0.70860362]\n",
      " [0.70210423]\n",
      " [0.40021531]\n",
      " [0.96957033]\n",
      " [0.95248791]\n",
      " [0.64595275]\n",
      " [0.87164662]\n",
      " [0.35026121]\n",
      " [0.68782874]\n",
      " [0.83747121]\n",
      " [0.51381498]\n",
      " [0.32608629]\n",
      " [0.31951483]\n",
      " [0.91352973]\n",
      " [0.0974818 ]\n",
      " [0.37928761]\n",
      " [0.02291157]\n",
      " [0.54518344]\n",
      " [0.96328801]\n",
      " [0.68369323]\n",
      " [0.72936412]\n",
      " [0.97156835]\n",
      " [0.94925085]\n",
      " [0.95982737]\n",
      " [0.18686996]\n",
      " [0.27645377]\n",
      " [0.97979651]\n",
      " [0.11279655]\n",
      " [0.43472532]\n",
      " [0.08546692]\n",
      " [0.65998192]\n",
      " [0.89718288]\n",
      " [0.49572363]\n",
      " [0.97837491]\n",
      " [0.71861627]\n",
      " [0.67514088]\n",
      " [0.89774348]\n",
      " [0.65997337]\n",
      " [0.57553152]\n",
      " [0.97873951]\n",
      " [0.72948989]\n",
      " [0.8687794 ]\n",
      " [0.66715871]\n",
      " [0.21816954]\n",
      " [0.73457378]\n",
      " [0.94941008]\n",
      " [0.94708023]\n",
      " [0.93149285]\n",
      " [0.80488501]\n",
      " [0.2967212 ]\n",
      " [0.90701618]\n",
      " [0.91763062]\n",
      " [0.94371584]\n",
      " [0.91413989]\n",
      " [0.8823969 ]\n",
      " [0.31128819]\n",
      " [0.84015976]\n",
      " [0.53334362]\n",
      " [0.86840262]\n",
      " [0.33333208]\n",
      " [0.93146731]\n",
      " [0.97272666]\n",
      " [0.79299998]\n",
      " [0.78792231]\n",
      " [0.73338112]\n",
      " [0.76885348]\n",
      " [0.53079198]\n",
      " [0.92569798]\n",
      " [0.9881597 ]\n",
      " [0.90911504]\n",
      " [0.51422965]\n",
      " [0.16419327]\n",
      " [0.66338202]\n",
      " [0.76960056]\n",
      " [0.97836631]\n",
      " [0.77655749]\n",
      " [0.75627397]\n",
      " [0.96803667]\n",
      " [0.65255198]\n",
      " [0.93286634]\n",
      " [0.84628301]\n",
      " [0.41357718]\n",
      " [0.25685562]\n",
      " [0.95709655]\n",
      " [0.90464263]\n",
      " [0.32933866]\n",
      " [0.4790416 ]\n",
      " [0.64550066]\n",
      " [0.85342811]\n",
      " [0.89872853]\n",
      " [0.95726502]\n",
      " [0.05395707]\n",
      " [0.73056037]\n",
      " [0.87515684]\n",
      " [0.69817147]\n",
      " [0.6706147 ]\n",
      " [0.6100085 ]\n",
      " [0.62000201]\n",
      " [0.83456215]\n",
      " [0.84340032]\n",
      " [0.7199153 ]\n",
      " [0.43465009]\n",
      " [0.36010825]\n",
      " [0.32077103]\n",
      " [0.79561948]\n",
      " [0.96456364]\n",
      " [0.81319463]\n",
      " [0.81860466]\n",
      " [0.88075826]\n",
      " [0.4878502 ]\n",
      " [0.80542792]\n",
      " [0.83601143]\n",
      " [0.73310364]\n",
      " [0.87800672]\n",
      " [0.65698778]\n",
      " [0.49542117]\n",
      " [0.72758315]\n",
      " [0.95045087]\n",
      " [0.76828221]\n",
      " [0.4291309 ]\n",
      " [0.95945089]\n",
      " [0.60068808]\n",
      " [0.85765593]\n",
      " [0.21538337]\n",
      " [0.31017563]\n",
      " [0.04448029]\n",
      " [0.13744285]\n",
      " [0.932733  ]\n",
      " [0.90153566]\n",
      " [0.96470411]\n",
      " [0.05508115]\n",
      " [0.56709673]\n",
      " [0.78191367]\n",
      " [0.54498957]\n",
      " [0.89469158]\n",
      " [0.46618748]\n",
      " [0.8400913 ]\n",
      " [0.57185371]\n",
      " [0.68146711]\n",
      " [0.75794291]\n",
      " [0.91250079]\n",
      " [0.83899229]\n",
      " [0.56848416]\n",
      " [0.91803171]\n",
      " [0.86329735]\n",
      " [0.96839296]\n",
      " [0.15488944]\n",
      " [0.88114767]\n",
      " [0.11088897]\n",
      " [0.27803942]\n",
      " [0.35835849]\n",
      " [0.94720068]\n",
      " [0.62190899]\n",
      " [0.94975265]\n",
      " [0.94847454]\n",
      " [0.64122672]\n",
      " [0.07074264]\n",
      " [0.13492951]\n",
      " [0.67372994]\n",
      " [0.79868548]\n",
      " [0.64799833]\n",
      " [0.89243932]\n",
      " [0.61132848]\n",
      " [0.31492719]\n",
      " [0.08896669]\n",
      " [0.93486065]\n",
      " [0.30531888]\n",
      " [0.91190911]\n",
      " [0.93055755]\n",
      " [0.71275575]\n",
      " [0.59747597]\n",
      " [0.65648693]\n",
      " [0.52975257]\n",
      " [0.75831683]\n",
      " [0.97304012]\n",
      " [0.76042949]\n",
      " [0.87805097]\n",
      " [0.06571359]\n",
      " [0.30366515]\n",
      " [0.91859948]\n",
      " [0.14480887]\n",
      " [0.95848805]\n",
      " [0.20725069]\n",
      " [0.21400453]\n",
      " [0.32237548]\n",
      " [0.73530384]\n",
      " [0.1200552 ]\n",
      " [0.73984151]\n",
      " [0.73319389]\n",
      " [0.86589036]\n",
      " [0.64881857]\n",
      " [0.07894714]\n",
      " [0.35473866]\n",
      " [0.76627029]\n",
      " [0.48091014]\n",
      " [0.95282728]\n",
      " [0.95585256]\n",
      " [0.73198849]\n",
      " [0.234735  ]\n",
      " [0.0163182 ]\n",
      " [0.57465064]\n",
      " [0.27955071]\n",
      " [0.33617249]\n",
      " [0.97659074]\n",
      " [0.63172923]\n",
      " [0.96950614]\n",
      " [0.12474776]\n",
      " [0.07281105]\n",
      " [0.24412367]\n",
      " [0.87703925]\n",
      " [0.93986019]\n",
      " [0.90712116]\n",
      " [0.66877993]\n",
      " [0.65124254]\n",
      " [0.52306299]\n",
      " [0.10794932]\n",
      " [0.56811341]\n",
      " [0.05920858]\n",
      " [0.54488507]\n",
      " [0.91224074]\n",
      " [0.67300691]\n",
      " [0.78482676]\n",
      " [0.97537593]\n",
      " [0.83501947]\n",
      " [0.79950462]\n",
      " [0.76717263]\n",
      " [0.78696304]\n",
      " [0.89093901]\n",
      " [0.28330128]\n",
      " [0.29992277]\n",
      " [0.50172695]\n",
      " [0.85540859]\n",
      " [0.64616415]\n",
      " [0.69308373]\n",
      " [0.83071601]\n",
      " [0.24708328]\n",
      " [0.40125378]\n",
      " [0.65461985]\n",
      " [0.6389229 ]\n",
      " [0.3566614 ]\n",
      " [0.93256065]\n",
      " [0.85403749]\n",
      " [0.95798138]\n",
      " [0.56610085]\n",
      " [0.76585711]\n",
      " [0.85349228]\n",
      " [0.85434266]\n",
      " [0.76347389]\n",
      " [0.89437211]\n",
      " [0.27562201]\n",
      " [0.54350072]\n",
      " [0.70148676]\n",
      " [0.3655343 ]\n",
      " [0.88244086]\n",
      " [0.23372204]\n",
      " [0.54644812]\n",
      " [0.96226659]\n",
      " [0.79276385]\n",
      " [0.88767684]\n",
      " [0.65365018]\n",
      " [0.39429775]\n",
      " [0.52925571]\n",
      " [0.38626442]\n",
      " [0.35334767]\n",
      " [0.66537577]\n",
      " [0.66716704]\n",
      " [0.64353693]\n",
      " [0.7169096 ]\n",
      " [0.14452353]\n",
      " [0.63711632]\n",
      " [0.93667717]\n",
      " [0.41120081]\n",
      " [0.72214857]\n",
      " [0.74293031]\n",
      " [0.44185001]\n",
      " [0.74319141]\n",
      " [0.47580785]\n",
      " [0.69959078]\n",
      " [0.94002553]\n",
      " [0.62600276]\n",
      " [0.70609813]\n",
      " [0.85622319]\n",
      " [0.55548619]\n",
      " [0.86150387]\n",
      " [0.97412422]\n",
      " [0.25648433]\n",
      " [0.76731174]\n",
      " [0.23835183]\n",
      " [0.77814643]\n",
      " [0.8468555 ]\n",
      " [0.73267084]\n",
      " [0.37876335]\n",
      " [0.80297441]\n",
      " [0.75897432]\n",
      " [0.72517001]\n",
      " [0.1177876 ]\n",
      " [0.80511245]\n",
      " [0.87822732]\n",
      " [0.6401987 ]\n",
      " [0.95209392]\n",
      " [0.12846603]\n",
      " [0.80811913]\n",
      " [0.96897198]\n",
      " [0.1064312 ]\n",
      " [0.441653  ]\n",
      " [0.73819888]\n",
      " [0.2706587 ]\n",
      " [0.11281882]\n",
      " [0.87316973]\n",
      " [0.94981262]\n",
      " [0.89015072]\n",
      " [0.66210861]\n",
      " [0.68576805]\n",
      " [0.55262572]\n",
      " [0.72931516]\n",
      " [0.87377191]\n",
      " [0.96134301]\n",
      " [0.73900655]\n",
      " [0.78045579]\n",
      " [0.6340098 ]\n",
      " [0.96372069]\n",
      " [0.95954364]\n",
      " [0.73605758]\n",
      " [0.26579808]\n",
      " [0.64221491]\n",
      " [0.24375549]\n",
      " [0.7839201 ]\n",
      " [0.12071889]\n",
      " [0.16600434]\n",
      " [0.39812524]\n",
      " [0.75949798]\n",
      " [0.31122969]\n",
      " [0.51477   ]\n",
      " [0.83895011]\n",
      " [0.71232768]\n",
      " [0.90679738]\n",
      " [0.97405527]\n",
      " [0.80870859]\n",
      " [0.05560693]\n",
      " [0.4077338 ]\n",
      " [0.84589744]\n",
      " [0.86211574]\n",
      " [0.61526677]\n",
      " [0.21927365]\n",
      " [0.92977742]\n",
      " [0.90358814]\n",
      " [0.17014647]\n",
      " [0.6317401 ]\n",
      " [0.87241837]\n",
      " [0.91119686]\n",
      " [0.8916481 ]\n",
      " [0.93556121]\n",
      " [0.91069005]\n",
      " [0.94009502]\n",
      " [0.70559073]\n",
      " [0.62586246]\n",
      " [0.54527506]\n",
      " [0.85972744]\n",
      " [0.89885923]\n",
      " [0.13028605]\n",
      " [0.84465972]\n",
      " [0.91747943]\n",
      " [0.29680639]\n",
      " [0.6143692 ]\n",
      " [0.9033677 ]\n",
      " [0.51785989]\n",
      " [0.96256265]\n",
      " [0.17431146]\n",
      " [0.8712003 ]\n",
      " [0.62207615]\n",
      " [0.9222444 ]\n",
      " [0.28523315]\n",
      " [0.57184691]\n",
      " [0.77905664]\n",
      " [0.86927868]\n",
      " [0.07438687]\n",
      " [0.1270373 ]\n",
      " [0.73923305]\n",
      " [0.83394664]\n",
      " [0.36620129]\n",
      " [0.81389781]\n",
      " [0.40400277]\n",
      " [0.29876061]\n",
      " [0.89241257]\n",
      " [0.39770023]\n",
      " [0.96875944]\n",
      " [0.84318046]\n",
      " [0.62629195]\n",
      " [0.94653791]\n",
      " [0.611488  ]\n",
      " [0.81388461]\n",
      " [0.22115821]\n",
      " [0.19114294]\n",
      " [0.78004388]\n",
      " [0.30955526]\n",
      " [0.43298408]\n",
      " [0.92266324]\n",
      " [0.94403094]\n",
      " [0.93798375]\n",
      " [0.96908912]\n",
      " [0.75874836]\n",
      " [0.93231679]\n",
      " [0.24665696]\n",
      " [0.3160243 ]\n",
      " [0.49262348]\n",
      " [0.97336032]\n",
      " [0.64402043]\n",
      " [0.13647762]\n",
      " [0.94791792]\n",
      " [0.81502912]\n",
      " [0.6243925 ]\n",
      " [0.81585299]\n",
      " [0.00450531]\n",
      " [0.94945775]\n",
      " [0.80350028]\n",
      " [0.76138817]\n",
      " [0.78262594]\n",
      " [0.98276804]\n",
      " [0.67436599]\n",
      " [0.76375885]\n",
      " [0.81705329]\n",
      " [0.83658596]\n",
      " [0.11375825]\n",
      " [0.63674458]\n",
      " [0.93628137]\n",
      " [0.63777698]\n",
      " [0.81599837]\n",
      " [0.97606526]\n",
      " [0.8756289 ]\n",
      " [0.93188041]\n",
      " [0.67513258]\n",
      " [0.81209446]\n",
      " [0.95755689]\n",
      " [0.7397528 ]\n",
      " [0.6627152 ]\n",
      " [0.18956509]\n",
      " [0.39867675]\n",
      " [0.50456937]\n",
      " [0.55467175]\n",
      " [0.59180827]\n",
      " [0.82288687]\n",
      " [0.62508798]\n",
      " [0.82400314]\n",
      " [0.88174457]\n",
      " [0.78999563]\n",
      " [0.71343827]\n",
      " [0.40634108]\n",
      " [0.61378828]\n",
      " [0.95502324]\n",
      " [0.86897393]\n",
      " [0.14509761]\n",
      " [0.32632865]\n",
      " [0.39214877]\n",
      " [0.04398163]\n",
      " [0.92947066]\n",
      " [0.12366697]\n",
      " [0.91193422]\n",
      " [0.92187449]\n",
      " [0.86492134]\n",
      " [0.69399762]\n",
      " [0.91954409]\n",
      " [0.35159894]\n",
      " [0.84220263]\n",
      " [0.96064734]\n",
      " [0.24949327]\n",
      " [0.41104924]\n",
      " [0.92336453]\n",
      " [0.89652851]\n",
      " [0.6387645 ]\n",
      " [0.82355282]\n",
      " [0.84150752]\n",
      " [0.87203902]\n",
      " [0.19698911]\n",
      " [0.76237844]\n",
      " [0.91216987]\n",
      " [0.71396418]\n",
      " [0.84796984]\n",
      " [0.73545325]\n",
      " [0.88497599]\n",
      " [0.91386709]\n",
      " [0.95009855]\n",
      " [0.54081703]\n",
      " [0.42068529]\n",
      " [0.81558153]\n",
      " [0.84821887]\n",
      " [0.98521892]\n",
      " [0.78973922]\n",
      " [0.70322757]\n",
      " [0.38129157]\n",
      " [0.72283317]\n",
      " [0.95666568]\n",
      " [0.9752932 ]\n",
      " [0.92421624]\n",
      " [0.71137094]\n",
      " [0.73906337]\n",
      " [0.81122657]\n",
      " [0.40357485]\n",
      " [0.81347339]\n",
      " [0.8468213 ]\n",
      " [0.91159229]\n",
      " [0.59766371]\n",
      " [0.79926789]\n",
      " [0.95106201]\n",
      " [0.46567534]\n",
      " [0.55256998]\n",
      " [0.64393359]\n",
      " [0.73084417]\n",
      " [0.71104179]\n",
      " [0.92576967]\n",
      " [0.94969893]\n",
      " [0.1524561 ]\n",
      " [0.05915929]\n",
      " [0.74687532]\n",
      " [0.4908655 ]\n",
      " [0.24932473]\n",
      " [0.87978648]\n",
      " [0.93008333]\n",
      " [0.79011279]\n",
      " [0.95414144]\n",
      " [0.92530386]\n",
      " [0.8088637 ]\n",
      " [0.85395104]\n",
      " [0.76918149]\n",
      " [0.46150292]\n",
      " [0.83593356]\n",
      " [0.62378864]\n",
      " [0.04821315]\n",
      " [0.91536818]\n",
      " [0.90680994]\n",
      " [0.78899379]\n",
      " [0.93773163]\n",
      " [0.86627772]\n",
      " [0.90263589]\n",
      " [0.54865451]\n",
      " [0.66909474]\n",
      " [0.92500515]\n",
      " [0.84586024]\n",
      " [0.87220088]\n",
      " [0.91405121]\n",
      " [0.62794491]\n",
      " [0.76552894]\n",
      " [0.84575565]\n",
      " [0.50225181]\n",
      " [0.56187206]\n",
      " [0.06261539]\n",
      " [0.19495351]\n",
      " [0.87383802]\n",
      " [0.68964652]\n",
      " [0.68806898]\n",
      " [0.59331273]\n",
      " [0.96150147]\n",
      " [0.38313027]\n",
      " [0.87897067]\n",
      " [0.21426719]\n",
      " [0.9505193 ]\n",
      " [0.25903818]\n",
      " [0.78285028]\n",
      " [0.58900346]\n",
      " [0.88595072]\n",
      " [0.58448336]\n",
      " [0.16650615]\n",
      " [0.79810622]\n",
      " [0.93694711]\n",
      " [0.29797005]\n",
      " [0.93282043]\n",
      " [0.92317412]\n",
      " [0.908676  ]\n",
      " [0.85773714]\n",
      " [0.3505806 ]\n",
      " [0.25770647]\n",
      " [0.6550444 ]\n",
      " [0.10864173]\n",
      " [0.97501222]\n",
      " [0.25313629]\n",
      " [0.94859131]\n",
      " [0.88767284]\n",
      " [0.29743938]\n",
      " [0.14811993]\n",
      " [0.74270291]\n",
      " [0.3618756 ]\n",
      " [0.89694367]\n",
      " [0.80786486]\n",
      " [0.99114192]\n",
      " [0.61084532]\n",
      " [0.63621897]\n",
      " [0.80443732]\n",
      " [0.87774979]\n",
      " [0.0430754 ]\n",
      " [0.69905509]\n",
      " [0.84165296]\n",
      " [0.88142652]\n",
      " [0.70096554]\n",
      " [0.47497503]\n",
      " [0.61997552]\n",
      " [0.93999047]\n",
      " [0.68667779]\n",
      " [0.80848016]\n",
      " [0.86567815]\n",
      " [0.89676838]\n",
      " [0.86464862]\n",
      " [0.61640041]\n",
      " [0.85253904]\n",
      " [0.92674118]\n",
      " [0.68108466]\n",
      " [0.97953143]\n",
      " [0.85144011]\n",
      " [0.62099991]\n",
      " [0.51510673]\n",
      " [0.8819895 ]\n",
      " [0.89251246]\n",
      " [0.38488825]\n",
      " [0.67479347]\n",
      " [0.11716217]\n",
      " [0.60343877]\n",
      " [0.84914427]\n",
      " [0.96587429]\n",
      " [0.82659311]\n",
      " [0.73558564]\n",
      " [0.79100311]\n",
      " [0.90245482]\n",
      " [0.3567027 ]\n",
      " [0.9593347 ]\n",
      " [0.55465064]\n",
      " [0.88879922]\n",
      " [0.32133485]\n",
      " [0.03765283]\n",
      " [0.25405036]\n",
      " [0.28468148]\n",
      " [0.69148385]\n",
      " [0.85791619]\n",
      " [0.57600119]\n",
      " [0.78869803]\n",
      " [0.81348316]\n",
      " [0.46768177]\n",
      " [0.2819066 ]\n",
      " [0.92813755]\n",
      " [0.93827613]\n",
      " [0.26681368]\n",
      " [0.72491614]\n",
      " [0.12902343]\n",
      " [0.44733053]\n",
      " [0.75891963]\n",
      " [0.67163753]\n",
      " [0.93130192]\n",
      " [0.98937222]\n",
      " [0.08952388]\n",
      " [0.65983604]\n",
      " [0.65336314]\n",
      " [0.44566238]\n",
      " [0.72515889]\n",
      " [0.79951142]\n",
      " [0.90768321]\n",
      " [0.77630394]\n",
      " [0.37858037]\n",
      " [0.7649547 ]\n",
      " [0.13214752]\n",
      " [0.63259148]\n",
      " [0.46301418]\n",
      " [0.95065323]\n",
      " [0.60430408]\n",
      " [0.51912797]\n",
      " [0.85706878]\n",
      " [0.73381325]\n",
      " [0.38094434]\n",
      " [0.75058351]\n",
      " [0.70183555]\n",
      " [0.27679104]\n",
      " [0.546518  ]\n",
      " [0.91600398]\n",
      " [0.86697682]\n",
      " [0.58042211]\n",
      " [0.73564052]\n",
      " [0.2484736 ]\n",
      " [0.84768098]\n",
      " [0.53234716]\n",
      " [0.78512286]\n",
      " [0.3018795 ]\n",
      " [0.64579631]\n",
      " [0.88862795]\n",
      " [0.08560688]\n",
      " [0.23169598]\n",
      " [0.87453426]\n",
      " [0.81886559]\n",
      " [0.81692157]\n",
      " [0.94802091]\n",
      " [0.76959124]\n",
      " [0.68829522]\n",
      " [0.72421233]\n",
      " [0.84313525]\n",
      " [0.69360014]\n",
      " [0.80441754]\n",
      " [0.48757914]\n",
      " [0.51628456]\n",
      " [0.91247354]\n",
      " [0.8285545 ]\n",
      " [0.73253632]\n",
      " [0.17755673]\n",
      " [0.89760312]\n",
      " [0.89322311]\n",
      " [0.84443775]\n",
      " [0.72528738]\n",
      " [0.92304984]\n",
      " [0.85848084]\n",
      " [0.78486969]\n",
      " [0.33800148]\n",
      " [0.89141105]\n",
      " [0.92648354]\n",
      " [0.35415109]\n",
      " [0.10386369]\n",
      " [0.7869312 ]\n",
      " [0.29637042]\n",
      " [0.78040847]\n",
      " [0.20365185]\n",
      " [0.44909805]\n",
      " [0.43078631]\n",
      " [0.76415842]\n",
      " [0.90461007]\n",
      " [0.08045704]\n",
      " [0.33640829]\n",
      " [0.61385113]\n",
      " [0.53631805]\n",
      " [0.49281451]\n",
      " [0.81396104]\n",
      " [0.11610624]\n",
      " [0.93909614]\n",
      " [0.09292894]\n",
      " [0.91340528]\n",
      " [0.76203148]\n",
      " [0.71202189]\n",
      " [0.86663059]\n",
      " [0.73259496]\n",
      " [0.92895364]] \n",
      "predicted :  [[0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] \n",
      "accuracy :  0.769433465085639\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "diab=pd.read_csv('data-03-diabetes.csv',header=None)\n",
    "\n",
    "xArr=diab.iloc[:,:8].values   \n",
    "yArr=diab.iloc[:,8].values\n",
    "yArr=yArr[:,np.newaxis]      \n",
    "#print(type(yArr[0][0]))     #값이 float64가 나와서 dtype을 모두 float64로 해주었습니다.\n",
    "\n",
    "X=tf.placeholder(tf.float64, shape=[None,8])\n",
    "Y=tf.placeholder(tf.float64, shape=[None,1])\n",
    "W=tf.Variable(tf.random_normal([8,1],dtype=tf.float64),name='weight')\n",
    "b=tf.Variable(tf.random_normal([1],dtype=tf.float64), name='bias')\n",
    "\n",
    "hypothesis = tf.sigmoid(tf.matmul(X,W)+b)\n",
    "\n",
    "cost=-(tf.reduce_mean(Y*tf.log(hypothesis)+(1-Y)*tf.log(1-hypothesis)))\n",
    "\n",
    "train=tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
    "\n",
    "predicted=tf.cast(hypothesis > 0.5, dtype=tf.float64)\n",
    "\n",
    "accuracy=tf.reduce_mean(tf.cast(tf.equal(predicted,Y),dtype=tf.float64))\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(100001):\n",
    "        cos, _=sess.run([cost, train], feed_dict={X:xArr, Y:yArr})\n",
    "        if step%2000==0:\n",
    "            print(step,cos)\n",
    "    h,o,a=sess.run([hypothesis,predicted,accuracy],feed_dict={X:xArr,Y:yArr,})\n",
    "    \n",
    "    print('\\nhypothesis : ',h,\n",
    "          '\\npredicted : ',o,\n",
    "          '\\naccuracy : ',a)        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.float64"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diab=pd.read_csv('data-03-diabetes.csv',header=None)\n",
    "\n",
    "xArr2=diab.iloc[:,:8].values\n",
    "yArr2=diab.iloc[:,8].values\n",
    "\n",
    "yArr=yArr[:,np.newaxis]\n",
    "type(xArr2[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.9972946 None\n",
      "200 0.5946644 None\n",
      "400 0.5459097 None\n",
      "600 0.5145314 None\n",
      "800 0.49075547 None\n",
      "1000 0.47081375 None\n",
      "1200 0.45309293 None\n",
      "1400 0.43684104 None\n",
      "1600 0.42168126 None\n",
      "1800 0.40741113 None\n",
      "2000 0.39391276 None\n",
      "2200 0.38111183 None\n",
      "2400 0.368955 None\n",
      "2600 0.3574009 None\n",
      "2800 0.34641466 None\n",
      "3000 0.33596468 None\n",
      "3200 0.32602188 None\n",
      "3400 0.3165588 None\n",
      "3600 0.30754933 None\n",
      "3800 0.29896852 None\n",
      "4000 0.29079273 None\n",
      "4200 0.28299913 None\n",
      "4400 0.2755664 None\n",
      "4600 0.26847404 None\n",
      "4800 0.2617029 None\n",
      "5000 0.2552347 None\n",
      "5200 0.24905229 None\n",
      "5400 0.24313973 None\n",
      "5600 0.2374817 None\n",
      "5800 0.232064 None\n",
      "6000 0.22687317 None\n",
      "6200 0.22189678 None\n",
      "6400 0.21712299 None\n",
      "6600 0.21254085 None\n",
      "6800 0.20813985 None\n",
      "7000 0.20391048 None\n",
      "7200 0.1998436 None\n",
      "7400 0.19593066 None\n",
      "7600 0.19216366 None\n",
      "7800 0.18853517 None\n",
      "8000 0.18503807 None\n",
      "8200 0.18166573 None\n",
      "8400 0.17841206 None\n",
      "8600 0.17527103 None\n",
      "8800 0.1722374 None\n",
      "9000 0.16930582 None\n",
      "9200 0.16647156 None\n",
      "9400 0.16372998 None\n",
      "9600 0.16107684 None\n",
      "9800 0.15850808 None\n",
      "10000 0.15601982 None\n",
      "\n",
      "hypothesis :  [[0.03353174]\n",
      " [0.16246563]\n",
      " [0.317957  ]\n",
      " [0.77546483]\n",
      " [0.9357847 ]\n",
      " [0.97890526]] \n",
      "predicted :  [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]] \n",
      "accuracy :  1.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X=tf.placeholder(tf.float32, shape=[None,2])\n",
    "Y=tf.placeholder(tf.float32, shape=[None,1])\n",
    "W=tf.Variable(tf.random_normal([2,1]), name='weight')\n",
    "b=tf.Variable(tf.random_normal([1]), name='bias')\n",
    "\n",
    "hypothesis=tf.sigmoid(tf.matmul(X,W)+b) # == hypohesis=1.0/(1+tf.exp(-(tf.matmul(X,W)+b))))   #로지스틱 함수\n",
    "\n",
    "cost=-(tf.reduce_mean(Y*tf.log(hypothesis)+(1-Y)*tf.log(1-hypothesis)))   #엔트로피\n",
    "\n",
    "train=tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)  #그래디언트옵티마이저로 cost값을 가장 최소화하는 \n",
    "\n",
    "predicted=tf.cast(hypothesis > 0.5, dtype=tf.float32)  # 시그모이드함수에서 나온값이 0.5보다 크면 1(float32형), 작으면, 0(float32형)\n",
    "\n",
    "accuracy=tf.reduce_mean(tf.cast(tf.equal(predicted,Y), dtype=tf.float32))   \n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(10001):\n",
    "        cos, val =sess.run([cost, train], feed_dict={X:x_data,Y:y_data})\n",
    "        \n",
    "        if step %200 ==0:\n",
    "            print(step, cos,val)\n",
    "            \n",
    "    h,o,a=sess.run([hypothesis,predicted,accuracy],feed_dict={X:x_data,Y:y_data,})\n",
    "    \n",
    "    print('\\nhypothesis : ',h,\n",
    "         '\\npredicted : ',o,\n",
    "         '\\naccuracy : ',a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
