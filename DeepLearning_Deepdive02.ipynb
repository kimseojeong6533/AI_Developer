{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN을 해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'placeholder'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-5f8b996a8fd6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;31m# RNN\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m \u001b[0mX\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msequence_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_dim\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplaceholder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msequence_length\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# index를 맞추는 모델 생성\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[0mcell\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBasicLSTMCell\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_units\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate_is_tuple\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'placeholder'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "#from tensorflow.examples.tutorials.mnist import input_data\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc=OneHotEncoder()\n",
    "\n",
    "tf.compat.v1.reset_default_graph()   # RNN keep weights in the memory\n",
    "\n",
    "idx2char=['h','i','e','l','o']\n",
    "x_data=[[0,1,0,2,3,3]]   #'hi hell', 입력값과 출력값개수를 맞추기위해서 마지막 인덱스 제외\n",
    "y_data=[[1,0,2,3,3,4]]   # i h e l l o       => h i h e l l -> i h e l l o\n",
    "\n",
    "#one_hot encoding\n",
    "x_one_hot=[[[1,0,0,0,0],\n",
    "            [0,1,0,0,0],\n",
    "            [1,0,0,0,0],\n",
    "            [0,0,1,0,0],\n",
    "            [0,0,0,1,0],\n",
    "            [0,0,0,1,0]]]    # [0,1,0,2,3,3]을 one_hot encoding한 결과\n",
    "#print(np.array(enc.fit(x_data)))\n",
    "\n",
    "\n",
    "input_dim=5   # x_one_hot에서 axis=2의 차원 수\n",
    "batch_size=1  # 예를들어, 예측할 단어가 hihell friend에서 hihell\n",
    "sequence_length=6  # n개의 sequence를 가진 데이터를 이용해 n개의 sequence(셀의 개수)를 예측하도록 그 n개 = sequence_length\n",
    "num_class=5\n",
    "\n",
    "learning_rate=0.1\n",
    "\n",
    "# RNN\n",
    "X=tf.placeholder(tf.float32, [None,sequence_length, input_dim])\n",
    "Y=tf.placeholder(tf.int32, [None,sequence_length])  # index를 맞추는 모델 생성\n",
    "cell=tf.contrib.rnn.BasicLSTMCell(num_units=5, state_is_tuple=True) \n",
    "initial_state=cell.zero_state(batch_size, tf.float32)  # 처음에 셀에 들어가는 초기값은 0으로 설정 \n",
    "output, _staste=tf.nn.dynamic_rnn(cell, X, initial_state=initial_state, dtype=tf.float32)\n",
    "output_shape=output.shape\n",
    "\n",
    "# Fully Connected\n",
    "x_for_fc = tf.reshape(output, [-1,5])  # num_units=5이기때문에, [None,5]로 reshape해줌\n",
    "outputs=tf.contrib.layers.fully_connected(inputs=x_for_fc,\n",
    "                                              num_outputs=num_class,\n",
    "                                          activation_fn=None)  # 선형회귀(activation_fn=None)로 학습\n",
    "\n",
    "outputs=tf.reshape(outputs,[batch_size, sequence_length, num_class])\n",
    "\n",
    "weight=tf.ones([batch_size, sequence_length])  #weight=[1,1,1,1,1,1]으로 설정\n",
    "sequence_loss=tf.contrib.seq2seq.sequence_loss(logits=outputs,  # seq2seq : sequence(outputs)와 sequence(Y) 사이의 거리(차)를 계산해줌\n",
    "                                               targets=Y,       # Y의 차원이 logits차원을 기준으로 one_hot_encoding 됨(seq2seq).\n",
    "                                               weights=weight)  # weights= weight(1로된 벡터)  -> 1 * 거리 + 1*거리... = 값 \n",
    "\n",
    "loss=tf.reduce_mean(sequence_loss)  \n",
    "train=tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "prediction=tf.argmax(outputs,axis=2)\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(50):\n",
    "        l, _ = sess.run([loss,train], feed_dict={X : x_one_hot, Y : y_data})\n",
    "        result=sess.run(prediction, feed_dict={X : x_one_hot})\n",
    "        print('loss : ',l, 'prediction : ',result,'Y : ',y_data)\n",
    "\n",
    "        result_str=[idx2char[c] for c in np.squeeze(result)]  # np.squeeze() -> [5,1,5] -> [5,5] (차원수가 1인 차원 제거 )\n",
    "        print('\\tPrediction : ',''.join(result_str))\n",
    "    \n",
    "    output_shape=sess.run()\n",
    "    print(output_shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3)\n",
      "(50000, 10)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Shape (1024, ?) must have rank at least 3",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-53-74ddbbe9f193>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[0mcell\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBasicLSTMCell\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_units\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate_is_tuple\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[0minitial_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcell\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# 처음에 셀에 들어가는 초기값은 0으로 설정\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_staste\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdynamic_rnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcell\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_img\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;31m# Fully Connected\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\venv\\lib\\site-packages\\tensorflow_core\\python\\util\\deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    322\u001b[0m               \u001b[1;34m'in a future version'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'after %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m               instructions)\n\u001b[1;32m--> 324\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    325\u001b[0m     return tf_decorator.make_decorator(\n\u001b[0;32m    326\u001b[0m         \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'deprecated'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\venv\\lib\\site-packages\\tensorflow_core\\python\\ops\\rnn.py\u001b[0m in \u001b[0;36mdynamic_rnn\u001b[1;34m(cell, inputs, sequence_length, initial_state, dtype, parallel_iterations, swap_memory, time_major, scope)\u001b[0m\n\u001b[0;32m    705\u001b[0m         \u001b[0mswap_memory\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mswap_memory\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    706\u001b[0m         \u001b[0msequence_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msequence_length\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 707\u001b[1;33m         dtype=dtype)\n\u001b[0m\u001b[0;32m    708\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    709\u001b[0m     \u001b[1;31m# Outputs of _dynamic_rnn_loop are always shaped [time, batch, depth].\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\venv\\lib\\site-packages\\tensorflow_core\\python\\ops\\rnn.py\u001b[0m in \u001b[0;36m_dynamic_rnn_loop\u001b[1;34m(cell, inputs, initial_state, parallel_iterations, swap_memory, sequence_length, dtype)\u001b[0m\n\u001b[0;32m    772\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    773\u001b[0m   inputs_got_shape = tuple(\n\u001b[1;32m--> 774\u001b[1;33m       input_.get_shape().with_rank_at_least(3) for input_ in flat_input)\n\u001b[0m\u001b[0;32m    775\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    776\u001b[0m   \u001b[0mconst_time_steps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconst_batch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minputs_got_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\venv\\lib\\site-packages\\tensorflow_core\\python\\ops\\rnn.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    772\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    773\u001b[0m   inputs_got_shape = tuple(\n\u001b[1;32m--> 774\u001b[1;33m       input_.get_shape().with_rank_at_least(3) for input_ in flat_input)\n\u001b[0m\u001b[0;32m    775\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    776\u001b[0m   \u001b[0mconst_time_steps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconst_batch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minputs_got_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\venv\\lib\\site-packages\\tensorflow_core\\python\\framework\\tensor_shape.py\u001b[0m in \u001b[0;36mwith_rank_at_least\u001b[1;34m(self, rank)\u001b[0m\n\u001b[0;32m   1030\u001b[0m     \"\"\"\n\u001b[0;32m   1031\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrank\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrank\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mrank\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1032\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Shape %s must have rank at least %d\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrank\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1033\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1034\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Shape (1024, ?) must have rank at least 3"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import cifar_cnn\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc=OneHotEncoder()\n",
    "\n",
    "d = cifar_cnn.CifarDataManager()\n",
    "images = d.train.images\n",
    "\n",
    "#cifar_cnn.display_cifar(images, 10)\n",
    "print(d.train.images.shape)  # 50000, 32, 32, 3\n",
    "print(d.train.labels.shape)  #50000, 10\n",
    "\n",
    "tf.compat.v1.reset_default_graph()   # RNN keep weights in the memory\n",
    "\n",
    "input_dim=10   # x_one_hot에서 axis=2의 차원 수\n",
    "batch_size=32  \n",
    "sequence_length=32  # n개의 sequence를 가진 데이터를 이용해 n개의 sequence(셀의 개수)를 예측하도록 그 n개 = sequence_length\n",
    "num_class=10\n",
    "\n",
    "learning_rate=0.1\n",
    "\n",
    "x_data=d.train.images\n",
    "y_data=d.train.labels\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 32,32,3])  # N,sequence_length, sequence_length,3\n",
    "Y = tf.placeholder(tf.float32, [None,10])      \n",
    "X_img=tf.reshape(X,[-1,32*32*1])\n",
    "\n",
    "idx2char=[]\n",
    "\n",
    "cell=tf.contrib.rnn.BasicLSTMCell(num_units=10, state_is_tuple=True) \n",
    "initial_state=cell.zero_state(batch_size, tf.float32)  # 처음에 셀에 들어가는 초기값은 0으로 설정 \n",
    "output, _staste=tf.nn.dynamic_rnn(cell, X_img, initial_state=initial_state, dtype=tf.float32)\n",
    "\n",
    "# Fully Connected\n",
    "x_for_fc = tf.reshape(output, [-1,10])  # num_units=5이기때문에, [None,5]로 reshape해줌\n",
    "outputs=tf.contrib.layers.fully_connected(inputs=x_for_fc,\n",
    "                                          num_outputs=num_class,\n",
    "                                          activation_fn=None)  # 선형회귀(activation_fn=None)로 학습\n",
    "\n",
    "outputs=tf.reshape(outputs,[batch_size, sequence_length, num_class])\n",
    "\n",
    "weight=tf.ones([batch_size, sequence_length])  #weight=[1,1,1,1,1,1]으로 설정\n",
    "sequence_loss=tf.contrib.seq2seq.sequence_loss(logits=outputs,  # seq2seq : sequence(outputs)와 sequence(Y) 사이의 거리(차)를 계산해줌\n",
    "                                               targets=Y,       # Y의 차원이 logits차원을 기준으로 one_hot_encoding 됨(seq2seq).\n",
    "                                               weights=weight)  # weights= weight(1로된 벡터)  -> 1 * 거리 + 1*거리... = 값 \n",
    "\n",
    "loss=tf.reduce_mean(sequence_loss)  \n",
    "train=tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "prediction=tf.argmax(outputs,axis=2)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    '''\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0\n",
    "        total_batch = int(d.train.images.shape[0] / batch_size)\n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = d.train.next_batch(batch_size)\n",
    "            feed_dict = {X_img:batch_xs, Y:batch_ys, keep_prob:0.7}\n",
    "            c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
    "            avg_cost += c / total_batch\n",
    "            \n",
    "        print(f'Epoch : {epoch+1}, Cost : {avg_cost}')\n",
    "        \n",
    "    print(\"Learning finished\")\n",
    "    '''\n",
    "    for i in range(50):\n",
    "        l, _ = sess.run([loss,train], feed_dict={X : x_data, Y : y_data})\n",
    "        result=sess.run(prediction, feed_dict={X : x_data})\n",
    "        print('loss : ',l, 'prediction : ',result,'Y : ',y_data)\n",
    "\n",
    "        result_str=[idx2char[c] for c in np.squeeze(result)]  # np.squeeze() -> [5,1,5] -> [5,5] (차원수가 1인 차원 제거 )\n",
    "        print('\\tPrediction : '.join(result_str))\n",
    "\n",
    "\n",
    "'''\n",
    "x_data=\n",
    "x_data=np.array(x_data).reshape(-1,10)\n",
    "enc.fit(x_data)\n",
    "x_one_hot=enc.transform(x_data).toarray()\n",
    "x_one_hot\n",
    "#x_data=tf.reshape(x_data,[,-1,32,1])\n",
    "#y_data=[0,0,1,0,0,0,0,0,0]   # 숫자는 2\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "원핫 인코딩하는 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]]\n",
      "(50000, 10)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc=OneHotEncoder()\n",
    "\n",
    "tf.compat.v1.reset_default_graph()   # RNN keep weights in the memory\n",
    "\n",
    "idx2char=['h','i','e','l','o']\n",
    "x_data=[[0,1,0,2,3,3]]   #'hi hell', 입력값과 출력값개수를 맞추기위해서 마지막 인덱스 제외\n",
    "y_data=[[1,0,2,3,3,4]]   # i h e l l o       => h i h e l l -> i h e l l o\n",
    "\n",
    "#one_hot encoding\n",
    "#x_one_hot=[[[1,0,0,0,0],\n",
    "#            [0,1,0,0,0],\n",
    "#            [1,0,0,0,0],\n",
    "#            [0,0,1,0,0],\n",
    "#            [0,0,0,1,0],\n",
    "#            [0,0,0,1,0]]]    # [0,1,0,2,3,3]을 one_hot encoding한 결과\n",
    "x_data=np.array(x_data).reshape(-1,10)\n",
    "enc.fit(x_data)\n",
    "x_one_hot=enc.transform(x_data).toarray()\n",
    "x_one_hot\n",
    "\n",
    "print(d.train.labels)\n",
    "print(d.train.labels.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./samples/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting ./samples/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting ./samples/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./samples/MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "(3,)\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"./samples/MNIST_data/\", one_hot=True)\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
