{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Shape of passed values is (1030, 8), indices imply (1030, 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\users\\user\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36mcreate_block_manager_from_blocks\u001b[1;34m(blocks, axes)\u001b[0m\n\u001b[0;32m   1653\u001b[0m                 blocks = [\n\u001b[1;32m-> 1654\u001b[1;33m                     \u001b[0mmake_block\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mblocks\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplacement\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mslice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1655\u001b[0m                 ]\n",
      "\u001b[1;32mc:\\users\\user\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\blocks.py\u001b[0m in \u001b[0;36mmake_block\u001b[1;34m(values, placement, klass, ndim, dtype)\u001b[0m\n\u001b[0;32m   3040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3041\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mklass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mndim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplacement\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mplacement\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3042\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\blocks.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, values, placement, ndim)\u001b[0m\n\u001b[0;32m    124\u001b[0m             raise ValueError(\n\u001b[1;32m--> 125\u001b[1;33m                 \u001b[1;34mf\"Wrong number of items passed {len(self.values)}, \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    126\u001b[0m                 \u001b[1;34mf\"placement implies {len(self.mgr_locs)}\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Wrong number of items passed 8, placement implies 9",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-216-6a295223ca26>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mfitted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmin_max_scaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxArr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mxArr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmin_max_scaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxArr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mxArr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxArr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    462\u001b[0m                 \u001b[0mmgr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minit_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    463\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 464\u001b[1;33m                 \u001b[0mmgr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minit_ndarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    465\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    466\u001b[0m         \u001b[1;31m# For data is list-like, or Iterable (will consume into list)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36minit_ndarray\u001b[1;34m(values, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    208\u001b[0m         \u001b[0mblock_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 210\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mcreate_block_manager_from_blocks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblock_values\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36mcreate_block_manager_from_blocks\u001b[1;34m(blocks, axes)\u001b[0m\n\u001b[0;32m   1662\u001b[0m         \u001b[0mblocks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"values\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mblocks\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1663\u001b[0m         \u001b[0mtot_items\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mblocks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1664\u001b[1;33m         \u001b[0mconstruction_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtot_items\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mblocks\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1665\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1666\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36mconstruction_error\u001b[1;34m(tot_items, block_shape, axes, e)\u001b[0m\n\u001b[0;32m   1692\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mblock_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1693\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Empty data passed with indices specified.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1694\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Shape of passed values is {passed}, indices imply {implied}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1695\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1696\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Shape of passed values is (1030, 8), indices imply (1030, 9)"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "f=pd.read_csv('concrete.csv')   \n",
    "xArr=f.iloc[:,:-1]\n",
    "f['strength_cut']=pd.qcut(f.strength,q=5,labels=[1,2,3,4,5])  #수치형데이터 -> 범주형데이터\n",
    "yArr=pd.get_dummies(f['strength_cut'])  #범주형데이터 -> 더미형  # (N x 5)\n",
    "\n",
    "min_max_scaler = MinMaxScaler()\n",
    "fitted = min_max_scaler.fit(xArr)\n",
    "xArr = min_max_scaler.transform(xArr)\n",
    "xArr = pd.DataFrame(xArr, columns=df.columns, index=list(df.index.values))\n",
    "print(output.head())\n",
    "\n",
    "\n",
    "X=tf.placeholder(tf.float32, shape=[None,8])\n",
    "Y=tf.placeholder(tf.float32, shape=[None,5])\n",
    "W1=tf.Variable(tf.random_normal([8,8]), name='weight')\n",
    "b1=tf.Variable(tf.random_normal([8]), name='bias')\n",
    "\n",
    "layer=tf.sigmoid(tf.matmul(X,W1)+b1)\n",
    "\n",
    "W2=tf.Variable(tf.random_normal([8,5]), name='weight')\n",
    "b2=tf.Variable(tf.random_normal([5]), name='bias')\n",
    "\n",
    "hypothesis=tf.sigmoid(tf.matmul(X,W2)+b2)\n",
    "\n",
    "cost=-tf.reduce_mean(Y*tf.log(hypothesis)+(1-Y)*tf.log(1-hypothesis))\n",
    "\n",
    "train=tf.train.GradientDescentOptimizer(learning_rate=0.000001).minimize(cost)\n",
    "predicted=tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
    "accuracy=tf.reduce_mean(tf.cast(tf.equal(predicted,Y),dtype=tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for step in range(100001):\n",
    "        cost_val, _ = sess.run([cost,train], feed_dict={X : xArr, Y : yArr})\n",
    "        if step%2000==0:\n",
    "            print(step,cost_val)\n",
    "            \n",
    "    h,c,a=sess.run([hypothesis,predicted,accuracy],feed_dict={X:x_data,Y:y_data})\n",
    "    print(\"\\nHypothesis: \",h,\n",
    "          \"\\nPredicted: \",c,\n",
    "          \"\\nAccuracy: \",a) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits=tf.matmul(X,W)+b\n",
    "hypothesis=tf.nn.softmax(logits)\n",
    "\n",
    "#손실값(y-y_hat) 계산\n",
    "cost_i=tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y_one_hot)   #소프트맥스의 엔트로피계산(손실값(y-y_hat)), 확률 계산을 해주는 함수\n",
    "cost=tf.reduce_mean(cost_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cement</th>\n",
       "      <th>slag</th>\n",
       "      <th>ash</th>\n",
       "      <th>water</th>\n",
       "      <th>superplastic</th>\n",
       "      <th>coarseagg</th>\n",
       "      <th>fineagg</th>\n",
       "      <th>age</th>\n",
       "      <th>strength</th>\n",
       "      <th>strength_cut</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>141.3</td>\n",
       "      <td>212.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>203.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>971.8</td>\n",
       "      <td>748.5</td>\n",
       "      <td>28</td>\n",
       "      <td>29.89</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>168.9</td>\n",
       "      <td>42.2</td>\n",
       "      <td>124.3</td>\n",
       "      <td>158.3</td>\n",
       "      <td>10.8</td>\n",
       "      <td>1080.8</td>\n",
       "      <td>796.2</td>\n",
       "      <td>14</td>\n",
       "      <td>23.51</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>250.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>95.7</td>\n",
       "      <td>187.4</td>\n",
       "      <td>5.5</td>\n",
       "      <td>956.9</td>\n",
       "      <td>861.2</td>\n",
       "      <td>28</td>\n",
       "      <td>29.22</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>266.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>932.0</td>\n",
       "      <td>670.0</td>\n",
       "      <td>28</td>\n",
       "      <td>45.85</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>154.8</td>\n",
       "      <td>183.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>193.3</td>\n",
       "      <td>9.1</td>\n",
       "      <td>1047.4</td>\n",
       "      <td>696.7</td>\n",
       "      <td>28</td>\n",
       "      <td>18.29</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1025</th>\n",
       "      <td>135.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>961.0</td>\n",
       "      <td>805.0</td>\n",
       "      <td>28</td>\n",
       "      <td>13.29</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1026</th>\n",
       "      <td>531.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>141.8</td>\n",
       "      <td>28.2</td>\n",
       "      <td>852.1</td>\n",
       "      <td>893.7</td>\n",
       "      <td>3</td>\n",
       "      <td>41.30</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1027</th>\n",
       "      <td>276.4</td>\n",
       "      <td>116.0</td>\n",
       "      <td>90.3</td>\n",
       "      <td>179.6</td>\n",
       "      <td>8.9</td>\n",
       "      <td>870.1</td>\n",
       "      <td>768.3</td>\n",
       "      <td>28</td>\n",
       "      <td>44.28</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1028</th>\n",
       "      <td>342.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>932.0</td>\n",
       "      <td>670.0</td>\n",
       "      <td>270</td>\n",
       "      <td>55.06</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1029</th>\n",
       "      <td>540.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>173.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1125.0</td>\n",
       "      <td>613.0</td>\n",
       "      <td>7</td>\n",
       "      <td>52.61</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1030 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      cement   slag    ash  water  superplastic  coarseagg  fineagg  age  \\\n",
       "0      141.3  212.0    0.0  203.5           0.0      971.8    748.5   28   \n",
       "1      168.9   42.2  124.3  158.3          10.8     1080.8    796.2   14   \n",
       "2      250.0    0.0   95.7  187.4           5.5      956.9    861.2   28   \n",
       "3      266.0  114.0    0.0  228.0           0.0      932.0    670.0   28   \n",
       "4      154.8  183.4    0.0  193.3           9.1     1047.4    696.7   28   \n",
       "...      ...    ...    ...    ...           ...        ...      ...  ...   \n",
       "1025   135.0    0.0  166.0  180.0          10.0      961.0    805.0   28   \n",
       "1026   531.3    0.0    0.0  141.8          28.2      852.1    893.7    3   \n",
       "1027   276.4  116.0   90.3  179.6           8.9      870.1    768.3   28   \n",
       "1028   342.0   38.0    0.0  228.0           0.0      932.0    670.0  270   \n",
       "1029   540.0    0.0    0.0  173.0           0.0     1125.0    613.0    7   \n",
       "\n",
       "      strength strength_cut  \n",
       "0        29.89            2  \n",
       "1        23.51            2  \n",
       "2        29.22            2  \n",
       "3        45.85            4  \n",
       "4        18.29            1  \n",
       "...        ...          ...  \n",
       "1025     13.29            1  \n",
       "1026     41.30            4  \n",
       "1027     44.28            4  \n",
       "1028     55.06            5  \n",
       "1029     52.61            5  \n",
       "\n",
       "[1030 rows x 10 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "f=pd.read_csv('concrete.csv')\n",
    "\n",
    "\n",
    "yArr.shape\n",
    "xArr.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1025</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1026</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1027</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1028</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1029</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1030 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      1  2  3  4  5\n",
       "0     0  1  0  0  0\n",
       "1     0  1  0  0  0\n",
       "2     0  1  0  0  0\n",
       "3     0  0  0  1  0\n",
       "4     1  0  0  0  0\n",
       "...  .. .. .. .. ..\n",
       "1025  1  0  0  0  0\n",
       "1026  0  0  0  1  0\n",
       "1027  0  0  0  1  0\n",
       "1028  0  0  0  0  1\n",
       "1029  0  0  0  0  1\n",
       "\n",
       "[1030 rows x 5 columns]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "f=pd.read_csv('concrete.csv') \n",
    "df=f.copy()\n",
    "\n",
    "min_max_scaler = MinMaxScaler()  # min_max 정규화\n",
    "fitted = min_max_scaler.fit(f)\n",
    "f = min_max_scaler.transform(f)\n",
    "f = pd.DataFrame(f, columns=df.columns, index=list(df.index.values))\n",
    "f=pd.DataFrame(f)\n",
    "\n",
    "xArr=f.iloc[:,:-1]\n",
    "f['strength_cut']=pd.qcut(f.strength,q=5,labels=[1,2,3,4,5])  #수치형데이터 -> 범주형데이터\n",
    "yArr=pd.get_dummies(f['strength_cut'])  #범주형데이터 -> one_hot형\n",
    "yArr.shape\n",
    "xArr\n",
    "yArr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.91182846 0.21941748\n",
      "200 0.7174694 0.2446602\n",
      "400 0.6137623 0.26893204\n",
      "600 0.5589732 0.27281553\n",
      "800 0.5292865 0.28252426\n",
      "1000 0.5126008 0.2883495\n",
      "1200 0.502883 0.29514563\n",
      "1400 0.49704477 0.29223302\n",
      "1600 0.49344054 0.29514563\n",
      "1800 0.49115852 0.29029125\n",
      "2000 0.48967618 0.29029125\n",
      "2200 0.488686 0.29126215\n",
      "2400 0.48800185 0.2932039\n",
      "2600 0.4875103 0.30097088\n",
      "2800 0.48714074 0.30194175\n",
      "3000 0.48684874 0.3038835\n",
      "3200 0.4866063 0.30970874\n",
      "3400 0.48639578 0.30970874\n",
      "3600 0.48620525 0.30970874\n",
      "3800 0.4860279 0.30873787\n",
      "4000 0.48585838 0.30582523\n",
      "4200 0.48569444 0.30485436\n",
      "4400 0.48553327 0.30485436\n",
      "4600 0.48537412 0.3038835\n",
      "4800 0.485216 0.30582523\n",
      "5000 0.4850584 0.30582523\n",
      "5200 0.48490068 0.3067961\n",
      "5400 0.4847429 0.3067961\n",
      "5600 0.4845851 0.3067961\n",
      "5800 0.48442668 0.3067961\n",
      "6000 0.48426786 0.30970874\n",
      "6200 0.4841087 0.3106796\n",
      "6400 0.483949 0.31165048\n",
      "6600 0.48378858 0.31359223\n",
      "6800 0.48362797 0.31359223\n",
      "7000 0.4834666 0.31359223\n",
      "7200 0.4833048 0.31262136\n",
      "7400 0.48314235 0.31262136\n",
      "7600 0.48297957 0.31262136\n",
      "7800 0.4828161 0.31262136\n",
      "8000 0.48265207 0.31262136\n",
      "8200 0.48248753 0.31262136\n",
      "8400 0.4823226 0.31262136\n",
      "8600 0.48215678 0.31359223\n",
      "8800 0.48199075 0.3145631\n",
      "9000 0.48182398 0.31359223\n",
      "9200 0.4816566 0.3145631\n",
      "9400 0.48148888 0.31359223\n",
      "9600 0.4813206 0.31359223\n",
      "9800 0.48115155 0.3145631\n",
      "10000 0.4809823 0.3145631\n",
      "\n",
      "Hypothesis:  [[0.22517622 0.25959802 0.21103513 0.19130003 0.11309803]\n",
      " [0.20849812 0.24196166 0.2097382  0.1885682  0.13213414]\n",
      " [0.20286149 0.22252706 0.20628399 0.18003759 0.14420831]\n",
      " ...\n",
      " [0.20304596 0.21333474 0.20165789 0.18838137 0.15505737]\n",
      " [0.18183094 0.14703271 0.17900738 0.18819392 0.2367093 ]\n",
      " [0.17964382 0.11132512 0.20342883 0.20601521 0.42716646]] \n",
      "Correct_Predicted:  [1. 1. 1. ... 0. 1. 1.] \n",
      "Accuracy:  0.3145631\n"
     ]
    }
   ],
   "source": [
    "#Concrete.csv\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "f=pd.read_csv('concrete.csv') \n",
    "df=f.copy()\n",
    "\n",
    "min_max_scaler = MinMaxScaler()  # min_max 정규화\n",
    "fitted = min_max_scaler.fit(f)\n",
    "f = min_max_scaler.transform(f)\n",
    "f = pd.DataFrame(f, columns=df.columns, index=list(df.index.values))\n",
    "f=pd.DataFrame(f)\n",
    "\n",
    "xArr=f.iloc[:,:-1]\n",
    "f['strength_cut']=pd.qcut(f.strength,q=5,labels=[1,2,3,4,5])  #수치형데이터 -> 범주형데이터\n",
    "yArr=pd.get_dummies(f['strength_cut'])  #범주형데이터 -> 더미형  # (N x 5)\n",
    "\n",
    "X = tf.placeholder(tf.float32,[None,8])\n",
    "Y = tf.placeholder(tf.float32,[None,5])\n",
    "#Y_one_hot = tf.one_hot(Y,5) #3차원의  data로 \n",
    "#Y_one_hot = tf.reshape(Y_one_hot,[-1,5])  #Y_one_hot변수의 끝 차원을 7로, 나머지 차원은 알아서 조정~\n",
    "\n",
    "\n",
    "W=tf.Variable(tf.random_normal([8,5]), name='weight')\n",
    "b=tf.Variable(tf.random_normal([5]), name='bias')\n",
    "\n",
    "layer1=tf.sigmoid(tf.matmul(X,W)+b)\n",
    "\n",
    "W2=tf.Variable(tf.random_normal([5,5]), name='weight')\n",
    "b2=tf.Variable(tf.random_normal([5]), name='bias')\n",
    "\n",
    "hypothesis=tf.sigmoid(tf.matmul(layer1,W2)+b2)\n",
    "\n",
    "cost = -tf.reduce_mean(Y*tf.log(hypothesis)+(1-Y)*tf.log(1-hypothesis))\n",
    "\n",
    "optimizer=tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
    "\n",
    "prediction=tf.argmax(hypothesis,1)  #y_hat행렬에서 1번째(2차원(행 x 열)이라면 열) 차원에서 가장 큰값  -> [[0.7,0.1,0.2],...] -> [0,...]\n",
    "correct_prediction=tf.cast(tf.equal(prediction, tf.argmax(Y,1)),tf.float32)   #예측값과 라벨값(실제값)과 같다면 True, 아니면, False\n",
    "accuracy=tf.reduce_mean(correct_prediction)  #True,False값을 float32형으로 바꾸고, 그것의 평균값 \n",
    "\n",
    "\n",
    "#predicted = tf.cast(hypothesis>0.5,dtype=tf.float32) \n",
    "#accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted,Y),dtype=tf.float32)) \n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())   #세션함수를 실행하기위해 필요하 모든 변수들 초기화\n",
    "    for step in range(10001):\n",
    "        sess.run(optimizer, feed_dict={X : xArr, Y : yArr})\n",
    "        if step%200==0:\n",
    "            loss, acc = sess.run([cost,accuracy], feed_dict={X :xArr, Y :yArr})\n",
    "            print(step,loss,acc)\n",
    "            \n",
    "    h,c,a=sess.run([hypothesis,correct_prediction,accuracy],feed_dict={X:xArr,Y:yArr})\n",
    "    print(\"\\nHypothesis: \",h,\n",
    "          \"\\nCorrect_Predicted: \",c,\n",
    "          \"\\nAccuracy: \",a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNIST 데이터가지고 SoftMax이용해서 accuracy 뽑기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist=input_data.read_data_sets('MNIST_data/', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 728103000.0 0.45252728\n",
      "200 20517420.0 0.9168\n",
      "400 309344160.0 0.74487275\n",
      "600 30060466.0 0.87756366\n",
      "800 18794958.0 0.9246909\n",
      "1000 42492500.0 0.86161816\n",
      "1200 23473164.0 0.9170909\n",
      "1400 26864494.0 0.90803635\n",
      "1600 57448380.0 0.82256365\n",
      "1800 20602812.0 0.9082\n",
      "\n",
      "Hypothesis:  [[0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00]\n",
      " [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00]\n",
      " [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 9.9999964e-01\n",
      "  0.0000000e+00 3.1870985e-07]\n",
      " ...\n",
      " [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00]\n",
      " [0.0000000e+00 1.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00]\n",
      " [0.0000000e+00 0.0000000e+00 0.0000000e+00 ... 0.0000000e+00\n",
      "  0.0000000e+00 0.0000000e+00]] \n",
      "Prediction:  [5 4 7 ... 5 1 3] \n",
      "Accuracy:  0.92669094\n"
     ]
    }
   ],
   "source": [
    "# MNIST by SoftMax \n",
    "import matplotlib.pyplot as plt  #소프트맥스로 분류모델, NN로 분류모델 만들어보기 \n",
    "\n",
    "\n",
    "#for i in range(5):\n",
    "#    print(mnist.train.labels[i])\n",
    "#    print(np.where(mnist.train.labels[i] == 1))\n",
    "#    plt.imshow(mnist.train.images[i].reshape(28,28), cmap='Greys')\n",
    "#    plt.show()\n",
    "    \n",
    "mnist_label=pd.DataFrame(mnist.train.labels)\n",
    "\n",
    "#mnist_image=pd.DataFrame(mnist.train.images.reshape(28,28))\n",
    "\n",
    "xArr=mnist.train.images\n",
    "yArr=mnist.train.labels\n",
    "\n",
    "#print(xArr.shape)\n",
    "\n",
    "X = tf.placeholder(tf.float32,[None,784])\n",
    "Y = tf.placeholder(tf.float32,[None,10])\n",
    "\n",
    "W=tf.Variable(tf.random_normal([784,10]), name='weight')\n",
    "b=tf.Variable(tf.random_normal([10]), name='bias')\n",
    "logits=tf.matmul(X,W)+b\n",
    "hypothesis=tf.nn.softmax(logits)\n",
    "\n",
    "\n",
    "#손실값(y-y_hat) 계산\n",
    "cost_i=tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y)   #소프트맥스의 엔트로피계산(손실값(y-y_hat)), 확률 계산을 해주는 함수\n",
    "cost=tf.reduce_sum(cost_i)\n",
    "optimizer=tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "#소프트맥스값으로부터 accuracy도출\n",
    "prediction=tf.argmax(hypothesis,1)  #y_hat행렬에서 1번째(2차원(행 x 열)이라면 열) 차원에서 가장 큰값  -> [[0.7,0.1,0.2],...] -> [0,...]\n",
    "correct_prediction=tf.equal(prediction, tf.argmax(Y,1))   #예측값과 라벨값(실제값)과 같다면 True, 아니면, False\n",
    "accuracy=tf.reduce_mean(tf.cast(correct_prediction, tf.float32))  #True,False값을 float32형으로 바꾸고, 그것의 평균값 \n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())   #세션함수를 실행하기위해 필요하 모든 변수들 초기화\n",
    "    for step in range(2000):\n",
    "        sess.run(optimizer, feed_dict={X : xArr, Y : yArr})\n",
    "        if step%200==0:\n",
    "            loss, acc, c_pred = sess.run([cost,accuracy,correct_prediction], feed_dict={X :xArr, Y :yArr})\n",
    "            print(step,loss,acc)\n",
    "            \n",
    "    pred=sess.run(prediction, feed_dict={X:xArr})\n",
    "    #for p,y in zip(pred,yArr.flatten()):  #y_data의 인덱스 값을 가져옴. \n",
    "    #    print('{} Prediction: {} True Y : {}'.format(p==int(y),p,int(y)))\n",
    "    h,c,a=sess.run([hypothesis,prediction,accuracy],feed_dict={X:xArr,Y:yArr})\n",
    "    print(\"\\nHypothesis: \",h,\n",
    "          \"\\nPrediction: \",c,\n",
    "          \"\\nAccuracy: \",a)        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting ./samples/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting ./samples/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting ./samples/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting ./samples/MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "0.9203\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "# Dataset loading\n",
    "mnist = input_data.read_data_sets(\"./samples/MNIST_data/\", one_hot=True)\n",
    "\n",
    "# Set up model\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "y = tf.nn.softmax(tf.matmul(x, W) + b)\n",
    "\n",
    "y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "cross_entropy = -tf.reduce_sum(y_*tf.log(y))\n",
    "train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)\n",
    "\n",
    "# Session\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "# Learning\n",
    "for i in range(1000):\n",
    "  batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "  sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})\n",
    "\n",
    "# Validation\n",
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# Result should be approximately 91%.\n",
    "print(sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST by SoftMax \n",
    "import matplotlib.pyplot as plt  #소프트맥스로 분류모델, NN로 분류모델 만들어보기 \n",
    "\n",
    "\n",
    "#for i in range(5):\n",
    "#    print(mnist.train.labels[i])\n",
    "#    print(np.where(mnist.train.labels[i] == 1))\n",
    "#    plt.imshow(mnist.train.images[i].reshape(28,28), cmap='Greys')\n",
    "#    plt.show()\n",
    "    \n",
    "mnist_label=pd.DataFrame(mnist.train.labels)\n",
    "\n",
    "#mnist_image=pd.DataFrame(mnist.train.images.reshape(28,28))\n",
    "\n",
    "xArr=mnist.train.images\n",
    "yArr=mnist.train.labels\n",
    "\n",
    "#print(xArr.shape)\n",
    "\n",
    "X = tf.placeholder(tf.float32,[None,784])\n",
    "Y = tf.placeholder(tf.float32,[None,10])\n",
    "\n",
    "W=tf.Variable(tf.random_normal([784,10]), name='weight')\n",
    "b=tf.Variable(tf.random_normal([10]), name='bias')\n",
    "logits=tf.matmul(X,W)+b\n",
    "hypothesis=tf.nn.softmax(logits)\n",
    "\n",
    "\n",
    "#손실값(y-y_hat) 계산\n",
    "cost_i=tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y)   #소프트맥스의 엔트로피계산(손실값(y-y_hat)), 확률 계산을 해주는 함수\n",
    "cost=tf.reduce_sum(cost_i)\n",
    "optimizer=tf.train.AdamOptimizer(learning_rate=0.1).minimize(cost)\n",
    "\n",
    "#소프트맥스값으로부터 accuracy도출\n",
    "prediction=tf.argmax(hypothesis,1)  #y_hat행렬에서 1번째(2차원(행 x 열)이라면 열) 차원에서 가장 큰값  -> [[0.7,0.1,0.2],...] -> [0,...]\n",
    "correct_prediction=tf.equal(prediction, tf.argmax(Y,1))   #예측값과 라벨값(실제값)과 같다면 True, 아니면, False\n",
    "accuracy=tf.reduce_mean(tf.cast(correct_prediction, tf.float32))  #True,False값을 float32형으로 바꾸고, 그것의 평균값 \n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())   #세션함수를 실행하기위해 필요하 모든 변수들 초기화\n",
    "    for step in range(2000):\n",
    "        sess.run(optimizer, feed_dict={X : xArr, Y : yArr})\n",
    "        if step%200==0:\n",
    "            loss, acc, c_pred = sess.run([cost,accuracy,correct_prediction], feed_dict={X :xArr, Y :yArr})\n",
    "            print(step,loss,acc)\n",
    "            \n",
    "    pred=sess.run(prediction, feed_dict={X:xArr})\n",
    "    #for p,y in zip(pred,yArr.flatten()):  #y_data의 인덱스 값을 가져옴. \n",
    "    #    print('{} Prediction: {} True Y : {}'.format(p==int(y),p,int(y)))\n",
    "    h,c,a=sess.run([hypothesis,prediction,accuracy],feed_dict={X:xArr,Y:yArr})\n",
    "    print(\"\\nHypothesis: \",h,\n",
    "          \"\\nPrediction: \",c,\n",
    "          \"\\nAccuracy: \",a)        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1,Cost:5.470295367674387\n",
      "Epoch:2,Cost:1.830300206054341\n",
      "Epoch:3,Cost:1.1598414309458294\n",
      "Epoch:4,Cost:0.9118864035606391\n",
      "Epoch:5,Cost:0.757100760882551\n",
      "Epoch:6,Cost:0.6767385424267166\n",
      "Epoch:7,Cost:0.6057347195527772\n",
      "Epoch:8,Cost:0.5630745476484295\n",
      "Epoch:9,Cost:0.5299443320794539\n",
      "Epoch:10,Cost:0.5063825126398693\n",
      "Epoch:11,Cost:0.46769461740146906\n",
      "Epoch:12,Cost:0.45662229582667385\n",
      "Epoch:13,Cost:0.4425627184320577\n",
      "Epoch:14,Cost:0.4215375991436572\n",
      "Epoch:15,Cost:0.4192619674991473\n",
      "Learning Finished\n",
      "Traindata_Acc : 0.8986181616783142\n",
      "Testdata_Acc : 0.9006999731063843\n"
     ]
    }
   ],
   "source": [
    "# 손글씨 인식 정확도 추출\n",
    "import matplotlib.pyplot as plt  #소프트맥스로 분류모델, NN로 분류모델 만들어보기 \n",
    "\n",
    "\n",
    "#for i in range(5):\n",
    "#    print(mnist.train.labels[i])\n",
    "#    print(np.where(mnist.train.labels[i] == 1))\n",
    "#    plt.imshow(mnist.train.images[i].reshape(28,28), cmap='Greys')\n",
    "#    plt.show()\n",
    "    \n",
    "#mnist_label=pd.DataFrame(mnist.train.labels)\n",
    "\n",
    "#mnist_image=pd.DataFrame(mnist.train.images.reshape(28,28))\n",
    "\n",
    "#batch_xs=mnist.train.images\n",
    "#batch_ys=mnist.train.labels\n",
    "\n",
    "\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100   #100개의 행을 먼저 연산 -> 200번 행 연산 -> ... \n",
    "\n",
    "X=tf.placeholder(tf.float32, shape=[None,784]) #28*28 개의 픽셀값 2차원 데이터로 저장되어있음\n",
    "Y=tf.placeholder(tf.float32, [None, 10]) #10개로 맵핑\n",
    "\n",
    "w = tf.Variable(tf.random_normal([784, 10]),name='weight')\n",
    "b = tf.Variable(tf.random_normal([10]),name='bias')\n",
    "\n",
    "hypothesis = tf.matmul(X,w) + b\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=hypothesis,labels=Y)) #0-1사이값 : corss_entropy 하는이유\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(training_epochs):  # epoch만큼 data 반복. -> 너무크면 overfitting\n",
    "        \n",
    "        avg_cost=0\n",
    "        total_batch=int(mnist.train.num_examples/batch_size)  #batch가 큼 -> 전체데이터를 고려한 가중치가 계산 / batch가 작 -> 각 행을 리뷰하며 가중치가 계산 및 조정됨.\n",
    "    \n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            feed_dict = {X:batch_xs, Y:batch_ys}\n",
    "            c,_=sess.run([cost,optimizer],feed_dict=feed_dict)\n",
    "            avg_cost += c/total_batch  # (0~99번째 행의 cost값)/(0~99번째 행의 데이터)\n",
    "            \n",
    "        print(\"Epoch:{},Cost:{}\".format(epoch+1,avg_cost))\n",
    "    print(\"Learning Finished\")\n",
    "    \n",
    "    correct_prediction=tf.equal(tf.argmax(hypothesis,1),tf.argmax(Y,1))\n",
    "    accuracy=tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    print('Traindata_Acc : {}'.format(sess.run(accuracy, feed_dict={X:mnist.train.images, Y : mnist.train.labels})))\n",
    "    print('Testdata_Acc : {}'.format(sess.run(accuracy, feed_dict={X:mnist.test.images, Y : mnist.test.labels})))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNIST(손글씨)인식해서 NN으로 모델만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1,Cost:186.01757405194354\n",
      "Epoch:2,Cost:40.5350787110762\n",
      "Epoch:3,Cost:25.617610054883098\n",
      "Epoch:4,Cost:17.699897062873305\n",
      "Epoch:5,Cost:13.483053535819053\n",
      "Epoch:6,Cost:9.660054807566802\n",
      "Epoch:7,Cost:7.336930938967034\n",
      "Epoch:8,Cost:5.406569056660226\n",
      "Epoch:9,Cost:4.260584696213256\n",
      "Epoch:10,Cost:3.0729697660389212\n",
      "Epoch:11,Cost:2.423628734307871\n",
      "Epoch:12,Cost:1.887124719543891\n",
      "Epoch:13,Cost:1.460996061237228\n",
      "Epoch:14,Cost:1.0408633215458558\n",
      "Epoch:15,Cost:0.950023523156202\n",
      "Learning Finished\n",
      "Traindata_Acc : 0.9919818043708801\n",
      "Testdata_Acc : 0.9459999799728394\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt  #소프트맥스로 분류모델, NN로 분류모델 만들어보기 \n",
    "\n",
    "\n",
    "#for i in range(5):\n",
    "#    print(mnist.train.labels[i])\n",
    "#    print(np.where(mnist.train.labels[i] == 1))\n",
    "#    plt.imshow(mnist.train.images[i].reshape(28,28), cmap='Greys')\n",
    "#    plt.show()\n",
    "    \n",
    "#mnist_label=pd.DataFrame(mnist.train.labels)\n",
    "\n",
    "#mnist_image=pd.DataFrame(mnist.train.images.reshape(28,28))\n",
    "\n",
    "#batch_xs=mnist.train.images\n",
    "#batch_ys=mnist.train.labels\n",
    "\n",
    "\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100   #100개의 행을 먼저 연산 -> 200번 행 연산 -> ... \n",
    "\n",
    "X=tf.placeholder(tf.float32, shape=[None,784]) #28*28 개의 픽셀값 2차원 데이터로 저장되어있음\n",
    "Y=tf.placeholder(tf.float32, [None, 10]) #10개로 맵핑\n",
    "\n",
    "w1 = tf.Variable(tf.random_normal([784, 256]),name='weight')\n",
    "b1 = tf.Variable(tf.random_normal([256]),name='bias')\n",
    "L1=tf.nn.relu(tf.matmul(X,w1)+b1)\n",
    "\n",
    "w2 = tf.Variable(tf.random_normal([256, 256]),name='weight')\n",
    "b2 = tf.Variable(tf.random_normal([256]),name='bias')\n",
    "L2=tf.nn.relu(tf.matmul(L1,w2)+b2)                               \n",
    "\n",
    "w3 = tf.Variable(tf.random_normal([256, 10]),name='weight')\n",
    "b3 = tf.Variable(tf.random_normal([10]),name='bias')\n",
    "hypothesis=tf.matmul(L2,w3)+b3\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=hypothesis,labels=Y)) #0-1사이값 : corss_entropy 하는이유\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(training_epochs):  # epoch만큼 data 반복. -> 너무크면 overfitting\n",
    "        avg_cost=0\n",
    "        total_batch=int(mnist.train.num_examples/batch_size)  #batch가 큼 -> 전체데이터를 고려한 가중치가 계산 / batch가 작 -> 각 행을 리뷰하며 가중치가 계산 및 조정됨.\n",
    "    \n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            feed_dict = {X:batch_xs, Y:batch_ys}\n",
    "            c,_=sess.run([cost,optimizer],feed_dict=feed_dict)\n",
    "            avg_cost += c/total_batch  # (0~99번째 행의 cost값)/(0~99번째 행의 데이터)\n",
    "            \n",
    "        print(\"Epoch:{},Cost:{}\".format(epoch+1,avg_cost))\n",
    "    print(\"Learning Finished\")\n",
    "    \n",
    "    correct_prediction=tf.equal(tf.argmax(hypothesis,1),tf.argmax(Y,1))\n",
    "    accuracy=tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    print('Traindata_Acc : {}'.format(sess.run(accuracy, feed_dict={X:mnist.train.images, Y : mnist.train.labels})))\n",
    "    print('Testdata_Acc : {}'.format(sess.run(accuracy, feed_dict={X:mnist.test.images, Y : mnist.test.labels})))  \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "직접쓴 손글씨를 test_data로 넣어보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 784)\n",
      "Epoch:1,Cost:149.10528017390854\n",
      "Epoch:2,Cost:38.99322890325027\n",
      "Epoch:3,Cost:23.691722461960516\n",
      "Epoch:4,Cost:16.636121921810243\n",
      "Epoch:5,Cost:12.161533050110393\n",
      "Epoch:6,Cost:8.955364884152834\n",
      "Epoch:7,Cost:6.724275336813903\n",
      "Epoch:8,Cost:5.02076455467344\n",
      "Epoch:9,Cost:3.8693865772928717\n",
      "Epoch:10,Cost:2.8197823581384722\n",
      "Epoch:11,Cost:2.094702265047934\n",
      "Epoch:12,Cost:1.6707638312134145\n",
      "Epoch:13,Cost:1.2269014083251009\n",
      "Epoch:14,Cost:0.9906549283278285\n",
      "Epoch:15,Cost:0.7386650119252889\n",
      "Learning Finished\n",
      "Traindata_Acc : 0.9929636120796204\n",
      "Prediction of my data:(array([0], dtype=int64), array([2], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt  #소프트맥스로 분류모델, NN로 분류모델 만들어보기 \n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "im = Image.open('my_written_2.png')\n",
    "pixel =np.array(im)  #3byte씩, 읽어옴  shape = (28,28,3)\n",
    "mytestimg=1-pixel[:,:,0]/255  # 0~1사이값으로 변경 (까만부분 : 1, 하얀부분 : 0에 가까움)\n",
    "mytestimg=mytestimg.reshape(1,784)    # X값이 (N x 784이므로 -> (784,1)로 바꾸어 줘야함)\n",
    "print(mytestimg.shape)\n",
    "\n",
    "\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100   #100개의 행을 먼저 연산 -> 200번 행 연산 -> ... \n",
    "\n",
    "X=tf.placeholder(tf.float32, shape=[None,784]) #28*28 개의 픽셀값 2차원 데이터로 저장되어있음\n",
    "Y=tf.placeholder(tf.float32, [None, 10]) #10개로 맵핑\n",
    "\n",
    "w1 = tf.Variable(tf.random_normal([784, 256]),name='weight')\n",
    "b1 = tf.Variable(tf.random_normal([256]),name='bias')\n",
    "L1=tf.nn.relu(tf.matmul(X,w1)+b1)\n",
    "\n",
    "w2 = tf.Variable(tf.random_normal([256, 256]),name='weight')\n",
    "b2 = tf.Variable(tf.random_normal([256]),name='bias')\n",
    "L2=tf.nn.relu(tf.matmul(L1,w2)+b2)                               \n",
    "\n",
    "w3 = tf.Variable(tf.random_normal([256, 10]),name='weight')\n",
    "b3 = tf.Variable(tf.random_normal([10]),name='bias')\n",
    "hypothesis=tf.matmul(L2,w3)+b3\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=hypothesis,labels=Y)) #0-1사이값 : corss_entropy 하는이유\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(training_epochs):  # epoch만큼 data 반복. -> 너무크면 overfitting\n",
    "        avg_cost=0\n",
    "        total_batch=int(mnist.train.num_examples/batch_size)  #batch가 큼 -> 전체데이터를 고려한 가중치가 계산 / batch가 작 -> 각 행을 리뷰하며 가중치가 계산 및 조정됨.\n",
    "    \n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            feed_dict = {X:batch_xs, Y:batch_ys}\n",
    "            c,_=sess.run([cost,optimizer],feed_dict=feed_dict)\n",
    "            avg_cost += c/total_batch  # (0~99번째 행의 cost값)/(0~99번째 행의 데이터)\n",
    "            \n",
    "        print(\"Epoch:{},Cost:{}\".format(epoch+1,avg_cost))\n",
    "    print(\"Learning Finished\")\n",
    "    \n",
    "    correct_prediction=tf.equal(tf.argmax(hypothesis,1),tf.argmax(Y,1))\n",
    "    accuracy=tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    print('Traindata_Acc : {}'.format(sess.run(accuracy, feed_dict={X:mnist.train.images, Y : mnist.train.labels})))\n",
    "\n",
    "    pred = sess.run(hypothesis, feed_dict={X:mytestimg})\n",
    "    print(\"Prediction of my data:{}\".format(np.where(pred==np.max(pred))))\n",
    "    #print(\"mytest.shape : {}\".format(sess.run(mytestimg.shape)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
